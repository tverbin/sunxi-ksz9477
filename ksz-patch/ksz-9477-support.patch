diff --git a/drivers/net/ethernet/micrel/Kconfig b/drivers/net/ethernet/micrel/Kconfig
index fe42fc0..6e29e74 100644
--- a/drivers/net/ethernet/micrel/Kconfig
+++ b/drivers/net/ethernet/micrel/Kconfig
@@ -68,3 +68,69 @@ config KSZ884X_PCI
 	  will be called ksz884x.
 
 endif # NET_VENDOR_MICREL
+
+menuconfig MICREL_SWITCHES
+	tristate "Drivers for Micrel switches"
+	---help---
+	  Supports Micrel switches.
+
+if MICREL_SWITCHES
+
+comment "Micrel switch device drivers"
+
+config MICREL_SWITCH
+	bool
+	default n
+
+config MICREL_SWITCH_EMBEDDED
+	bool
+	default n
+	select MICREL_SWITCH
+
+config HAVE_KSZ9897
+	bool
+	default n
+
+config HAVE_SPI_KSZ9897
+	bool
+	default n
+	select HAVE_KSZ9897
+
+config I2C_KSZ9897
+	tristate "I2C driver for Micrel KSZ9897 switch"
+	select MICREL_SWITCH if !NET_DSA_TAG_TAIL
+	select HAVE_KSZ9897
+	---help---
+	  Supports the Micrel KSZ9897 switch.
+
+config SPI_KSZ9897
+	tristate "SPI driver for Micrel KSZ9897 switch"
+	select MICREL_SWITCH if !NET_DSA_TAG_TAIL
+	select HAVE_SPI_KSZ9897
+	---help---
+	  Supports the Micrel KSZ9897 switch.
+
+config MICREL_KSZ9897_EMBEDDED
+	bool "Micrel KSZ9897 switch support in network controller"
+	depends on SPI_KSZ9897 = n && !NET_DSA_TAG_TAIL
+	select MICREL_SWITCH_EMBEDDED
+	select HAVE_SPI_KSZ9897
+	---help---
+	  Supports the Micrel KSZ9897 switch used within a network controller.
+
+config MICREL_KSZ9897_PTP
+	bool "1588 PTP support"
+	depends on HAVE_KSZ9897
+	default y
+	help
+	  Enable 1588 PTP support.
+
+config MICREL_KSZ9897_STP
+	bool "STP support"
+	depends on HAVE_KSZ9897
+	default n
+	help
+	  Enable STP support.
+
+endif
+
diff --git a/drivers/net/ethernet/micrel/Makefile b/drivers/net/ethernet/micrel/Makefile
index c83e4bc..080414d 100644
--- a/drivers/net/ethernet/micrel/Makefile
+++ b/drivers/net/ethernet/micrel/Makefile
@@ -7,3 +7,6 @@ obj-$(CONFIG_KS8842) += ks8842.o
 obj-$(CONFIG_KS8851) += ks8851.o
 obj-$(CONFIG_KS8851_MLL) += ks8851_mll.o
 obj-$(CONFIG_KSZ884X_PCI) += ksz884x.o
+
+obj-$(CONFIG_I2C_KSZ9897) += i2c-ksz9897.o
+obj-$(CONFIG_SPI_KSZ9897) += spi-ksz9897.o
diff --git a/drivers/net/ethernet/micrel/i2c-ksz9897.c b/drivers/net/ethernet/micrel/i2c-ksz9897.c
new file mode 100644
index 0000000..5a24cac
--- /dev/null
+++ b/drivers/net/ethernet/micrel/i2c-ksz9897.c
@@ -0,0 +1,1976 @@
+/**
+ * Micrel KSZ9897 I2C driver
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+#include <linux/i2c.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include "ksz_cfg_9897.h"
+
+#if 0
+#define NO_ACL
+#endif
+
+#if 1
+#define NO_EEE
+#endif
+
+#ifdef KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		i2c_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	i2c_wrreg16(ks, reg, val)
+#define HW_R24(ks, reg)		i2c_rdreg24(ks, reg)
+#define HW_W24(ks, reg, val)	i2c_wrreg24(ks, reg, val)
+#define HW_R32(ks, reg)		i2c_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	i2c_wrreg32(ks, reg, val)
+
+#include "ksz_sw_phy.h"
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define contain_reg(addr, len, reg)	\
+	(addr <= (reg) && (reg) <= (addr + len - 1))
+
+static void i2c_chk_regs(struct ksz_sw *sw, u32 addr, u8 *val, size_t txl)
+{
+	int i;
+	u32 port_reg;
+
+	port_reg = REG_PTP_MSG_CONF1 + 1;
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PTP_ENABLE)
+			sw->overrides |= PTP_TAG;
+		else
+			sw->overrides &= ~PTP_TAG;
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+		if (!(sw->features & NEW_CAP) &&
+		    sw->TAIL_TAG_LOOKUP >= 0x100)
+			sw->overrides |= PTP_TAG;
+#endif
+	}
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PORT_TAIL_TAG_ENABLE)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+	}
+}  /* i2c_chk_regs */
+
+/*
+ * I2C register read/write calls.
+ *
+ * All these calls issue I2C transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * i2c_wrreg - issue write register command
+ * @ks:		The switch device structure.
+ * @addr:	The register address.
+ * @val:	The value to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary i2c message(s)
+ * to write data to the register specified in @addr.
+ */
+static void i2c_wrreg(struct sw_priv *ks, u32 addr, void *txb, size_t txl)
+{
+	struct i2c_hw_priv *hw_priv = ks->hw_dev;
+	struct i2c_msg msg;
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("W not locked\n");
+
+	hw_priv->txd[0] = (u8)(addr >> 8);
+	hw_priv->txd[1] = (u8) addr;
+
+	memcpy(&hw_priv->txd[2], txb, txl);
+
+	msg.addr = i2c->addr;
+	msg.flags = 0;
+	msg.len = 2 + txl;
+	msg.buf = hw_priv->txd;
+
+	if (i2c_transfer(adapter, &msg, 1) != 1)
+		pr_alert("i2c_transfer() failed\n");
+	i2c_chk_regs(&ks->sw, addr, txb, txl);
+}
+
+/**
+ * i2c_wrreg32 - write 32bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg32(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 4;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 4);
+}
+
+/**
+ * i2c_wrreg24 - write 24bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg24(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 3;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 3);
+}
+
+/**
+ * i2c_wrreg16 - write 16bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg16(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 2;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 2);
+}
+
+/**
+ * i2c_wrreg8 - write 8bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg8(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 1;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 1);
+}
+
+/**
+ * i2c_rdreg - issue read register command and return the data
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary i2c message(s)
+ * to read data from the register specified in @op.
+ */
+static void i2c_rdreg(struct sw_priv *ks, u32 addr, void *rxb, size_t rxl)
+{
+	struct i2c_hw_priv *hw_priv = ks->hw_dev;
+	struct i2c_msg msg[2];
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("R not locked\n");
+
+	hw_priv->txd[0] = (u8)(addr >> 8);
+	hw_priv->txd[1] = (u8) addr;
+
+	msg[0].addr = i2c->addr;
+	msg[0].flags = 0;
+	msg[0].len = 2;
+	msg[0].buf = hw_priv->txd;
+
+	msg[1].addr = i2c->addr;
+	msg[1].flags = I2C_M_RD;
+	msg[1].len = rxl;
+	msg[1].buf = rxb;
+
+	if (i2c_transfer(adapter, msg, 2) != 2)
+		pr_alert("i2c_transfer() failed\n");
+}
+
+/**
+ * i2c_rdreg8 - read 8 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 i2c_rdreg8(struct sw_priv *ks, u32 reg)
+{
+	u8 rxb[1];
+
+	i2c_rdreg(ks, reg, rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * i2c_rdreg16 - read 16 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 i2c_rdreg16(struct sw_priv *ks, u32 reg)
+{
+	__le16 rx = 0;
+
+	i2c_rdreg(ks, reg, &rx, 2);
+	return be16_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg24 - read 24 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 24bit register from the chip.
+ */
+static u32 i2c_rdreg24(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(ks, reg, &rx, 3);
+	return be32_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg32 - read 32 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ */
+static u32 i2c_rdreg32(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(ks, reg, &rx, 4);
+	return be32_to_cpu(rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * delay_micro - delay in microsecond
+ * @microsec:	Number of microseconds to delay.
+ *
+ * This routine delays in microseconds.
+ */
+static inline void delay_micro(uint microsec)
+{
+	uint millisec = microsec / 1000;
+
+	microsec %= 1000;
+	if (millisec)
+		mdelay(millisec);
+	if (microsec)
+		udelay(microsec);
+}
+
+/**
+ * delay_milli - delay in millisecond
+ * @millisec:	Number of milliseconds to delay.
+ *
+ * This routine delays in milliseconds.
+ */
+static void delay_milli(uint millisec)
+{
+	unsigned long ticks = millisec * HZ / 1000;
+
+	if (!ticks || in_interrupt())
+		mdelay(millisec);
+	else {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(ticks);
+	}
+}
+
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+static inline void copy_old_skb(struct sk_buff *old, struct sk_buff *skb)
+{
+	skb->dev = old->dev;
+	skb->protocol = old->protocol;
+	skb->ip_summed = old->ip_summed;
+	skb->csum = old->csum;
+	skb_set_network_header(skb, ETH_HLEN);
+
+	dev_kfree_skb(old);
+}  /* copy_old_skb */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define KSZSW_REGS_SIZE			0x8000
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x0000, 0x7FFF },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->reglock);
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed\n", __func__);
+		mutex_unlock(sw->reglock);
+		sw->reg->lock(sw);
+	}
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R24(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	i2c_rdreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	i2c_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W24(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+static void sw_port_r16(struct ksz_sw *sw, int port, int offset, u16 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw_r16(sw, addr);
+}  /* sw_port_r16 */
+
+static void sw_port_w16(struct ksz_sw *sw, int port, int offset, u16 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw_w16(sw, addr, data);
+}  /* sw_port_w16 */
+
+#ifdef CONFIG_KSZ_STP
+static u8 get_port_state(struct net_device *dev, struct net_device **br_dev)
+{
+	struct net_bridge_port *p;
+	u8 state;
+
+	/* This state is not defined in kernel. */
+	state = STP_STATE_SIMPLE;
+	if (br_port_exists(dev)) {
+		p = br_port_get_rcu(dev);
+		state = p->state;
+
+		/* Port is under bridge. */
+		*br_dev = p->br->dev;
+	}
+	return state;
+}  /* get_port_state */
+#endif
+
+static void link_update_work(struct work_struct *work)
+{
+	struct ksz_port *port =
+		container_of(work, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct phy_device *phydev;
+	int i;
+	int link;
+
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ops->link_change(dlr,
+			sw->port_info[dlr->ports[0]].state == media_connected,
+			sw->port_info[dlr->ports[1]].state == media_connected);
+	}
+#endif
+
+	/* This only matters when one phy device is used for the switch. */
+	if (1 == sw->dev_count) {
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		for (i = 0; i < sw->mib_port_cnt; i++) {
+			if (i == sw->HOST_PORT)
+				continue;
+			if (port->linked == &sw->port_info[i]) {
+				hw_priv->phy_id = i + 1;
+				break;
+			}
+		}
+	}
+	for (i = 0; i < sw->phy_port_cnt; i++) {
+		struct ksz_port_info *info = &sw->port_info[i];
+
+		if (!info->report)
+			continue;
+		info->report = false;
+		phydev = sw->phy[i + 1];
+		if (i == sw->HOST_PORT)
+			phydev = sw->phydev;
+		phydev->link = (info->state == media_connected);
+		phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		phydev->duplex = (info->duplex == 2);
+		if (phydev->attached_dev) {
+			link = netif_carrier_ok(phydev->attached_dev);
+			if (link != phydev->link) {
+				if (phydev->link)
+					netif_carrier_on(phydev->attached_dev);
+				else
+					netif_carrier_off(phydev->attached_dev);
+				if (netif_msg_link(sw))
+					pr_info("%s link %s\n",
+						phydev->attached_dev->name,
+						phydev->link ? "on" : "off");
+			}
+			if (phydev->adjust_link)
+				phydev->adjust_link(phydev->attached_dev);
+		}
+	}
+
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->HOST_PORT >= sw->phy_port_cnt) {
+		struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+		int phy_link;
+
+		phydev = sw->phydev;
+		phydev->link = (info->state == media_connected);
+		phy_link = phydev->link;
+		if (!sw->need_link_up &&
+		    (1 == sw->dev_count || 1 == sw->dev_offset))
+			phy_link = (port->linked->state == media_connected);
+		phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		phydev->duplex = (info->duplex == 2);
+		if (phydev->attached_dev) {
+			link = netif_carrier_ok(phydev->attached_dev);
+			if (link != phy_link) {
+				if (phy_link)
+					netif_carrier_on(phydev->attached_dev);
+				else
+					netif_carrier_off(phydev->attached_dev);
+				if (netif_msg_link(sw))
+					pr_info("%s link %s\n",
+						phydev->attached_dev->name,
+						phy_link ? "on" : "off");
+			}
+			if (phydev->adjust_link)
+				phydev->adjust_link(phydev->attached_dev);
+		}
+	}
+}  /* link_update_work */
+
+static void sw_dis_intr(struct ksz_sw *sw);
+static void sw_ena_intr(struct ksz_sw *sw);
+
+#define USE_DIFF_PORT_PRIORITY
+#include "ksz_sw_9897.c"
+
+static void sw_dis_intr(struct ksz_sw *sw)
+{
+	HW_W32(sw->dev, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	HW_W32(sw->dev, REG_SW_PORT_INT_MASK__4, sw->PORT_MASK);
+}  /* sw_dis_intr */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	int i;
+
+	HW_W32(sw->dev, REG_SW_INT_MASK__4,
+		~sw->intr_mask & SWITCH_INT_MASK);
+	HW_W32(sw->dev, REG_SW_PORT_INT_MASK__4,
+		~sw->port_intr_mask & sw->PORT_MASK);
+	for (i = 0; i < sw->mib_port_cnt; i++)
+		port_w(sw, i, REG_PORT_INT_MASK,
+			~sw->info->port_cfg[i].intr_mask & PORT_INT_MASK);
+}  /* sw_ena_intr */
+
+/* -------------------------------------------------------------------------- */
+
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *ks = seq->private;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&ks->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = HW_R(ks, i + j * SW_SIZE);
+		mutex_unlock(&ks->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, "\n");
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @ks:		The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void __devinit create_debugfs(struct sw_priv *ks)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "i2c_%s",
+		 dev_name(ks->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root\n");
+		return;
+	}
+
+	ks->debug_root = root;
+	ks->debug_file = debugfs_create_file("state", 0444, root,
+		ks, &state_fops);
+	if (IS_ERR(ks->debug_file))
+		pr_err("cannot create debugfs state file\n");
+}
+
+static void __devexit delete_debugfs(struct sw_priv *ks)
+{
+	debugfs_remove(ks->debug_file);
+	debugfs_remove(ks->debug_root);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs_9897.c"
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_sysfs.c"
+#endif
+#ifdef KSZ_DLR
+#include "ksz_dlr_sysfs.c"
+#endif
+
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct phy_device *phydev = phy_dat;
+	struct sw_priv *ks = phydev->bus->priv;
+
+	if (IRQF_TRIGGER_LOW == ks->intr_mode)
+		disable_irq_nosync(irq);
+	atomic_inc(&phydev->irq_disable);
+	ks->sw.intr_using = 1;
+	schedule_work(&phydev->phy_queue);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct phy_device *phydev =
+		container_of(work, struct phy_device, phy_queue);
+	struct sw_priv *ks = phydev->bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+
+	ks->intr_working |= 1;
+	if (ks->sw.info->port_cfg[0].intr_mask & PORT_PHY_INT)
+		ks->intr_working |= 2;
+	mutex_lock(&sw->lock);
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	sw_proc_intr(&ks->sw);
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	mutex_unlock(&ks->hwlock);
+	mutex_unlock(&sw->lock);
+	sw->intr_using = 0;
+
+	atomic_dec(&phydev->irq_disable);
+	if (IRQF_TRIGGER_LOW == ks->intr_mode)
+		enable_irq(ks->irq);
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	struct phy_device *phydev = ks->phydev;
+	int err = 0;
+
+	INIT_WORK(&phydev->phy_queue, sw_change);
+
+	atomic_set(&phydev->irq_disable, 0);
+	if (request_irq(ks->irq, sw_interrupt, ks->intr_mode, name,
+			phydev) < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)\n",
+			phydev->bus->name,
+			ks->irq);
+		phydev->irq = PHY_POLL;
+		return 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	struct phy_device *phydev = ks->phydev;
+
+	free_irq(ks->irq, phydev);
+	cancel_work_sync(&phydev->phy_queue);
+	while (atomic_dec_return(&phydev->irq_disable) >= 0)
+		enable_irq(ks->irq);
+}  /* sw_stop_interrupt */
+
+#define KSZ9897_SW_ID		0x9897
+#define PHY_ID_KSZ_SW		((KSZ9897_ID_HI << 16) | KSZ9897_SW_ID)
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver = {
+	.phy_id		= PHY_ID_KSZ_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Micrel KSZ9897 Switch",
+	.features	= (PHY_GBIT_FEATURES |
+				SUPPORTED_Pause | SUPPORTED_Asym_Pause),
+	.flags		= PHY_HAS_MAGICANEG | PHY_HAS_INTERRUPT,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.driver		= { .owner = THIS_MODULE, },
+};
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy_id, u16 phy, u16 reg, u16 *val)
+{
+	u16 data;
+	u16 ret;
+	int id = phy;
+
+	if (0 == phy || phy > sw->phy_port_cnt)
+		phy = phy_id;
+	sw_port_r16(sw, phy - 1, P_PHY_CTRL + reg * 2, &data);
+	ret = data;
+
+	/* Use unique switch id to differentiate from regular PHY. */
+	if (3 == reg)
+		ret = KSZ9897_SW_ID;
+	if (0 == id || id > sw->phy_port_cnt) {
+		switch (reg) {
+		case 0:
+			ret = 0x1140;
+			break;
+		case 1:
+			ret = 0x796d;
+			break;
+		case 4:
+			ret = 0x05e1;
+			break;
+		case 5:
+			ret = 0xc5e1;
+			break;
+		case 9:
+			ret = 0x0700;
+			break;
+		case 10:
+			if (0 == id)
+				id = sw->HOST_PORT;
+			if (sw->port_info[id].tx_rate >= 1000 * TX_RATE_UNIT)
+				ret = 0x7800;
+			else
+				ret = 0;
+			break;
+		}
+	}
+	if (1 == reg && !(ret & PORT_LINK_STATUS)) {
+		sw_port_r16(sw, phy - 1, P_SPEED_STATUS, &data);
+		if ((ret & PORT_AUTO_NEG_ACKNOWLEDGE) &&
+		    (data & (PORT_STAT_SPEED_1000MBIT |
+		    PORT_STAT_SPEED_100MBIT |
+		    PORT_STAT_SPEED_10MBIT)))
+			ret |= PORT_LINK_STATUS;
+	}
+	*val = ret;
+}  /* sw_r_phy */
+
+static int ksz_mii_addr(int *reg, int *bank)
+{
+	int ret;
+
+	ret = (*reg & 0xC000) >> ADDR_SHIFT;
+	*bank = (*reg & 0x3000) >> BANK_SHIFT;
+	*reg &= 0x0FFF;
+	return ret;
+}
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int addr;
+	int bank;
+	int ret = 0xffff;
+
+	if (phy_id > sw->mib_port_cnt)
+		return 0xffff;
+
+	addr = ksz_mii_addr(&regnum, &bank);
+
+	mutex_lock(&ks->lock);
+
+#ifdef KSZ_IBA
+	/* Indicate okay to use SPI when IBA is enabled. */
+	sw->info->iba.use_iba |= 0x40;
+#endif
+	switch (addr) {
+	case ADDR_8:
+		ret = HW_R8(ks, regnum);
+		break;
+	case ADDR_16:
+		ret = HW_R16(ks, regnum);
+		break;
+	case ADDR_32:
+		ret = HW_R32(ks, regnum);
+		break;
+	default:
+		if (regnum < 11) {
+			u16 data;
+
+			sw_r_phy(sw, ks->phy_id, phy_id, regnum, &data);
+			ret = data;
+		} else
+			ret = 0;
+	}
+#ifdef KSZ_IBA
+	sw->info->iba.use_iba &= ~0x40;
+#endif
+	mutex_unlock(&ks->lock);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	static int last_reg;
+	static int last_val;
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int addr;
+	int bank;
+	int reg;
+
+	if (phy_id > sw->mib_port_cnt)
+		return -EINVAL;
+
+	reg = regnum;
+	addr = ksz_mii_addr(&regnum, &bank);
+
+	mutex_lock(&ks->lock);
+
+#ifdef KSZ_IBA
+	/* Indicate okay to use SPI when IBA is enabled. */
+	sw->info->iba.use_iba |= 0x40;
+#endif
+	switch (addr) {
+	case ADDR_8:
+		HW_W8(ks, regnum, val);
+		break;
+	case ADDR_16:
+		HW_W16(ks, regnum, val);
+		break;
+	case ADDR_32:
+		/*
+		 * The phy_write interface allows only 16-bit value.  Break
+		 * the 32-bit write into two calls for I2C efficiency.
+		 */
+
+		/* Previous write to high word. */
+		if (last_reg == reg + 2) {
+			last_val <<= 16;
+			last_val |= val;
+			HW_W32(ks, regnum, last_val);
+			last_reg = 0;
+		} else {
+			/* Somebody has written to different address! */
+			if (last_reg) {
+				int last_bank;
+
+				addr = ksz_mii_addr(&last_reg, &last_bank);
+				HW_W16(ks, last_reg, last_val);
+				last_reg = 0;
+			}
+
+			/* Cache the 16-bit write to high word. */
+			if (reg & 3) {
+				last_reg = reg;
+				last_val = val;
+
+			/* Did not find the previous write to high word.*/
+			} else
+				HW_W16(ks, regnum, val);
+		}
+		break;
+	default:
+		if (regnum < 11) {
+			int i;
+
+			/* PHY device driver resets or powers down the PHY. */
+			if (0 == regnum &&
+			    (val & (PORT_PHY_RESET | PORT_POWER_DOWN)))
+				break;
+			for (i = 0; i < sw->phy_port_cnt; i++) {
+				if (i + 1 == phy_id || 0 == phy_id)
+					sw_port_w16(sw, i,
+						P_PHY_CTRL + regnum * 2, val);
+			}
+		}
+		break;
+	}
+#ifdef KSZ_IBA
+	sw->info->iba.use_iba &= ~0x40;
+#endif
+	mutex_unlock(&ks->lock);
+	return 0;
+}  /* ksz_mii_write */
+
+static int __devinit ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	int err;
+	int i;
+	int driver_installed = false;
+
+	pdev = platform_device_register_simple("Switch MII bus", ks->sw.id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	err = phy_driver_register(&kszsw_phy_driver);
+	if (err)
+		goto mii_init_free_mii_bus;
+	driver_installed = true;
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.mib_port_cnt + 1)) - 1);
+	bus->priv = ks;
+	bus->irq = ks->bus_irqs;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = ks->irq;
+
+	ks->phy_id = 1;
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	if (!bus->phy_map[0]) {
+		printk(KERN_WARNING "No PHY detected\n");
+		mdiobus_unregister(bus);
+		err = -ENODEV;
+		goto mii_init_free_mii_bus;
+	}
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i]) {
+			struct phy_priv *phydata;
+
+			phydata = kzalloc(sizeof(struct phy_priv), GFP_KERNEL);
+			if (!phydata) {
+				err = -ENOMEM;
+				goto mii_init_free_mii_bus;
+			}
+			phydata->port.sw = &ks->sw;
+			INIT_WORK(&phydata->port.link_update, link_update_work);
+			bus->phy_map[i]->priv = phydata;
+		}
+
+	ks->bus = bus;
+	ks->pdev = pdev;
+	ks->phydev = bus->phy_map[0];
+	ks->phydev->interface = ks->sw.interface;
+
+	/* The switch is always linked; speed and duplex are also fixed. */
+	do {
+		struct ksz_port_info *info =
+			&ks->sw.port_info[ks->sw.HOST_PORT];
+
+		ks->phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		ks->phydev->duplex = (info->duplex == 2);
+		ks->phydev->pause = 1;
+	} while (0);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i])
+			kfree(bus->phy_map[i]->priv);
+	if (driver_installed)
+		phy_driver_unregister(&kszsw_phy_driver);
+	mdiobus_free(bus);
+
+mii_init_reg:
+	platform_device_unregister(pdev);
+
+	return err;
+}  /* ksz_mii_init */
+
+static void __devexit ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct platform_device *pdev = ks->pdev;
+	struct mii_bus *bus = ks->bus;
+
+	if (ks->irq > 0) {
+		mutex_lock(&ks->lock);
+#if 1
+		HW_W32(ks, REG_SW_INT_MASK__4, 0xffffffff);
+		HW_W32(ks, REG_SW_PORT_INT_MASK__4, 0xffffffff);
+#endif
+		mutex_unlock(&ks->lock);
+		sw_stop_interrupt(ks);
+	}
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i])
+			kfree(bus->phy_map[i]->priv);
+	mdiobus_unregister(bus);
+	phy_driver_unregister(&kszsw_phy_driver);
+	mdiobus_free(bus);
+	platform_device_unregister(pdev);
+}  /* ksz_mii_exit */
+
+/* driver bus management functions */
+
+static void ksz9897_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	int i;
+	int cnt = 0;
+
+	/* Find out how many ports are connected. */
+	for (i = 0; i < sw->mib_port_cnt; i++)
+		if (media_connected == sw->port_state[i].state)
+			++cnt;
+	if (!cnt)
+		cnt++;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += MIB_READ_INTERVAL * cnt;
+	}
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		mib = &sw->port_mib[i];
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[i].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, i))
+				return;
+			hw_priv->counter[i].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				hw_priv->counter[i].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[i].counter);
+			}
+		} else if (time_after_eq(jiffies, hw_priv->counter[i].time)) {
+			hw_priv->counter[i].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[i].state) {
+				hw_priv->counter[i].read = 1;
+				sw->next_jiffies += MIB_READ_INTERVAL;
+			}
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[i].link_down) {
+			sw->port_state[i].link_down = 0;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[i].read = 1;
+		}
+	}
+}  /* ksz9897_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct phy_device *phydev;
+	struct ksz_port *port = NULL;
+	int i;
+	int changes = 0;
+	int s = 1;
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+	sw->ops->acquire(sw);
+	for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+		struct phy_priv *phydata;
+		struct net_device *dev = sw->netdev[i];
+
+		phydev = sw->phy[i + s];
+		phydata = phydev->priv;
+		if (dev && sw->net_ops->get_priv_port)
+			port = sw->net_ops->get_priv_port(dev);
+		else
+			port = &phydata->port;
+		changes |= port_get_link_speed(port);
+
+		/* Copy all port information for user access. */
+		if (port != &phydata->port) {
+			copy_port_status(port, &phydata->port);
+			if (phydata != hw_priv->phydev->priv) {
+				phydata = hw_priv->phydev->priv;
+				copy_port_status(port, &phydata->port);
+			}
+		}
+	}
+	sw->ops->release(sw);
+
+	/* Not to read PHY registers unnecessarily if no link change. */
+	if (!changes)
+		return;
+}  /* link_read_work */
+
+static void stp_work(struct work_struct *work)
+{
+#ifdef CONFIG_KSZ_STP
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, stp_monitor);
+	struct ksz_sw *sw = &hw_priv->sw;
+
+	sw->net_ops->monitor_ports(sw);
+#endif
+}  /* stp_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz9897_mib_monitor(unsigned long ptr)
+{
+	struct sw_priv *hw_priv = (struct sw_priv *) ptr;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz9897_mib_monitor */
+
+static void ksz9897_dev_monitor(unsigned long ptr)
+{
+	struct sw_priv *hw_priv = (struct sw_priv *) ptr;
+
+#if 1
+	if ((hw_priv->intr_working & 2) &&
+	    !(hw_priv->sw.features & STP_SUPPORT))
+		return;
+	if (!(hw_priv->intr_working & 2))
+#endif
+		schedule_delayed_work(&hw_priv->link_read, 0);
+	if (hw_priv->sw.features & STP_SUPPORT)
+		schedule_delayed_work(&hw_priv->stp_monitor, 0);
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz9897_dev_monitor */
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+#include "ksz_dsa.c"
+#endif
+
+static int intr_mode;
+static int need_link_up;
+static int sw_host_port;
+
+#define MAX_I2C_DEVICES		1
+
+static int sw_device_present;
+
+static int __devinit ksz9897_probe(struct i2c_client *i2c,
+	const struct i2c_device_id *i2c_id)
+{
+	struct i2c_hw_priv *hw_priv;
+	struct sw_priv *ks;
+	struct ksz_sw *sw;
+	struct ksz_port *port;
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	u32 id;
+	u32 id1;
+	u32 id2;
+	int cnt;
+	int i;
+	int mib_port_count;
+	int phy_port_count;
+	int pi;
+	int port_count;
+	int ret;
+
+	ks = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!ks)
+		return -ENOMEM;
+
+	ks->hw_dev = kzalloc(sizeof(struct i2c_hw_priv), GFP_KERNEL);
+	if (!ks->hw_dev) {
+		kfree(ks);
+		return -ENOMEM;
+	}
+	hw_priv = ks->hw_dev;
+
+	hw_priv->i2cdev = i2c;
+
+	ks->intr_mode = intr_mode ? IRQF_TRIGGER_FALLING :
+		IRQF_TRIGGER_LOW;
+	ks->irq = i2c->irq;
+	ks->dev = &i2c->dev;
+
+	dev_set_drvdata(ks->dev, ks);
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	/* simple check for a valid chip being connected to the bus */
+	mutex_lock(&ks->lock);
+	id = HW_R32(ks, REG_CHIP_ID0__1);
+	mutex_unlock(&ks->lock);
+	id1 = id;
+	id1 >>= 8;
+	id1 &= 0xffff;
+	id2 = id1 & 0xff;
+	id1 >>= 8;
+dbg_msg("%02x %02x\n", id1, id2);
+	if (id1 != FAMILY_ID_95 && id1 != FAMILY_ID_98 &&
+	    id1 != FAMILY_ID_94 && id1 != FAMILY_ID_85 && id1 != 0x64) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)\n", id);
+		ret = -ENODEV;
+		goto err_sw;
+	}
+	dev_info(ks->dev, "chip id 0x%08x\n", id);
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	mutex_init(&sw->acllock);
+	mutex_init(&sw->alulock);
+	mutex_init(&sw->vlanlock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+
+	port_count = 1;
+	mib_port_count = 1;
+	phy_port_count = 1;
+#ifdef CONFIG_1588_PTP
+	if ((FAMILY_ID_95 & 0x0f) == (id1 & 0x0f)) {
+		sw->features |= PTP_HW;
+		sw->features |= ACL_CORRUPT_BUG;
+	}
+	if (0x64 == id1) {
+		sw->features |= PTP_HW;
+#if 1
+		sw->features |= SETUP_PHY;
+#endif
+	}
+#endif
+	if ((FAMILY_ID_85 & 0xf0) == (id1 & 0xf0))
+		sw->features |= QW_HW;
+	sw->features |= NO_GLOBAL_RESET;
+
+	/* Check for S2 revision. */
+	mutex_lock(&ks->lock);
+	id = HW_R(ks, REG_GLOBAL_OPTIONS);
+	mutex_unlock(&ks->lock);
+	if (id) {
+		sw->features &= ~ACL_CORRUPT_BUG;
+		sw->features &= ~SETUP_PHY;
+		sw->features &= ~NO_GLOBAL_RESET;
+		sw->features |= NEW_CAP;
+		if (id & SW_AVB_ABLE) {
+			sw->features |= AVB_SUPPORT;
+			sw->features |= PTP_HW;
+		}
+		if (id & SW_REDUNDANCY_ABLE) {
+			sw->features |= REDUNDANCY_SUPPORT;
+#ifdef KSZ_DLR
+			sw->features |= DLR_HW;
+#endif
+			sw->features |= HSR_HW;
+		}
+dbg_msg("avb=%d  rr=%d  giga=%d\n",
+!!(id & SW_AVB_ABLE), !!(id & SW_REDUNDANCY_ABLE), !!(id & SW_GIGABIT_ABLE));
+	}
+	if ((CHIP_ID_67 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 7;
+		mib_port_count = 7;
+		phy_port_count = 5;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+	} else if ((CHIP_ID_66 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 6;
+		mib_port_count = 6;
+		phy_port_count = 5;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+	} else if ((CHIP_ID_63 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 3;
+		mib_port_count = 3;
+		phy_port_count = 2;
+		sw->TAIL_TAG_LOOKUP = (1 << (3 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (3 + 2));
+	}
+dbg_msg("port: %x %x %x\n", port_count, mib_port_count, phy_port_count);
+
+	sw->dev_count = 1;
+
+	sw->PORT_MASK = (1 << (mib_port_count + 0)) - 1;
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = mib_port_count;
+	sw->phy_port_cnt = phy_port_count;
+	sw->port_cnt = mib_port_count;
+	if (sw_host_port && sw_host_port <= port_count)
+		sw->HOST_PORT = sw_host_port - 1;
+	else
+		sw->HOST_PORT = port_count - 1;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+dbg_msg("mask: %x %x; %x %x\n", sw->HOST_MASK, sw->PORT_MASK,
+sw->TAIL_TAG_LOOKUP, sw->TAIL_TAG_OVERRIDE);
+
+	/* Last port is the host port. */
+	if (sw->HOST_PORT == port_count - 1 || !sw->HOST_PORT) {
+		port_count = port_count - 1;
+		mib_port_count = mib_port_count - 1;
+	}
+
+	sw->dev = ks;
+	sw->id = sw_device_present;
+
+	sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+	if (!sw->info) {
+		ret = -ENOMEM;
+		goto err_sw;
+	}
+
+	sw->reg = &sw_reg_ops;
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+
+	init_waitqueue_head(&sw->queue);
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+	INIT_DELAYED_WORK(&ks->stp_monitor, stp_work);
+
+	for (cnt = 0, pi = 0; cnt < phy_port_count; cnt++, pi++) {
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		sw->port_info[pi].link = 0xFF;
+		sw->port_info[pi].state = media_disconnected;
+		sw->port_info[pi].report = true;
+	}
+	sw->interface = PHY_INTERFACE_MODE_MII;
+	mutex_lock(&ks->lock);
+	for (; cnt < sw->mib_port_cnt; cnt++, pi++) {
+		u16 data;
+		u16 orig;
+		u8 *data_lo;
+		u8 *data_hi;
+		int speed;
+		phy_interface_t phy;
+		struct ksz_port_info *info = &sw->port_info[pi];
+		int gbit;
+		int mode;
+
+		port_r16(sw, pi, REG_PORT_XMII_CTRL_0, &data);
+		orig = data;
+		data_hi = (u8 *) &data;
+		data_lo = data_hi + 1;
+
+/**
+ * THa  2015/08/27
+ * Port 6 or 7 may never start transmiting and cause flow control problem in
+ * the receive port.
+ * Not guaranteed to work all the time.
+ */
+		if (sw->features & NEW_CAP) {
+			*data_hi ^= (PORT_MII_NOT_1GBIT | PORT_MII_MAC_MODE |
+				PORT_MII_SEL_M);
+			port_w8(sw, pi, REG_PORT_XMII_CTRL_1, *data_hi);
+			data = orig;
+			port_w8(sw, pi, REG_PORT_XMII_CTRL_1, *data_hi);
+		}
+#ifdef USE_10_MBIT_MODE
+		*data_lo &= ~PORT_MII_100MBIT;
+#endif
+#ifdef USE_HALF_DUPLEX
+		*data_lo &= ~PORT_MII_FULL_DUPLEX;
+#endif
+#ifdef USE_RGMII_MODE
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+#endif
+#ifdef USE_GMII_100_MODE
+		sw_set_gbit(sw, false, data_hi);
+#endif
+#ifdef USE_MII_MODE
+		sw_set_xmii(sw, 0, data_hi);
+#endif
+#ifdef USE_GMII_MODE
+		sw_set_xmii(sw, 2, data_hi);
+#endif
+#ifdef USE_RMII_MODE
+		sw_set_xmii(sw, 1, data_hi);
+#endif
+/* Strap options may not valid after reset. */
+#if 1
+if (PORT_RMII_SEL == (*data_hi & PORT_MII_SEL_M)) {
+dbg_msg("?%02x\n", *data_hi);
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+}
+#endif
+		gbit = sw_get_gbit(sw, *data_hi);
+		mode = sw_get_xmii(sw, *data_hi);
+		switch (mode) {
+		case 2:
+			phy = PHY_INTERFACE_MODE_GMII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 1000;
+			if (gbit)
+				break;
+		case 0:
+			phy = PHY_INTERFACE_MODE_MII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			break;
+		case 1:
+			phy = PHY_INTERFACE_MODE_RMII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			break;
+		default:
+			phy = PHY_INTERFACE_MODE_RGMII;
+			if (*data_hi & PORT_RGMII_ID_IG_ENABLE)
+				phy = PHY_INTERFACE_MODE_RGMII_RXID;
+			if (*data_hi & PORT_RGMII_ID_EG_ENABLE) {
+				if (PHY_INTERFACE_MODE_RGMII_RXID == phy)
+					phy = PHY_INTERFACE_MODE_RGMII_ID;
+				else
+					phy = PHY_INTERFACE_MODE_RGMII_TXID;
+			}
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			if (gbit)
+				speed = 1000;
+			break;
+		}
+		if (sw->HOST_PORT == pi)
+dbg_msg("host: %d %d\n", sw->HOST_PORT, sw->interface);
+		info->state = media_connected;
+		if (!(*data_lo & PORT_MII_100MBIT))
+			info->tx_rate = 10 * TX_RATE_UNIT;
+		else
+			info->tx_rate = speed * TX_RATE_UNIT;
+		if (*data_lo & PORT_MII_FULL_DUPLEX)
+			info->duplex = 2;
+		else
+			info->duplex = 1;
+		info->flow_ctrl = 0x33;
+dbg_msg("xmii: %04x %02x %02x; %u %u\n", orig, *data_lo, *data_hi,
+info->tx_rate / TX_RATE_UNIT, info->duplex);
+	}
+	mutex_unlock(&ks->lock);
+
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+
+#ifdef KSZ_IBA
+	sw->need_link_up = need_link_up;
+#endif
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+
+	sw->phydev = ks->phydev;
+	sw->counter = ks->counter;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+	sw->stp_monitor = &ks->stp_monitor;
+
+	sw_setup_mib(sw);
+	sw_init_mib(sw);
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+	create_debugfs(ks);
+
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_init(&sw->info->dlr, sw);
+#endif
+	sw->ops->acquire(sw);
+
+	/* Turn off PTP in case the feature is not enabled. */
+	sw_w16(sw, REG_PTP_MSG_CONF1, 0);
+
+#if 1
+	/* Turn off IBA first for KSZ9893. */
+	if (sw->features & SETUP_PHY)
+		sw_w8(sw, REG_SW_IBA__4, 0);
+#endif
+	sw_reset(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw->ops->release(sw);
+
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		init_dlr_sysfs(ks->dev);
+#endif
+#endif
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+	sema_init(&ks->proc_sem, 1);
+
+	for (i = 0; i <= sw->mib_port_cnt; i++) {
+		sw->phy[i] = ks->bus->phy_map[i];
+		phydev = sw->phy[i];
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		port = &priv->port;
+		port->port_cnt = port_count;
+		port->mib_port_cnt = mib_port_count;
+		port->first_port = 0;
+		port->flow_ctrl = PHY_TX_ONLY;
+
+		port->linked = &sw->port_info[port->first_port];
+	}
+#ifdef NO_ATTACHED_DEV
+	sw->ops->acquire(sw);
+	phydev = sw->phydev;
+	priv = phydev->priv;
+	port = &priv->port;
+	port_set_link_speed(port);
+	sw->ops->release(sw);
+#endif
+
+	INIT_WORK(&sw->set_addr, sw_delayed_set_addr);
+
+#ifdef KSZ_IBA
+	ksz_iba_init(&sw->info->iba, sw);
+	INIT_DELAYED_WORK(&sw->set_ops, sw_set_ops);
+#endif
+
+	INIT_WORK(&ks->mib_read, ksz9897_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz9897_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz9897_dev_monitor, ks);
+
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!sw->multi_dev && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period);
+
+	sw_device_present++;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ports = sw->mib_port_cnt - 0;
+		ptp->reg = &ptp_reg_ops;
+		ptp->ops = &ptp_ops;
+		ptp->parent = ks->dev;
+#ifdef NO_ATTACHED_DEV
+		ptp->ops->init(ptp, sw->info->mac_addr);
+		ptp->reg->start(ptp, true);
+#endif
+		init_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+	}
+#endif
+#ifdef KSZ_MRP
+	sw->features |= MRP_SUPPORT;
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops = &mrp_ops;
+		mrp->ops->init(mrp);
+	}
+#endif
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+	ksz_dsa_init();
+#endif
+
+	if (ks->irq <= 0)
+		return 0;
+	mutex_lock(&ks->lock);
+	HW_W32(ks, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	HW_W32(ks, REG_SW_PORT_INT_MASK__4, sw->PORT_MASK);
+	sw_w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	for (i = 0; i < sw->phy_port_cnt; i++)
+		port_w8(sw, i, REG_PORT_PHY_INT_ENABLE, 0);
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		port_w(sw, i, REG_PORT_INT_MASK, 0xff);
+		port_w16(sw, i, REG_PTP_PORT_TX_INT_STATUS__2, 0xffff);
+	}
+	mutex_unlock(&ks->lock);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0)
+		printk(KERN_WARNING "No switch interrupt\n");
+	else {
+		mutex_lock(&ks->lock);
+		sw_ena_intr(sw);
+		mutex_unlock(&ks->lock);
+	}
+
+	return 0;
+
+err_mii:
+	kfree(sw->info);
+
+err_sw:
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return ret;
+}
+
+static int __devexit ksz9897_remove(struct i2c_client *i2c)
+{
+	struct sw_priv *ks = dev_get_drvdata(&i2c->dev);
+	struct ksz_sw *sw = &ks->sw;
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+	ksz_dsa_cleanup();
+#endif
+#ifdef NO_ATTACHED_DEV
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->stop(ptp);
+		ptp->ops->exit(ptp);
+	}
+#endif
+#endif
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		exit_dlr_sysfs(ks->dev);
+#endif
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		exit_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+#endif
+	exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+	cancel_delayed_work_sync(&ks->link_read);
+	cancel_delayed_work_sync(&ks->stp_monitor);
+	delete_debugfs(ks);
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_exit(&sw->info->dlr);
+#endif
+#ifdef KSZ_IBA
+	ksz_iba_exit(&sw->info->iba);
+#endif
+	kfree(sw->info);
+	ksz_mii_exit(ks);
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return 0;
+}
+
+#define I2C_SWITCH_NAME			"i2c-ksz9897"
+
+/* Please change the I2C address if necessary. */
+#define I2C_SWITCH_ADDR			0x5F
+
+/* Please provide a system interrupt number here. */
+#define I2C_SWITCH_INTR			-1
+
+static int ksz9897_detect(struct i2c_client *i2c, struct i2c_board_info *info)
+{
+	strncpy(info->type, I2C_SWITCH_NAME, I2C_NAME_SIZE);
+	info->irq = I2C_SWITCH_INTR;
+	return 0;
+}
+
+static unsigned short i2c_address_list[] = {
+	I2C_SWITCH_ADDR,
+
+	I2C_CLIENT_END
+};
+
+static const struct i2c_device_id i2c_id[] = {
+	{ I2C_SWITCH_NAME, 0 },
+	{ },
+};
+
+static struct i2c_driver ksz9897_driver = {
+	.driver.name	= I2C_SWITCH_NAME,
+	.probe		= ksz9897_probe,
+	.remove		= ksz9897_remove,
+	.id_table	= i2c_id,
+
+	/* Big enough to be accepted in all cases. */
+	.class		= 0xffff,
+	.detect		= ksz9897_detect,
+	.address_list	= i2c_address_list,
+};
+
+static int __init ksz9897_init(void)
+{
+	int ret;
+
+	ret = i2c_add_driver(&ksz9897_driver);
+	if (ret)
+		return ret;
+
+	/* Probe not called. */
+	if (!sw_device_present) {
+		struct i2c_adapter *adap;
+
+		/* Assume I2C bus starts at 0. */
+		adap = i2c_get_adapter(0);
+
+		/* I2C master may not be created yet. */
+		if (!adap) {
+#if !defined(CONFIG_I2C_KSZ9897_MODULE)
+			struct i2c_board_info info = {
+				.type	= I2C_SWITCH_NAME,
+				.addr	= I2C_SWITCH_ADDR,
+				.irq	= I2C_SWITCH_INTR,
+			};
+
+			ret = i2c_register_board_info(0, &info, 1);
+#else
+			return -ENODEV;
+#endif
+		} else
+			i2c_put_adapter(adap);
+	}
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	return ret;
+}
+
+static void __exit ksz9897_exit(void)
+{
+	i2c_del_driver(&ksz9897_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_MICREL_KSZ9897_EMBEDDED
+module_init(ksz9897_init);
+module_exit(ksz9897_exit);
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+
+#ifdef KSZ_IBA
+module_param(iba, int, 0);
+MODULE_PARM_DESC(iba, "IBA support");
+#endif
+#endif
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(0=level low, 1=falling)");
+
+module_param(need_link_up, int, 0);
+MODULE_PARM_DESC(need_link_up,
+	"Configure whether to always indicate link is up");
+
+module_param(sw_host_port, int, 0);
+MODULE_PARM_DESC(sw_host_port,
+	"Configure switch host port");
+
+#ifndef CONFIG_MICREL_KSZ9897_EMBEDDED
+MODULE_DESCRIPTION("Micrel KSZ9897 I2C Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("i2c:ksz9897");
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897.h b/drivers/net/ethernet/micrel/ksz9897.h
new file mode 100644
index 0000000..57bc142
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897.h
@@ -0,0 +1,1491 @@
+/**
+ * Micrel KSZ9897 definition file
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ9897_H
+#define __KSZ9897_H
+
+
+#define KS_PRIO_M			0x7
+#define KS_PRIO_S			4
+
+
+/* 0 - Operation */
+#define REG_CHIP_ID0__1			0x0000
+
+#define REG_CHIP_ID1__1			0x0001
+
+#define FAMILY_ID			0x95
+#define FAMILY_ID_94			0x94
+#define FAMILY_ID_95			0x95
+#define FAMILY_ID_85			0x85
+#define FAMILY_ID_98			0x98
+#define FAMILY_ID_88			0x88
+
+#define REG_CHIP_ID2__1			0x0002
+
+#define CHIP_ID_63			0x63
+#define CHIP_ID_66			0x66
+#define CHIP_ID_67			0x67
+#define CHIP_ID_77			0x77
+#define CHIP_ID_93			0x93
+#define CHIP_ID_96			0x96
+#define CHIP_ID_97			0x97
+
+#define REG_CHIP_ID3__1			0x0003
+
+#define SWITCH_REVISION_M		0x0F
+#define SWITCH_REVISION_S		4
+#define SWITCH_RESET			0x01
+
+#define REG_SW_PME_CTRL			0x0006
+
+#define PME_ENABLE			(1 << 1)
+#define PME_POLARITY			(1 << 0)
+
+#define REG_GLOBAL_OPTIONS		0x000F
+
+#define SW_GIGABIT_ABLE			(1 << 6)
+#define SW_REDUNDANCY_ABLE		(1 << 5)
+#define SW_AVB_ABLE			(1 << 4)
+
+#define REG_SW_INT_STATUS__4		0x0010
+#define REG_SW_INT_MASK__4		0x0014
+
+#define LUE_INT				(1 << 31)
+#define TRIG_TS_INT			(1 << 30)
+#define APB_TIMEOUT_INT			(1 << 29)
+
+#define SWITCH_INT_MASK			\
+	(TRIG_TS_INT | APB_TIMEOUT_INT)
+
+#define REG_SW_PORT_INT_STATUS__4	0x0018
+#define REG_SW_PORT_INT_MASK__4		0x001C
+#define REG_SW_PHY_INT_STATUS		0x0020
+#define REG_SW_PHY_INT_ENABLE		0x0024
+
+/* 1 - */
+#define REG_SW_IBA__4			0x0104
+
+#define SW_IBA_ENABLE			(1 << 31)
+#define SW_IBA_DA_MATCH			(1 << 30)
+#define SW_IBA_INIT			(1 << 29)
+#define SW_IBA_QID_S			22
+#define SW_IBA_QID_M			0xF
+#define SW_IBA_PORT_S			16
+#define SW_IBA_PORT_M			0x2F
+#define SW_IBA_FRAME_TPID_M		0xFFFF
+
+#define REG_SW_APB_TIMEOUT_ADDR__4	0x0108
+
+#define APB_TIMEOUT_ACKNOWLEDGE		(1 << 31)
+
+#define REG_SW_IBA_SYNC__1		0x010C
+
+#define REG_SW_IO_STRENGTH__1		0x010D
+
+#define REG_SW_IBA_STATUS__4		0x0110
+
+#define SW_IBA_REQ			(1 << 31)
+#define SW_IBA_RESP			(1 << 30)
+#define SW_IBA_DA_MISMATCH		(1 << 14)
+#define SW_IBA_FMT_MISMATCH		(1 << 13)
+#define SW_IBA_CODE_ERROR		(1 << 12)
+#define SW_IBA_CMD_ERROR		(1 << 11)
+#define SW_IBA_CMD_LOC_M		((1 << 6) - 1)
+
+#define REG_SW_IBA_STATES__4		0x0114
+
+#define SW_IBA_BUF_STATE_S		30
+#define SW_IBA_CMD_STATE_S		28
+#define SW_IBA_RESP_STATE_S		26
+#define SW_IBA_STATE_M			0x3
+#define SW_IBA_PACKET_SIZE_S		16
+#define SW_IBA_PACKET_SIZE_M		0x7F
+#define SW_IBA_FMT_ID_M			0xFFFF
+
+#define REG_SW_IBA_RESULT__4		0x0118
+
+#define SW_IBA_SIZE_S			24
+
+#define SW_IBA_RETRY_CNT_M		((1 << 5) - 1)
+
+
+/* 2 - PHY */
+#define REG_SW_POWER_MANAGEMENT_CTRL	0x0201
+
+#define SW_PLL_POWER_DOWN		(1 << 5)
+#define SW_POWER_DOWN_MODE		0x3
+#define SW_ENERGY_DETECTION		1
+#define SW_SOFT_POWER_DOWN		2
+#define SW_POWER_SAVING			3
+
+/* 3 - Operation Control */
+#define REG_SW_OPERATION		0x0300
+
+#define SW_DOUBLE_TAG			(1 << 7)
+#define SW_RESET			(1 << 1)
+#define SW_START			(1 << 0)
+
+#define REG_SW_MAC_ADDR_0		0x0302
+#define REG_SW_MAC_ADDR_1		0x0303
+#define REG_SW_MAC_ADDR_2		0x0304
+#define REG_SW_MAC_ADDR_3		0x0305
+#define REG_SW_MAC_ADDR_4		0x0306
+#define REG_SW_MAC_ADDR_5		0x0307
+
+#define REG_SW_MTU__2			0x0308
+
+#define REG_SW_ISP_TPID__2		0x030A
+
+#define REG_SW_HSR_TPID__2		0x030C
+
+#define REG_AVB_STRATEGY__2		0x030E
+
+#define SW_SHAPING_CREDIT_ACCT		(1 << 1)
+#define SW_POLICING_CREDIT_ACCT		(1 << 0)
+
+#define REG_SW_LUE_CTRL_0		0x0310
+
+#define SW_VLAN_ENABLE			(1 << 7)
+#define SW_DROP_INVALID_VID		(1 << 6)
+#define SW_AGE_CNT_M			0x7
+#define SW_AGE_CNT_S			3
+#define SW_RESV_MCAST_ENABLE		(1 << 2)
+#define SW_HASH_OPTION_M		0x03
+#define SW_HASH_OPTION_CRC		1
+#define SW_HASH_OPTION_XOR		2
+#define SW_HASH_OPTION_DIRECT		3
+
+#define REG_SW_LUE_CTRL_1		0x0311
+
+#define UNICAST_LEARN_DISABLE		(1 << 7)
+#define SW_SRC_ADDR_FILTER		(1 << 6)
+#define SW_FLUSH_STP_TABLE		(1 << 5)
+#define SW_FLUSH_MSTP_TABLE		(1 << 4)
+#define SW_FWD_MCAST_SRC_ADDR		(1 << 3)
+#define SW_AGING_ENABLE			(1 << 2)
+#define SW_FAST_AGING			(1 << 1)
+#define SW_LINK_AUTO_AGING		(1 << 0)
+
+#define REG_SW_LUE_CTRL_2		0x0312
+
+#define SW_TRAP_DOUBLE_TAG		(1 << 6)
+#define SW_EGRESS_VLAN_FILTER_DYN	(1 << 5)
+#define SW_EGRESS_VLAN_FILTER_STA	(1 << 4)
+#define SW_FLUSH_OPTION_M		0x3
+#define SW_FLUSH_OPTION_S		2
+#define SW_FLUSH_OPTION_DYN_MAC		1
+#define SW_FLUSH_OPTION_STA_MAC		2
+#define SW_FLUSH_OPTION_BOTH		3
+#define SW_PRIO_M			0x3
+#define SW_PRIO_DA			0
+#define SW_PRIO_SA			1
+#define SW_PRIO_HIGHEST_DA_SA		2
+#define SW_PRIO_LOWEST_DA_SA		3
+
+#define REG_SW_LUE_CTRL_3		0x0313
+
+#define REG_SW_LUE_INT_STATUS		0x0314
+#define REG_SW_LUE_INT_ENABLE		0x0315
+
+#define LEARN_FAIL_INT			(1 << 2)
+#define ALMOST_FULL_INT			(1 << 1)
+#define WRITE_FAIL_INT			(1 << 0)
+
+#define REG_SW_LUE_INDEX_0__2		0x0316
+
+#define ENTRY_INDEX_M			0x0FFF
+
+#define REG_SW_LUE_INDEX_1__2		0x0318
+
+#define FAIL_INDEX_M			0x03FF
+
+#define REG_SW_LUE_INDEX_2__2		0x031A
+
+#define REG_SW_LUE_UNK_UCAST_CTRL__4	0x0320
+
+#define SW_UNK_UNICAST_ENABLE		(1 << 31)
+
+#define REG_SW_LUE_UNK_MCAST_CTRL__4	0x0324
+
+#define SW_UNK_MULTICAST_ENABLE		(1 << 31)
+
+#define REG_SW_LUE_UNK_VID_CTRL__4	0x0328
+
+#define SW_UNK_VID_ENABLE		(1 << 31)
+
+#define REG_SW_MAC_CTRL_0		0x0330
+
+#define SW_NEW_BACKOFF			(1 << 7)
+#define SW_CHECK_LENGTH			(1 << 3)
+#define SW_AGGR_BACKOFF			(1 << 0)
+
+#define REG_SW_MAC_CTRL_1		0x0331
+
+#define MULTICAST_STORM_DISABLE		(1 << 6)
+#define SW_BACK_PRESSURE		(1 << 5)
+#define FAIR_FLOW_CTRL			(1 << 4)
+#define NO_EXC_COLLISION_DROP		(1 << 3)
+#define SW_JUMBO_PACKET			(1 << 2)
+#define SW_LEGAL_PACKET_DISABLE		(1 << 1)
+#define SW_PASS_SHORT_FRAME		(1 << 0)
+
+#define REG_SW_MAC_CTRL_2		0x0332
+
+#define SW_REPLACE_VID			(1 << 3)
+#define BROADCAST_STORM_RATE_HI		0x07
+
+#define REG_SW_MAC_CTRL_3		0x0333
+
+#define BROADCAST_STORM_RATE_LO		0xFF
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SW_MAC_CTRL_4		0x0334
+
+#define SW_PASS_PAUSE			(1 << 3)
+
+#define REG_SW_MAC_CTRL_5		0x0335
+
+#define REG_SW_MAC_CTRL_6		0x0336
+
+#define SW_MIB_COUNTER_FLUSH		(1 << 7)
+#define SW_MIB_COUNTER_FREEZE		(1 << 6)
+#if 0
+#define SW_MIB_COUNTER_DONE		(1 << 0)
+#endif
+
+#define REG_SW_MAC_802_1P_MAP_0		0x0338
+#define REG_SW_MAC_802_1P_MAP_1		0x0339
+#define REG_SW_MAC_802_1P_MAP_2		0x033A
+#define REG_SW_MAC_802_1P_MAP_3		0x033B
+
+#define SW_802_1P_MAP_M			KS_PRIO_M
+#define SW_802_1P_MAP_S			KS_PRIO_S
+
+#define REG_SW_MAC_ISP_CTRL		0x033C
+
+#define REG_SW_MAC_TOS_CTRL		0x033E
+
+#define SW_TOS_DSCP_REMARK		(1 << 1)
+#define SW_TOS_DSCP_REMAP		(1 << 0)
+
+#define REG_SW_MAC_TOS_PRIO_0		0x0340
+#define REG_SW_MAC_TOS_PRIO_1		0x0341
+#define REG_SW_MAC_TOS_PRIO_2		0x0342
+#define REG_SW_MAC_TOS_PRIO_3		0x0343
+#define REG_SW_MAC_TOS_PRIO_4		0x0344
+#define REG_SW_MAC_TOS_PRIO_5		0x0345
+#define REG_SW_MAC_TOS_PRIO_6		0x0346
+#define REG_SW_MAC_TOS_PRIO_7		0x0347
+#define REG_SW_MAC_TOS_PRIO_8		0x0348
+#define REG_SW_MAC_TOS_PRIO_9		0x0349
+#define REG_SW_MAC_TOS_PRIO_10		0x034A
+#define REG_SW_MAC_TOS_PRIO_11		0x034B
+#define REG_SW_MAC_TOS_PRIO_12		0x034C
+#define REG_SW_MAC_TOS_PRIO_13		0x034D
+#define REG_SW_MAC_TOS_PRIO_14		0x034E
+#define REG_SW_MAC_TOS_PRIO_15		0x034F
+#define REG_SW_MAC_TOS_PRIO_16		0x0350
+#define REG_SW_MAC_TOS_PRIO_17		0x0351
+#define REG_SW_MAC_TOS_PRIO_18		0x0352
+#define REG_SW_MAC_TOS_PRIO_19		0x0353
+#define REG_SW_MAC_TOS_PRIO_20		0x0354
+#define REG_SW_MAC_TOS_PRIO_21		0x0355
+#define REG_SW_MAC_TOS_PRIO_22		0x0356
+#define REG_SW_MAC_TOS_PRIO_23		0x0357
+#define REG_SW_MAC_TOS_PRIO_24		0x0358
+#define REG_SW_MAC_TOS_PRIO_25		0x0359
+#define REG_SW_MAC_TOS_PRIO_26		0x035A
+#define REG_SW_MAC_TOS_PRIO_27		0x035B
+#define REG_SW_MAC_TOS_PRIO_28		0x035C
+#define REG_SW_MAC_TOS_PRIO_29		0x035D
+#define REG_SW_MAC_TOS_PRIO_30		0x035E
+#define REG_SW_MAC_TOS_PRIO_31		0x035F
+
+#define REG_SW_MRI_CTRL_0		0x0370
+
+#define SW_IGMP_SNOOP			(1 << 6)
+#define SW_IPV6_MLD_OPTION		(1 << 3)
+#define SW_IPV6_MLD_SNOOP		(1 << 2)
+#define SW_MIRROR_RX_TX			(1 << 0)
+
+#define REG_SW_CLASS_D_IP_CTRL__4	0x0374
+
+#define SW_CLASS_D_IP_ENABLE		(1 << 31)
+
+#define REG_SW_MRI_CTRL_8		0x0378
+
+#define SW_NO_COLOR_S			6
+#define SW_RED_COLOR_S			4
+#define SW_YELLOW_COLOR_S		2
+#define SW_GREEN_COLOR_S		0
+#define SW_COLOR_M			0x3
+
+#define REG_SW_QM_CTRL			0x0390
+
+#define PRIO_SCHEME_SELECT_M		0x3
+#define PRIO_SCHEME_SELECT_S		2
+#define UNICAST_VLAN_BOUNDARY		(1 << 1)
+
+#define REG_SW_EEE_QM_CTRL__2		0x03C0
+
+#define REG_SW_EEE_TXQ_WAIT_TIME__2	0x03C2
+
+/* 4 - */
+#define REG_SW_VLAN_ENTRY__4		0x0400
+
+#define VLAN_VALID			(1 << 31)
+#define VLAN_FORWARD_OPTION		(1 << 27)
+#define VLAN_PRIO_S			24
+#define VLAN_PRIO_M			0x3
+#define VLAN_MSTP_S			12
+#define VLAN_MSTP_M			0x3
+#define VLAN_FID_M			0x7F
+
+#define REG_SW_VLAN_ENTRY_UNTAG__4	0x0404
+#define REG_SW_VLAN_ENTRY_PORTS__4	0x0408
+
+#define REG_SW_VLAN_ENTRY_INDEX__2	0x040C
+
+#define VLAN_INDEX_M			0x0FFF
+
+#define REG_SW_VLAN_CTRL		0x040E
+
+#define VLAN_START			(1 << 7)
+#define VLAN_ACTION			0x3
+#define VLAN_WRITE			1
+#define VLAN_READ			2
+#define VLAN_CLEAR			3
+
+#define REG_SW_ALU_INDEX_0		0x0410
+
+#define ALU_FID_INDEX_S			16
+#define ALU_MAC_ADDR_HI			0xFFFF
+
+#define REG_SW_ALU_INDEX_1		0x0414
+
+#define ALU_DIRECT_INDEX_M		((1 << 12) - 1)
+
+#define REG_SW_ALU_CTRL__4		0x0418
+
+#define ALU_VALID_CNT_S			16
+#define ALU_VALID_CNT_M			((1 << 14) - 1)
+#define ALU_START			(1 << 7)
+#define ALU_VALID			(1 << 6)
+#define ALU_DIRECT			(1 << 2)
+#define ALU_ACTION			0x3
+#define ALU_WRITE			1
+#define ALU_READ			2
+#define ALU_SEARCH			3
+
+#define REG_SW_ALU_STAT_CTRL__4		0x041C
+
+#define ALU_STAT_INDEX_S		16
+#define ALU_STAT_INDEX_M		((1 << 4) - 1)
+#define ALU_RESV_MCAST_INDEX_M		((1 << 6) - 1)
+#define ALU_STAT_START			(1 << 7)
+#define ALU_RESV_MCAST_ADDR		(1 << 1)
+#define ALU_STAT_READ			(1 << 0)
+
+#define REG_SW_ALU_VAL_A		0x0420
+
+#define ALU_V_STATIC_VALID		(1 << 31)
+#define ALU_V_SRC_FILTER		(1 << 30)
+#define ALU_V_DST_FILTER		(1 << 29)
+#define ALU_V_PRIO_AGE_CNT_S		26
+#define ALU_V_PRIO_AGE_CNT_M		((1 << 3) - 1)
+#define ALU_V_MSTP_M			0x7
+
+#define REG_SW_ALU_VAL_B		0x0424
+
+#define ALU_V_OVERRIDE			(1 << 31)
+#define ALU_V_USE_FID			(1 << 30)
+#define ALU_V_PORT_MAP			((1 << 24) - 1)
+
+#define REG_SW_ALU_VAL_C		0x0428
+
+#define ALU_V_FID_S			16
+#define ALU_V_FID_M			((1 << 16) - 1)
+#define ALU_V_MAC_ADDR_HI		0xFFFF
+
+#define REG_SW_ALU_VAL_D		0x042C
+
+/* 5 - PTP Clock */
+#define REG_PTP_CLK_CTRL		0x0500
+
+#define PTP_STEP_ADJ			(1 << 6)
+#define PTP_STEP_DIR			(1 << 5)
+#define PTP_READ_TIME			(1 << 4)
+#define PTP_LOAD_TIME			(1 << 3)
+#define PTP_CLK_ADJ_ENABLE		(1 << 2)
+#define PTP_CLK_ENABLE			(1 << 1)
+#define PTP_CLK_RESET			(1 << 0)
+
+#define REG_PTP_RTC_SUB_NANOSEC__2	0x0502
+
+#define PTP_RTC_SUB_NANOSEC_M		0x0007
+
+#define REG_PTP_RTC_NANOSEC		0x0504
+#define REG_PTP_RTC_NANOSEC_H		0x0504
+#define REG_PTP_RTC_NANOSEC_L		0x0506
+
+#define REG_PTP_RTC_SEC			0x0508
+#define REG_PTP_RTC_SEC_H		0x0508
+#define REG_PTP_RTC_SEC_L		0x050A
+
+#define REG_PTP_SUBNANOSEC_RATE		0x050C
+#define REG_PTP_SUBNANOSEC_RATE_H	0x050C
+
+#define PTP_RATE_DIR			(1 << 31)
+#define PTP_TMP_RATE_ENABLE		(1 << 30)
+
+#define REG_PTP_SUBNANOSEC_RATE_L	0x050E
+
+#define REG_PTP_RATE_DURATION		0x0510
+#define REG_PTP_RATE_DURATION_H		0x0510
+#define REG_PTP_RATE_DURATION_L		0x0512
+
+#define REG_PTP_MSG_CONF1		0x0514
+
+#define PTP_802_1AS			(1 << 7)
+#define PTP_ENABLE			(1 << 6)
+#define PTP_ETH_ENABLE			(1 << 5)
+#define PTP_IPV4_UDP_ENABLE		(1 << 4)
+#define PTP_IPV6_UDP_ENABLE		(1 << 3)
+#define PTP_TC_P2P			(1 << 2)
+#define PTP_MASTER			(1 << 1)
+#define PTP_1STEP			(1 << 0)
+
+#define REG_PTP_MSG_CONF2		0x0516
+
+#define PTP_UNICAST_ENABLE		(1 << 12)
+#define PTP_ALTERNATE_MASTER		(1 << 11)
+#define PTP_ALL_HIGH_PRIO		(1 << 10)
+#define PTP_SYNC_CHECK			(1 << 9)
+#define PTP_DELAY_CHECK			(1 << 8)
+#define PTP_PDELAY_CHECK		(1 << 7)
+#define PTP_DROP_SYNC_DELAY_REQ		(1 << 5)
+#define PTP_DOMAIN_CHECK		(1 << 4)
+#define PTP_UDP_CHECKSUM		(1 << 2)
+
+#define REG_PTP_DOMAIN_VERSION		0x0518
+#define PTP_VERSION_M			0xFF00
+#define PTP_DOMAIN_M			0x00FF
+
+#define REG_PTP_UNIT_INDEX__4		0x0520
+
+#define PTP_UNIT_M			0xF
+
+/* 2013-09-10 */
+#define PTP_GPIO_INDEX_S		16
+#define PTP_TSI_INDEX_S			8
+#define PTP_TOU_INDEX_S			0
+
+#define REG_PTP_TRIG_STATUS__4		0x0524
+
+#define TRIG_ERROR_S			16
+#define TRIG_DONE_S			0
+
+#define REG_PTP_INT_STATUS__4		0x0528
+
+#define TRIG_INT_S			16
+#define TS_INT_S			0
+
+#define TRIG_UNIT_M			0x7
+#define TS_UNIT_M			0x3
+
+#define REG_PTP_CTRL_STAT__4		0x052C
+
+#define GPIO_IN				(1 << 7)
+#define GPIO_OUT			(1 << 6)
+#define TS_INT_ENABLE			(1 << 5)
+#define TRIG_ACTIVE			(1 << 4)
+#define TRIG_ENABLE			(1 << 3)
+#define TRIG_RESET			(1 << 2)
+#define TS_ENABLE			(1 << 1)
+#define TS_RESET			(1 << 0)
+
+#define GPIO_CTRL_M			\
+	(GPIO_IN | GPIO_OUT)
+
+#define TRIG_CTRL_M			\
+	(TRIG_ACTIVE | TRIG_ENABLE | TRIG_RESET)
+
+#define TS_CTRL_M			\
+	(TS_INT_ENABLE | TS_ENABLE | TS_RESET)
+
+#define REG_TRIG_TARGET_NANOSEC		0x0530
+#define REG_TRIG_TARGET_SEC		0x0534
+
+#define REG_TRIG_CTRL__4		0x0538
+
+#define TRIG_CASCADE_ENABLE		(1 << 31)
+#define TRIG_CASCADE_TAIL		(1 << 30)
+#define TRIG_CASCADE_UPS_S		26
+#define TRIG_CASCADE_UPS_M		0xF
+#define TRIG_NOW			(1 << 25)
+#define TRIG_NOTIFY			(1 << 24)
+#define TRIG_EDGE			(1 << 23)
+#define TRIG_PATTERN_S			20
+#define TRIG_PATTERN_M			0x7
+#define TRIG_NEG_EDGE			0
+#define TRIG_POS_EDGE			1
+#define TRIG_NEG_PULSE			2
+#define TRIG_POS_PULSE			3
+#define TRIG_NEG_PERIOD			4
+#define TRIG_POS_PERIOD			5
+#define TRIG_REG_OUTPUT			6
+#define TRIG_GPO_S			16
+#define TRIG_GPO_M			0xF
+#define TRIG_CASCADE_ITERATE_CNT_M	0xFFFF
+
+#define REG_TRIG_CYCLE_WIDTH		0x053C
+
+#define REG_TRIG_CYCLE_CNT		0x0540
+
+#define TRIG_CYCLE_CNT_S		16
+#define TRIG_CYCLE_CNT_M		0xFFFF
+#define TRIG_BIT_PATTERN_M		0xFFFF
+
+#define REG_TRIG_ITERATE_TIME		0x0544
+
+#define REG_TRIG_PULSE_WIDTH__4		0x0548
+
+#define TRIG_PULSE_WIDTH_M		0x00FFFFFF
+
+#define REG_TS_CTRL_STAT__4		0x0550
+
+#define TS_EVENT_DETECT_S		17
+#define TS_EVENT_DETECT_M		0xF
+#define TS_EVENT_OVERFLOW		(1 << 16)
+#define TS_GPI_S			8
+#define TS_GPI_M			0xF
+#define TS_DETECT_RISE			(1 << 7)
+#define TS_DETECT_FALL			(1 << 6)
+#define TS_DETECT_S			6
+#define TS_CASCADE_TAIL			(1 << 5)
+#define TS_CASCADE_UPS_S		1
+#define TS_CASCADE_UPS_M		0xF
+#define TS_CASCADE_ENABLE		(1 << 0)
+
+#define DETECT_RISE			(TS_DETECT_RISE >> TS_DETECT_S)
+#define DETECT_FALL			(TS_DETECT_FALL >> TS_DETECT_S)
+
+#define REG_TS_EVENT_0_NANOSEC		0x0554
+#define REG_TS_EVENT_0_SEC		0x0558
+#define REG_TS_EVENT_0_SUB_NANOSEC	0x055C
+
+#define REG_TS_EVENT_1_NANOSEC		0x0560
+#define REG_TS_EVENT_1_SEC		0x0564
+#define REG_TS_EVENT_1_SUB_NANOSEC	0x0568
+
+#define REG_TS_EVENT_2_NANOSEC		0x056C
+#define REG_TS_EVENT_2_SEC		0x0570
+#define REG_TS_EVENT_2_SUB_NANOSEC	0x0574
+
+#define REG_TS_EVENT_3_NANOSEC		0x0578
+#define REG_TS_EVENT_3_SEC		0x057C
+#define REG_TS_EVENT_3_SUB_NANOSEC	0x0580
+
+#define REG_TS_EVENT_4_NANOSEC		0x0584
+#define REG_TS_EVENT_4_SEC		0x0588
+#define REG_TS_EVENT_4_SUB_NANOSEC	0x058C
+
+#define REG_TS_EVENT_5_NANOSEC		0x0590
+#define REG_TS_EVENT_5_SEC		0x0594
+#define REG_TS_EVENT_5_SUB_NANOSEC	0x0598
+
+#define REG_TS_EVENT_6_NANOSEC		0x059C
+#define REG_TS_EVENT_6_SEC		0x05A0
+#define REG_TS_EVENT_6_SUB_NANOSEC	0x05A4
+
+#define REG_TS_EVENT_7_NANOSEC		0x05A8
+#define REG_TS_EVENT_7_SEC		0x05AC
+#define REG_TS_EVENT_7_SUB_NANOSEC	0x05B0
+
+#define TS_EVENT_EDGE_S			30
+#define TS_EVENT_EDGE_M			0x1
+#define TS_EVENT_NANOSEC_M		((1 << 30) - 1)
+
+#define TS_EVENT_SUB_NANOSEC_M		0x7
+
+#define TS_EVENT_SAMPLE			\
+	(REG_TS_EVENT_1_NANOSEC - REG_TS_EVENT_0_NANOSEC)
+
+
+#define PORT_CTRL_ADDR(port, addr)	((addr) | (((port) + 1) << 12))
+
+#define REG_GLOBAL_RR_INDEX__1		0x0600
+
+/* DLR */
+#define REG_DLR_SRC_PORT__4		0x0604
+
+#define DLR_SRC_PORT_UNICAST		(1 << 31)
+#define DLR_SRC_PORT_M			0x3
+#define DLR_SRC_PORT_BOTH		0
+#define DLR_SRC_PORT_EACH		1
+
+#define REG_DLR_IP_ADDR__4		0x0608
+
+#define REG_DLR_CTRL__1			0x0610
+
+#define DLR_RESET_SEQ_ID		(1 << 3)
+#define DLR_BACKUP_AUTO_ON		(1 << 2)
+#define DLR_BEACON_TX_ENABLE		(1 << 1)
+#define DLR_ASSIST_ENABLE		(1 << 0)
+
+#define REG_DLR_STATE__1		0x0611
+
+#define DLR_NODE_STATE_M		0x3
+#define DLR_NODE_STATE_S		1
+#define DLR_NODE_STATE_IDLE		0
+#define DLR_NODE_STATE_FAULT		1
+#define DLR_NODE_STATE_NORMAL		2
+#define DLR_RING_STATE_FAULT		0
+#define DLR_RING_STATE_NORMAL		1
+
+#define REG_DLR_PRECEDENCE__1		0x0612
+
+#define REG_DLR_BEACON_INTERVAL__4	0x0614
+
+#define REG_DLR_BEACON_TIMEOUT__4	0x0618
+
+#define REG_DLR_TIMEOUT_WINDOW__4	0x061C
+
+#define DLR_TIMEOUT_WINDOW_M		((1 << 22) - 1)
+
+#define REG_DLR_VLAN_ID__2		0x0620
+
+#define DLR_VLAN_ID_M			((1 << 12) - 1)
+
+#define REG_DLR_DEST_ADDR_0		0x0622
+#define REG_DLR_DEST_ADDR_1		0x0623
+#define REG_DLR_DEST_ADDR_2		0x0624
+#define REG_DLR_DEST_ADDR_3		0x0625
+#define REG_DLR_DEST_ADDR_4		0x0626
+#define REG_DLR_DEST_ADDR_5		0x0627
+
+#define REG_DLR_PORT_MAP__4		0x0628
+
+#define REG_DLR_CLASS__1		0x062C
+
+#define DLR_FRAME_QID_M			0x3
+
+/* HSR */
+#define REG_HSR_PORT_MAP__4		0x0640
+
+#define REG_HSR_ALU_CTRL_0__1		0x0644
+
+#define HSR_DUPLICATE_DISCARD		(1 << 7)
+#define HSR_NODE_UNICAST		(1 << 6)
+#define HSR_AGE_CNT_DEFAULT_M		0x7
+#define HSR_AGE_CNT_DEFAULT_S		3
+#define HSR_LEARN_MCAST_DISABLE		(1 << 2)
+#define HSR_HASH_OPTION_M		0x3
+#define HSR_HASH_DISABLE		0
+#define HSR_HASH_UPPER_BITS		1
+#define HSR_HASH_LOWER_BITS		2
+#define HSR_HASH_XOR_BOTH_BITS		3
+
+#define REG_HSR_ALU_CTRL_1__1		0x0645
+
+#define HSR_LEARN_UCAST_DISABLE		(1 << 7)
+#define HSR_FLUSH_TABLE			(1 << 5)
+#define HSR_PROC_MCAST_SRC		(1 << 3)
+#define HSR_AGING_ENABLE		(1 << 2)
+
+#define REG_HSR_ALU_CTRL_2__2		0x0646
+
+#define REG_HSR_ALU_AGE_PERIOD__4	0x0648
+
+#define REG_HSR_ALU_INT_STATUS__1	0x064C
+#define REG_HSR_ALU_INT_MASK__1		0x064D
+
+#define HSR_WINDOW_OVERFLOW_INT		(1 << 3)
+#define HSR_LEARN_FAIL_INT		(1 << 2)
+#define HSR_ALMOST_FULL_INT		(1 << 1)
+#define HSR_WRITE_FAIL_INT		(1 << 0)
+
+#define REG_HSR_ALU_ENTRY_0__2		0x0650
+
+#define HSR_ENTRY_INDEX_M		((1 << 10) - 1)
+#define HSR_FAIL_INDEX_M		((1 << 8) - 1)
+
+#define REG_HSR_ALU_ENTRY_1__2		0x0652
+
+#define HSR_FAIL_LEARN_INDEX_M		((1 << 8) - 1)
+
+#define REG_HSR_ALU_ENTRY_3__2		0x0654
+
+#define HSR_CPU_ACCESS_ENTRY_INDEX_M	((1 << 8) - 1)
+
+
+/* 0 - Operation */
+#define REG_PORT_DEFAULT_VID		0x0000
+
+#define REG_PORT_CUSTOM_VID		0x0002
+#define REG_PORT_AVB_SR_1_VID		0x0004
+#define REG_PORT_AVB_SR_2_VID		0x0006
+
+#define REG_PORT_AVB_SR_1_TYPE		0x0008
+#define REG_PORT_AVB_SR_2_TYPE		0x000A
+
+#define REG_PORT_PME_STATUS		0x0013
+#define REG_PORT_PME_CTRL		0x0017
+
+#define PME_WOL_MAGICPKT		(1 << 2)
+#define PME_WOL_LINKUP			(1 << 1)
+#define PME_WOL_ENERGY			(1 << 0)
+
+#define REG_PORT_INT_STATUS		0x001B
+#define REG_PORT_INT_MASK		0x001F
+
+#define PORT_PTP_INT			(1 << 2)
+#define PORT_PHY_INT			(1 << 1)
+#define PORT_ACL_INT			(1 << 0)
+
+#define PORT_INT_MASK			\
+	(PORT_PTP_INT | PORT_PHY_INT | PORT_ACL_INT)
+
+#define REG_PORT_CTRL_0			0x0020
+
+#define PORT_MAC_LOOPBACK		(1 << 7)
+#define PORT_FORCE_TX_FLOW_CTRL		(1 << 4)
+#define PORT_FORCE_RX_FLOW_CTRL		(1 << 3)
+#define PORT_TAIL_TAG_ENABLE		(1 << 2)
+#define PORT_QUEUE_SPLIT_ENABLE		0x3
+
+#define REG_PORT_CTRL_1			0x0021
+
+#define PORT_SRP_ENABLE			0x3
+
+#define REG_PORT_STATUS_0		0x0030
+
+#define PORT_INTF_SPEED_M		0x3
+#define PORT_INTF_SPEED_S		3
+#define PORT_INTF_FULL_DUPLEX		(1 << 2)
+#define PORT_TX_FLOW_CTRL		(1 << 1)
+#define PORT_RX_FLOW_CTRL		(1 << 0)
+
+#define REG_PORT_STATUS_1		0x0034
+
+/* 1 - PHY */
+#define REG_PORT_PHY_CTRL		0x0100
+
+#define PORT_PHY_RESET			(1 << 15)
+#define PORT_PHY_LOOPBACK		(1 << 14)
+#define PORT_SPEED_100MBIT		(1 << 13)
+#define PORT_AUTO_NEG_ENABLE		(1 << 12)
+#define PORT_POWER_DOWN			(1 << 11)
+#define PORT_ISOLATE			(1 << 10)
+#define PORT_AUTO_NEG_RESTART		(1 << 9)
+#define PORT_FULL_DUPLEX		(1 << 8)
+#define PORT_COLLISION_TEST		(1 << 7)
+#define PORT_SPEED_1000MBIT		(1 << 6)
+#if 0
+#define PHY_HP_MDIX			(1 << 5)
+#define PHY_FORCE_MDIX			(1 << 4)
+#define PHY_AUTO_MDIX_DISABLE		(1 << 3)
+#define PHY_REMOTE_FAULT_DISABLE	(1 << 2)
+#define PHY_TRANSMIT_DISABLE		(1 << 1)
+#define PHY_LED_DISABLE			(1 << 0)
+#endif
+
+#define REG_PORT_PHY_STATUS		0x0102
+
+#define PORT_100BT4_CAPABLE		(1 << 15)
+#define PORT_100BTX_FD_CAPABLE		(1 << 14)
+#define PORT_100BTX_CAPABLE		(1 << 13)
+#define PORT_10BT_FD_CAPABLE		(1 << 12)
+#define PORT_10BT_CAPABLE		(1 << 11)
+#define PORT_EXTENDED_STATUS		(1 << 8)
+#define PORT_MII_SUPPRESS_CAPABLE	(1 << 6)
+#define PORT_AUTO_NEG_ACKNOWLEDGE	(1 << 5)
+#define PORT_REMOTE_FAULT		(1 << 4)
+#define PORT_AUTO_NEG_CAPABLE		(1 << 3)
+#define PORT_LINK_STATUS		(1 << 2)
+#define PORT_JABBER_DETECT		(1 << 1)
+#define PORT_EXTENDED_CAPABILITY	(1 << 0)
+
+#define REG_PORT_PHY_ID_HI		0x0104
+#define REG_PORT_PHY_ID_LO		0x0106
+
+#define KSZ9897_ID_HI			0x0022
+#define KSZ9897_ID_LO			0x1622
+
+#define REG_PORT_PHY_AUTO_NEGOTIATION	0x0108
+
+#define PORT_AUTO_NEG_NEXT_PAGE		(1 << 15)
+#define PORT_AUTO_NEG_REMOTE_FAULT	(1 << 13)
+#define PORT_AUTO_NEG_ASYM_PAUSE	(1 << 11)
+#define PORT_AUTO_NEG_SYM_PAUSE		(1 << 10)
+#define PORT_AUTO_NEG_100BT4		(1 << 9)
+#define PORT_AUTO_NEG_100BTX_FD		(1 << 8)
+#define PORT_AUTO_NEG_100BTX		(1 << 7)
+#define PORT_AUTO_NEG_10BT_FD		(1 << 6)
+#define PORT_AUTO_NEG_10BT		(1 << 5)
+#define PORT_AUTO_NEG_SELECTOR		0x001F
+#define PORT_AUTO_NEG_802_3		0x0001
+
+#define PORT_AUTO_NEG_PAUSE		\
+	(PORT_AUTO_NEG_ASYM_PAUSE | PORT_AUTO_NEG_SYM_PAUSE)
+
+#define REG_PORT_PHY_REMOTE_CAPABILITY	0x010A
+
+#define PORT_REMOTE_NEXT_PAGE		(1 << 15)
+#define PORT_REMOTE_ACKNOWLEDGE		(1 << 14)
+#define PORT_REMOTE_REMOTE_FAULT	(1 << 13)
+#define PORT_REMOTE_ASYM_PAUSE		(1 << 11)
+#define PORT_REMOTE_SYM_PAUSE		(1 << 10)
+#define PORT_REMOTE_100BTX_FD		(1 << 8)
+#define PORT_REMOTE_100BTX		(1 << 7)
+#define PORT_REMOTE_10BT_FD		(1 << 6)
+#define PORT_REMOTE_10BT		(1 << 5)
+
+#define REG_PORT_PHY_1000_CTRL		0x0112
+
+#define PORT_AUTO_NEG_MANUAL		(1 << 12)
+#define PORT_AUTO_NEG_MASTER		(1 << 11)
+#define PORT_AUTO_NEG_MASTER_PREFERRED	(1 << 10)
+#define PORT_AUTO_NEG_1000BT_FD		(1 << 9)
+#define PORT_AUTO_NEG_1000BT		(1 << 8)
+
+#define REG_PORT_PHY_1000_STATUS	0x0114
+
+#define PORT_MASTER_FAULT		(1 << 15)
+#define PORT_LOCAL_MASTER		(1 << 14)
+#define PORT_LOCAL_RX_OK		(1 << 13)
+#define PORT_REMOTE_RX_OK		(1 << 12)
+#define PORT_REMOTE_1000BT_FD		(1 << 11)
+#define PORT_REMOTE_1000BT		(1 << 10)
+#define PORT_REMOTE_IDLE_CNT_M		0x0F
+
+#define PORT_PHY_1000_STATIC_STATUS	\
+	(PORT_LOCAL_RX_OK |		\
+	PORT_REMOTE_RX_OK |		\
+	PORT_REMOTE_1000BT_FD |		\
+	PORT_REMOTE_1000BT)
+
+#define REG_PORT_PHY_MMD_SETUP		0x011A
+
+#define PORT_MMD_OP_MODE_S		14
+#define PORT_MMD_OP_MODE_M		0x3
+#define PORT_MMD_OP_INDEX		0
+#define PORT_MMD_OP_DATA_NO_INCR	1
+#define PORT_MMD_OP_DATA_INCR_RW	2
+#define PORT_MMD_OP_DATA_INCR_W		3
+#define PORT_MMD_DEVICE_ID_M		0x1F
+
+#define MMD_SETUP(mode, dev)		\
+	(((u16) mode << PORT_MMD_OP_MODE_S) | dev)
+
+#define REG_PORT_PHY_MMD_INDEX_DATA	0x011C
+
+#define MMD_DEVICE_ID_DSP		1
+#define MMD_DEVICE_ID_COMMON		2
+
+#define MMD_DEVICE_ID_EEE_ADV		7
+
+#define MMD_EEE_ADV			0x3C
+#define EEE_ADV_100MBIT			(1 << 1)
+#define EEE_ADV_1GBIT			(1 << 2)
+
+#define MMD_EEE_LP_ADV			0x3D
+#define MMD_EEE_MSG_CODE		0x3F
+
+#define MMD_DEVICE_ID_AFED		0x1C
+
+#define REG_PORT_PHY_EXTENDED_STATUS	0x011E
+
+#define PORT_100BTX_FD_ABLE		(1 << 15)
+#define PORT_100BTX_ABLE		(1 << 14)
+#define PORT_10BT_FD_ABLE		(1 << 13)
+#define PORT_10BT_ABLE			(1 << 12)
+
+#define REG_PORT_PHY_REMOTE_LB_LED	0x0122
+
+#define PORT_REMOTE_LOOPBACK		(1 << 8)
+#define PORT_LED_SELECT			(3 << 6)
+#define PORT_LED_CTRL			(3 << 4)
+#define PORT_LED_CTRL_TEST		(1 << 3)
+#define PORT_10BT_PREAMBLE		(1 << 2)
+#define PORT_LINK_MD_10BT_ENABLE	(1 << 1)
+#define PORT_LINK_MD_PASS		(1 << 0)
+
+#define REG_PORT_PHY_LINK_MD		0x0124
+
+#define PORT_START_CABLE_DIAG		(1 << 15)
+#define PORT_TX_DISABLE			(1 << 14)
+#define PORT_CABLE_DIAG_PAIR_S		12
+#define PORT_CABLE_DIAG_PAIR_M		0x3
+#define PORT_CABLE_DIAG_SELECT_S	10
+#define PORT_CABLE_DIAG_SELECT_M	0x3
+#define PORT_CABLE_DIAG_RESULT_S	8
+#define PORT_CABLE_DIAG_RESULT_M	0x3
+#define PORT_CABLE_STAT_NORMAL		0
+#define PORT_CABLE_STAT_OPEN		1
+#define PORT_CABLE_STAT_SHORT		2
+#define PORT_CABLE_STAT_FAILED		3
+#define PORT_CABLE_10M_SHORT		(1 << 12)
+#define PORT_CABLE_FAULT_COUNTER	0x00FF
+
+#define REG_PORT_PHY_PMA_STATUS		0x0126
+
+#define PORT_1000_LINK_GOOD		(1 << 1)
+#define PORT_100_LINK_GOOD		(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_STATUS	0x0128
+
+#define PORT_LINK_DETECT		(1 << 14)
+
+#define REG_PORT_PHY__CTRL		30
+
+#define PHY_STAT_REVERSED_POLARITY	(1 << 5)
+#define PHY_STAT_MDIX			(1 << 4)
+#define PHY_FORCE_LINK			(1 << 3)
+#define PHY_POWER_SAVING_DISABLE	(1 << 2)
+#define PHY_REMOTE_LOOPBACK		(1 << 1)
+
+#define REG_PORT_PHY_RXER_COUNTER	0x012A
+
+#define REG_PORT_PHY_INT_ENABLE		0x0136
+#define REG_PORT_PHY_INT_STATUS		0x0137
+
+#define JABBER_INT			(1 << 7)
+#define RX_ERR_INT			(1 << 6)
+#define PAGE_RX_INT			(1 << 5)
+#define PARALLEL_DETECT_FAULT_INT	(1 << 4)
+#define LINK_PARTNER_ACK_INT		(1 << 3)
+#define LINK_DOWN_INT			(1 << 2)
+#define REMOTE_FAULT_INT		(1 << 1)
+#define LINK_UP_INT			(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_1	0x0138
+
+#define PORT_REG_CLK_SPEED_25_MHZ	(1 << 14)
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_2	0x013A
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_3	0x013C
+
+#define PORT_100BT_FIXED_LATENCY	(1 << 15)
+
+#define REG_PORT_PHY_PHY_CTRL		0x013E
+
+#define PORT_INT_PIN_HIGH		(1 << 14)
+#define PORT_ENABLE_JABBER		(1 << 9)
+#define PORT_STAT_SPEED_1000MBIT	(1 << 6)
+#define PORT_STAT_SPEED_100MBIT		(1 << 5)
+#define PORT_STAT_SPEED_10MBIT		(1 << 4)
+#define PORT_STAT_FULL_DUPLEX		(1 << 3)
+#define PORT_STAT_MASTER		(1 << 2)
+#define PORT_RESET			(1 << 1)
+#define PORT_LINK_STATUS_FAIL		(1 << 0)
+
+/* 3 - xMII */
+#define REG_PORT_XMII_CTRL_0		0x0300
+
+#define PORT_SGMII_SEL			(1 << 7)
+#define PORT_MII_FULL_DUPLEX		(1 << 6)
+#define PORT_MII_100MBIT		(1 << 4)
+#define PORT_GRXC_ENABLE		(1 << 0)
+
+#define REG_PORT_XMII_CTRL_1		0x0301
+
+#define PORT_RMII_CLK_SEL		(1 << 7)
+/* S1 */
+#define PORT_MII_1000MBIT_S1		(1 << 6)
+/* S2 */
+#define PORT_MII_NOT_1GBIT		(1 << 6)
+#define PORT_MII_SEL_EDGE		(1 << 5)
+#define PORT_RGMII_ID_IG_ENABLE		(1 << 4)
+#define PORT_RGMII_ID_EG_ENABLE		(1 << 3)
+#define PORT_MII_MAC_MODE		(1 << 2)
+#define PORT_MII_SEL_M			0x3
+/* S1 */
+#define PORT_MII_SEL_S1			0x0
+#define PORT_RMII_SEL_S1		0x1
+#define PORT_GMII_SEL_S1		0x2
+#define PORT_RGMII_SEL_S1		0x3
+/* S2 */
+#define PORT_RGMII_SEL			0x0
+#define PORT_RMII_SEL			0x1
+#define PORT_GMII_SEL			0x2
+#define PORT_MII_SEL			0x3
+
+/* 4 - MAC */
+#define REG_PORT_MAC_CTRL_0		0x0400
+
+#define PORT_BROADCAST_STORM		(1 << 1)
+#define PORT_JUMBO_FRAME		(1 << 0)
+
+#define REG_PORT_MAC_CTRL_1		0x0401
+
+#define PORT_BACK_PRESSURE		(1 << 3)
+#define PORT_PASS_ALL			(1 << 0)
+
+#define REG_PORT_MAC_CTRL_2		0x0402
+
+#define PORT_100BT_EEE_DISABLE		(1 << 7)
+#define PORT_1000BT_EEE_DISABLE		(1 << 6)
+
+#define REG_PORT_MAC_IN_RATE_LIMIT	0x0403
+
+#define PORT_IN_PORT_BASED_S		6
+#define PORT_IN_PACKET_BASED_S		5
+#define PORT_IN_FLOW_CTRL_S		4
+#define PORT_COUNT_IFG_S		1
+#define PORT_COUNT_PREAMBLE_S		0
+#define PORT_IN_PORT_BASED		(1 << 6)
+#define PORT_IN_PACKET_BASED		(1 << 5)
+#define PORT_IN_FLOW_CTRL		(1 << 4)
+#define PORT_IN_LIMIT_MODE_M		0x3
+#define PORT_IN_LIMIT_MODE_S		2
+#define PORT_IN_ALL			0
+#define PORT_IN_UNICAST			1
+#define PORT_IN_MULTICAST		2
+#define PORT_IN_BROADCAST		3
+#define PORT_COUNT_IFG			(1 << 1)
+#define PORT_COUNT_PREAMBLE		(1 << 0)
+
+#define REG_PORT_IN_RATE_0		0x0410
+#define REG_PORT_IN_RATE_1		0x0411
+#define REG_PORT_IN_RATE_2		0x0412
+#define REG_PORT_IN_RATE_3		0x0413
+#define REG_PORT_IN_RATE_4		0x0414
+#define REG_PORT_IN_RATE_5		0x0415
+#define REG_PORT_IN_RATE_6		0x0416
+#define REG_PORT_IN_RATE_7		0x0417
+
+#define REG_PORT_OUT_RATE_0		0x0420
+#define REG_PORT_OUT_RATE_1		0x0421
+#define REG_PORT_OUT_RATE_2		0x0422
+#define REG_PORT_OUT_RATE_3		0x0423
+
+#define PORT_RATE_LIMIT_M		((1 << 7) - 1)
+
+/* 5 - MIB Counters */
+#define REG_PORT_MIB_CTRL_STAT__4	0x0500
+
+#define MIB_COUNTER_OVERFLOW		(1 << 31)
+#define MIB_COUNTER_VALID		(1 << 30)
+#define MIB_COUNTER_READ		(1 << 25)
+#define MIB_COUNTER_FLUSH_FREEZE	(1 << 24)
+#define MIB_COUNTER_INDEX_M		((1 << 8) - 1)
+#define MIB_COUNTER_INDEX_S		16
+#define MIB_COUNTER_DATA_HI_M		0xF
+
+#define REG_PORT_MIB_DATA		0x0504
+
+/* 6 - ACL */
+#define REG_PORT_ACL_0			0x0600
+
+#define ACL_FIRST_RULE_M		0xF
+
+#define REG_PORT_ACL_1			0x0601
+
+#define ACL_MODE_M			0x3
+#define ACL_MODE_S			4
+#define ACL_MODE_DISABLE		0
+#define ACL_MODE_LAYER_2		1
+#define ACL_MODE_LAYER_3		2
+#define ACL_MODE_LAYER_4		3
+#define ACL_ENABLE_M			0x3
+#define ACL_ENABLE_S			2
+#define ACL_ENABLE_2_COUNT		0
+#define ACL_ENABLE_2_TYPE		1
+#define ACL_ENABLE_2_MAC		2
+#define ACL_ENABLE_2_BOTH		3
+#define ACL_ENABLE_3_IP			1
+#define ACL_ENABLE_3_SRC_DST_COMP	2
+#define ACL_ENABLE_4_PROTOCOL		0
+#define ACL_ENABLE_4_TCP_PORT_COMP	1
+#define ACL_ENABLE_4_UDP_PORT_COMP	2
+#define ACL_ENABLE_4_TCP_SEQN_COMP	3
+#define ACL_SRC				(1 << 1)
+#define ACL_EQUAL			(1 << 0)
+
+#define REG_PORT_ACL_2			0x0602
+#define REG_PORT_ACL_3			0x0603
+
+#define ACL_MAX_PORT			0xFFFF
+
+#define REG_PORT_ACL_4			0x0604
+#define REG_PORT_ACL_5			0x0605
+
+#define ACL_MIN_PORT			0xFFFF
+#define ACL_IP_ADDR			0xFFFFFFFF
+#define ACL_TCP_SEQNUM			0xFFFFFFFF
+
+#define REG_PORT_ACL_6			0x0606
+
+#define ACL_RESERVED			0xF8
+#define ACL_PORT_MODE_M			0x3
+#define ACL_PORT_MODE_S			1
+#define ACL_PORT_MODE_DISABLE		0
+#define ACL_PORT_MODE_EITHER		1
+#define ACL_PORT_MODE_IN_RANGE		2
+#define ACL_PORT_MODE_OUT_OF_RANGE	3
+
+#define REG_PORT_ACL_7			0x0607
+
+#define ACL_TCP_FLAG_ENABLE		(1 << 0)
+
+#define REG_PORT_ACL_8			0x0608
+
+#define ACL_TCP_FLAG_M			0xFF
+
+#define REG_PORT_ACL_9			0x0609
+
+#define ACL_TCP_FLAG			0xFF
+#define ACL_ETH_TYPE			0xFFFF
+#define ACL_IP_M			0xFFFFFFFF
+
+#define REG_PORT_ACL_A			0x060A
+
+#define ACL_PRIO_MODE_M			0x3
+#define ACL_PRIO_MODE_S			6
+#define ACL_PRIO_MODE_DISABLE		0
+#define ACL_PRIO_MODE_HIGHER		1
+#define ACL_PRIO_MODE_LOWER		2
+#define ACL_PRIO_MODE_REPLACE		3
+#define ACL_PRIO_M			KS_PRIO_M
+#define ACL_PRIO_S			3
+#define ACL_VLAN_PRIO_REPLACE		(1 << 2)
+#define ACL_VLAN_PRIO_M			KS_PRIO_M
+#define ACL_VLAN_PRIO_HI_M		0x3
+
+#define REG_PORT_ACL_B			0x060B
+
+#define ACL_VLAN_PRIO_LO_M		0x8
+#define ACL_VLAN_PRIO_S			7
+#define ACL_MAP_MODE_M			0x3
+#define ACL_MAP_MODE_S			5
+#define ACL_MAP_MODE_DISABLE		0
+#define ACL_MAP_MODE_OR			1
+#define ACL_MAP_MODE_AND		2
+#define ACL_MAP_MODE_REPLACE		3
+
+#define ACL_CNT_M			((1 << 11) - 1)
+#define ACL_CNT_S			5
+
+#define REG_PORT_ACL_C			0x060C
+
+#define REG_PORT_ACL_D			0x060D
+#define ACL_MSEC_UNIT			(1 << 6)
+#define ACL_INTR_MODE			(1 << 5)
+#define ACL_PORT_MAP			0x7F
+
+#define REG_PORT_ACL_E			0x060E
+#define REG_PORT_ACL_F			0x060F
+
+#define REG_PORT_ACL_BYTE_EN_MSB	0x0610
+#define REG_PORT_ACL_BYTE_EN_LSB	0x0611
+
+#define ACL_ACTION_START		0xA
+#define ACL_ACTION_LEN			4
+#define ACL_INTR_CNT_START		0xD
+#define ACL_RULESET_START		0xE
+#define ACL_TABLE_LEN			16
+
+#define ACL_ACTION_ENABLE		0x003C
+#define ACL_MATCH_ENABLE		0xFFC3
+#define ACL_BYTE_ENABLE			0xFFFF
+
+#define REG_PORT_ACL_CTRL_0		0x0612
+
+#define PORT_ACL_WRITE_DONE		(1 << 6)
+#define PORT_ACL_READ_DONE		(1 << 5)
+#define PORT_ACL_WRITE			(1 << 4)
+#define PORT_ACL_INDEX_M		0xF
+
+#define REG_PORT_ACL_CTRL_1		0x0613
+
+/* 8 - Classification and Policing */
+#define REG_PORT_MRI_MIRROR_CTRL	0x0800
+
+#define PORT_MIRROR_RX			(1 << 6)
+#define PORT_MIRROR_TX			(1 << 5)
+#define PORT_MIRROR_SNIFFER		(1 << 1)
+
+#define REG_PORT_MRI_PRIO_CTRL		0x0801
+
+#define PORT_HIGHEST_PRIO		(1 << 7)
+#define PORT_OR_PRIO			(1 << 6)
+#define PORT_MAC_PRIO_ENABLE		(1 << 4)
+#define PORT_VLAN_PRIO_ENABLE		(1 << 3)
+#define PORT_802_1P_PRIO_ENABLE		(1 << 2)
+#define PORT_DIFFSERV_PRIO_ENABLE	(1 << 1)
+#define PORT_ACL_PRIO_ENABLE		(1 << 0)
+
+#define REG_PORT_MRI_MAC_CTRL		0x0802
+
+#define PORT_USER_PRIO_CEILING		(1 << 7)
+#define PORT_DROP_NON_VLAN		(1 << 4)
+#define PORT_DROP_TAG			(1 << 3)
+#define PORT_BASED_PRIO_M		KS_PRIO_M
+#define PORT_BASED_PRIO_S		0
+
+#define REG_PORT_MRI_AUTHEN_CTRL	0x0803
+
+#define PORT_ACL_ENABLE			(1 << 2)
+#define PORT_AUTHEN_MODE		0x3
+#define PORT_AUTHEN_PASS		0
+#define PORT_AUTHEN_BLOCK		1
+#define PORT_AUTHEN_TRAP		2
+
+#define REG_PORT_MRI_INDEX__4		0x0804
+
+#define MRI_INDEX_P_M			0x7
+#define MRI_INDEX_P_S			16
+#define MRI_INDEX_Q_M			0x3
+#define MRI_INDEX_Q_S			0
+
+#define REG_PORT_MRI_TC_MAP__4		0x0808
+
+#define PORT_TC_MAP_M			0xf
+#define PORT_TC_MAP_S			4
+
+#define REG_PORT_MRI_POLICE_CTRL__4	0x080C
+
+#define POLICE_DROP_ALL			(1 << 10)
+#define POLICE_PACKET_TYPE_M		0x3
+#define POLICE_PACKET_TYPE_S		8
+#define POLICE_PACKET_DROPPED		0
+#define POLICE_PACKET_GREEN		1
+#define POLICE_PACKET_YELLOW		2
+#define POLICE_PACKET_RED		3
+#define PORT_BASED_POLICING		(1 << 7)
+#define NON_DSCP_COLOR_M		0x3
+#define NON_DSCP_COLOR_S		5
+#define COLOR_MARK_ENABLE		(1 << 4)
+#define COLOR_REMAP_ENABLE		(1 << 3)
+#define POLICE_DROP_SRP			(1 << 2)
+#define POLICE_COLOR_NOT_AWARE		(1 << 1)
+#define POLICE_ENABLE			(1 << 0)
+
+#define REG_PORT_POLICE_COLOR_0__4	0x0810
+#define REG_PORT_POLICE_COLOR_1__4	0x0814
+#define REG_PORT_POLICE_COLOR_2__4	0x0818
+#define REG_PORT_POLICE_COLOR_3__4	0x081C
+
+#define POLICE_COLOR_MAP_S		2
+#define POLICE_COLOR_MAP_M		((1 << POLICE_COLOR_MAP_S) - 1)
+
+#define REG_PORT_POLICE_RATE__4		0x0820
+
+#define POLICE_CIR_S			16
+#define POLICE_PIR_S			0
+
+#define REG_PORT_POLICE_BURST_SIZE__4	0x0824
+
+#define POLICE_BURST_SIZE_M		0x3FFF
+#define POLICE_CBS_S			16
+#define POLICE_PBS_S			0
+
+#define REG_PORT_WRED_PM_CTRL_0__4	0x0830
+
+#define WRED_PM_CTRL_M			((1 << 11) - 1)
+
+#define WRED_PM_MAX_THRESHOLD_S		16
+#define WRED_PM_MIN_THRESHOLD_S		0
+
+#define REG_PORT_WRED_PM_CTRL_1__4	0x0834
+
+#define WRED_PM_MULTIPLIER_S		16
+#define WRED_PM_AVG_QUEUE_SIZE_S	0
+
+#define REG_PORT_WRED_QUEUE_CTRL_0__4	0x0840
+#define REG_PORT_WRED_QUEUE_CTRL_1__4	0x0844
+
+#define REG_PORT_WRED_QUEUE_PMON__4	0x0848
+
+#define WRED_RANDOM_DROP_ENABLE		(1 << 31)
+#define WRED_PMON_FLUSH			(1 << 30)
+#define WRED_DROP_GYR_DISABLE		(1 << 29)
+#define WRED_DROP_YR_DISABLE		(1 << 28)
+#define WRED_DROP_R_DISABLE		(1 << 27)
+#define WRED_DROP_ALL			(1 << 26)
+#define WRED_PMON_M			((1 << 24) - 1)
+
+/* 9 - Shaping */
+
+#define REG_PORT_MTI_QUEUE_INDEX__4	0x0900
+
+#define REG_PORT_MTI_QUEUE_CTRL_0__4	0x0904
+
+#define MTI_PVID_REPLACE		(1 << 0)
+
+#define REG_PORT_MTI_QUEUE_CTRL_0	0x0914
+
+#define MTI_SCHEDULE_MODE_M		0x3
+#define MTI_SCHEDULE_MODE_S		6
+#define MTI_SCHEDULE_STRICT_PRIO	0
+#define MTI_SCHEDULE_WRR		2
+#define MTI_SHAPING_M			0x3
+#define MTI_SHAPING_S			4
+#define MTI_SHAPING_OFF			0
+#define MTI_SHAPING_SRP			1
+#define MTI_SHAPING_TIME_AWARE		2
+#if 0
+#define MTI_PREEMPT_ENABLE		(1 << 3)
+#endif
+
+#define REG_PORT_MTI_QUEUE_CTRL_1	0x0915
+
+#define MTI_TX_RATIO_M			((1 << 7) - 1)
+
+#define REG_PORT_MTI_QUEUE_CTRL_2__2	0x0916
+#define REG_PORT_MTI_HI_WATER_MARK	0x0916
+#define REG_PORT_MTI_QUEUE_CTRL_3__2	0x0918
+#define REG_PORT_MTI_LO_WATER_MARK	0x0918
+#define REG_PORT_MTI_QUEUE_CTRL_4__2	0x091A
+#define REG_PORT_MTI_CREDIT_INCREMENT	0x091A
+
+/* A - QM */
+
+#define REG_PORT_QM_CTRL__4		0x0A00
+
+#define PORT_QM_DROP_PRIO_M		0x3
+
+#define REG_PORT_VLAN_MEMBERSHIP__4	0x0A04
+
+#define REG_PORT_QM_QUEUE_INDEX__4	0x0A08
+
+#define PORT_QM_QUEUE_INDEX_S		24
+#define PORT_QM_BURST_SIZE_S		16
+#define PORT_QM_MIN_RESV_SPACE_M	((1 << 11) - 1)
+
+#define REG_PORT_QM_WATER_MARK__4	0x0A0C
+
+#define PORT_QM_HI_WATER_MARK_S		16
+#define PORT_QM_LO_WATER_MARK_S		0
+#define PORT_QM_WATER_MARK_M		((1 << 11) - 1)
+
+#define REG_PORT_QM_TX_CNT_0__4		0x0A10
+
+#define PORT_QM_TX_CNT_USED_S		0
+#define PORT_QM_TX_CNT_M		((1 << 11) - 1)
+
+#define REG_PORT_QM_TX_CNT_1__4		0x0A14
+
+#define PORT_QM_TX_CNT_CALCULATED_S	16
+#define PORT_QM_TX_CNT_AVAIL_S		0
+
+/* B - LUE */
+#define REG_PORT_LUE_CTRL		0x0B00
+
+#define PORT_VLAN_LOOKUP_VID_0		(1 << 7)
+#define PORT_INGRESS_FILTER		(1 << 6)
+#define PORT_DISCARD_NON_VID		(1 << 5)
+#define PORT_MAC_BASED_802_1X		(1 << 4)
+#define PORT_SRC_ADDR_FILTER		(1 << 3)
+
+#define REG_PORT_LUE_MSTP_INDEX		0x0B01
+
+#define REG_PORT_LUE_MSTP_STATE		0x0B04
+
+#define PORT_TX_ENABLE			(1 << 2)
+#define PORT_RX_ENABLE			(1 << 1)
+#define PORT_LEARN_DISABLE		(1 << 0)
+
+/* C - PTP */
+
+#define REG_PTP_PORT_RX_DELAY__2	0x0C00
+#define REG_PTP_PORT_TX_DELAY__2	0x0C02
+#define REG_PTP_PORT_ASYM_DELAY__2	0x0C04
+
+#define REG_PTP_PORT_XDELAY_TS		0x0C08
+#define REG_PTP_PORT_XDELAY_TS_H	0x0C08
+#define REG_PTP_PORT_XDELAY_TS_L	0x0C0A
+
+#define REG_PTP_PORT_SYNC_TS		0x0C0C
+#define REG_PTP_PORT_SYNC_TS_H		0x0C0C
+#define REG_PTP_PORT_SYNC_TS_L		0x0C0E
+
+#define REG_PTP_PORT_PDRESP_TS		0x0C10
+#define REG_PTP_PORT_PDRESP_TS_H	0x0C10
+#define REG_PTP_PORT_PDRESP_TS_L	0x0C12
+
+#define REG_PTP_PORT_TX_INT_STATUS__2	0x0C14
+#define REG_PTP_PORT_TX_INT_ENABLE__2	0x0C16
+
+#define PTP_PORT_SYNC_INT		(1 << 15)
+#define PTP_PORT_XDELAY_REQ_INT		(1 << 14)
+#define PTP_PORT_PDELAY_RESP_INT	(1 << 13)
+
+#define REG_PTP_PORT_LINK_DELAY__4	0x0C18
+
+
+/* Default values are used in ksz_sw_9897.h if these are not defined. */
+#define PRIO_QUEUES			4
+#define RX_PRIO_QUEUES			8
+
+#define KS_PRIO_IN_REG			2
+
+#define SWITCH_PORT_NUM			6
+
+#define KSZ9897_COUNTER_NUM		0x20
+#define TOTAL_KSZ9897_COUNTER_NUM	(KSZ9897_COUNTER_NUM + 2 + 2)
+
+#define SWITCH_COUNTER_NUM		KSZ9897_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ9897_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw_9897.c */
+#define SW_D				u8
+#define SW_R(sw, addr)			(sw)->reg->r8(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w8(sw, addr, val)
+#define SW_SIZE				(1)
+#define SW_SIZE_STR			"%02x"
+#define port_r				port_r8
+#define port_w				port_w8
+
+#define P_BCAST_STORM_CTRL		REG_PORT_MAC_CTRL_0
+#define P_PRIO_CTRL			REG_PORT_MRI_PRIO_CTRL
+#define P_MIRROR_CTRL			REG_PORT_MRI_MIRROR_CTRL
+#define P_STP_CTRL			REG_PORT_LUE_MSTP_STATE
+#define P_PHY_CTRL			REG_PORT_PHY_CTRL
+#define P_NEG_RESTART_CTRL		REG_PORT_PHY_CTRL
+#define P_LINK_STATUS			REG_PORT_PHY_STATUS
+#define P_SPEED_STATUS			REG_PORT_PHY_PHY_CTRL
+#define P_RATE_LIMIT_CTRL		REG_PORT_MAC_IN_RATE_LIMIT
+
+#define S_LINK_AGING_CTRL		REG_SW_LUE_CTRL_1
+#define S_MIRROR_CTRL			REG_SW_MRI_CTRL_0
+#define S_REPLACE_VID_CTRL		REG_SW_MAC_CTRL_2
+#define S_802_1P_PRIO_CTRL		REG_SW_MAC_802_1P_MAP_0
+#define S_TOS_PRIO_CTRL			REG_SW_MAC_TOS_PRIO_0
+#define S_FLUSH_TABLE_CTRL		REG_SW_LUE_CTRL_1
+
+#define REG_SWITCH_RESET		REG_RESET_CTRL
+
+#define SW_FLUSH_DYN_MAC_TABLE		SW_FLUSH_MSTP_TABLE
+
+
+#define MAX_TIMESTAMP_UNIT		2
+#define MAX_TRIG_UNIT			3
+#define MAX_TIMESTAMP_EVENT_UNIT	8
+#define MAX_GPIO			4
+
+#define PTP_TRIG_UNIT_M			((1 << MAX_TRIG_UNIT) - 1)
+#define PTP_TS_UNIT_M			((1 << MAX_TIMESTAMP_UNIT) - 1)
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_cfg_9897.h b/drivers/net/ethernet/micrel/ksz_cfg_9897.h
new file mode 100644
index 0000000..21aa176
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_cfg_9897.h
@@ -0,0 +1,51 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_9897_H
+#define KSZ_CFG_9897_H
+
+#if defined(CONFIG_MICREL_KSZ9897_PTP)
+/* Support 1588 PTP. */
+#define CONFIG_1588_PTP
+#define PTP_SPI
+#endif
+
+#if defined(CONFIG_MICREL_KSZ9897_STP)
+/* Support STP. */
+#define CONFIG_KSZ_STP
+#endif
+
+#ifdef CONFIG_KSZ_STP
+#include <../net/bridge/br_private.h>
+#endif
+
+
+#if 1
+/* Support IBA. */
+#define KSZ_IBA
+#endif
+
+#if 1
+/* Support MRP. */
+#define KSZ_MRP
+#endif
+
+#if 1
+/* Support DLR. */
+#define KSZ_DLR
+#endif
+
+
+#include "ksz_common.h"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.h"
+
+#include "ksz9897.h"
+#include "ksz_sw_9897.h"
+
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz_common.c b/drivers/net/ethernet/micrel/ksz_common.c
new file mode 100644
index 0000000..ea01066
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_common.c
@@ -0,0 +1,298 @@
+/**
+ * Micrel Ethernet driver common code
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2011 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/**
+ * ksz_start_timer - start kernel timer
+ * @info:	Kernel timer information.
+ * @time:	The time tick.
+ *
+ * This routine starts the kernel timer after the specified time tick.
+ */
+static void ksz_start_timer(struct ksz_timer_info *info, int time)
+{
+	info->cnt = 0;
+	info->timer.expires = jiffies + time;
+	add_timer(&info->timer);
+
+	/* infinity */
+	info->max = -1;
+}  /* ksz_start_timer */
+
+/**
+ * ksz_stop_timer - stop kernel timer
+ * @info:	Kernel timer information.
+ *
+ * This routine stops the kernel timer.
+ */
+static void ksz_stop_timer(struct ksz_timer_info *info)
+{
+	if (info->max) {
+		info->max = 0;
+		del_timer_sync(&info->timer);
+	}
+}  /* ksz_stop_timer */
+
+static void ksz_init_timer(struct ksz_timer_info *info, int period,
+	void (*function)(unsigned long), void *data)
+{
+	info->max = 0;
+	info->period = period;
+	init_timer(&info->timer);
+	info->timer.function = function;
+	info->timer.data = (unsigned long) data;
+}  /* ksz_init_timer */
+
+static void ksz_update_timer(struct ksz_timer_info *info)
+{
+	++info->cnt;
+	if (info->max > 0) {
+		if (info->cnt < info->max) {
+			info->timer.expires = jiffies + info->period;
+			add_timer(&info->timer);
+		} else
+			info->max = 0;
+	} else if (info->max < 0) {
+		info->timer.expires = jiffies + info->period;
+		add_timer(&info->timer);
+	}
+}  /* ksz_update_timer */
+
+/* -------------------------------------------------------------------------- */
+
+#define DBG_CH  '-'
+
+#ifdef DEBUG_MSG
+
+/* 2 lines buffer. */
+#define DEBUG_MSG_BUF			(80 * 2)
+
+#define PRINT_MSG_SIZE			(80 * 150)
+#define PRINT_INT_SIZE			(80 * 10)
+
+#define DEBUG_MSG_SIZE			(PRINT_MSG_SIZE + PRINT_INT_SIZE + \
+	DEBUG_MSG_BUF * 2)
+
+struct dbg_print {
+	char *dbg_buf;
+	char *int_buf;
+	char *msg;
+	char *int_msg;
+	int msg_cnt;
+	int int_cnt;
+	int last_msg_line;
+	int last_int_line;
+	unsigned long lock;
+
+	struct work_struct dbg_print;
+	struct ksz_timer_info dbg_timer_info;
+};
+
+static struct dbg_print db;
+
+static void print_buf(char *buf, char **msg, int *cnt, int *last)
+{
+	char ch;
+	char *start;
+
+	if (*last)
+		printk(KERN_INFO "%c\n", DBG_CH);
+	*last = 0;
+	if ('\n' == buf[*cnt - 2] && DBG_CH == buf[*cnt - 1]) {
+		buf[*cnt - 1] = '\0';
+		*last = 1;
+	}
+	*msg = buf;
+
+	/* Kernel seems to limit printk buffer to 1024 bytes. */
+	while (strlen(buf) >= 1024) {
+		start = &buf[1020];
+		while (start != buf && *start != '\n')
+			start--;
+		if (start != buf) {
+			start++;
+			ch = *start;
+			*start = '\0';
+			printk(KERN_INFO "%s", buf);
+			*start = ch;
+			buf = start;
+		}
+	}
+	*cnt = 0;
+	printk(KERN_INFO "%s", buf);
+}  /* print_buf */
+
+static void dbg_print_work(struct work_struct *work)
+{
+	if (db.msg != db.dbg_buf)
+		print_buf(db.dbg_buf, &db.msg, &db.msg_cnt,
+			&db.last_msg_line);
+	if (db.int_msg != db.int_buf) {
+		printk(KERN_INFO "---\n");
+		print_buf(db.int_buf, &db.int_msg, &db.int_cnt,
+			&db.last_int_line);
+	}
+}  /* dbg_print_work */
+
+static void dbg_monitor(unsigned long ptr)
+{
+	struct dbg_print *dbp = (struct dbg_print *) ptr;
+
+	dbg_print_work(&dbp->dbg_print);
+	ksz_update_timer(&dbp->dbg_timer_info);
+}  /* dbg_monitor */
+
+static int init_dbg(void)
+{
+	db.dbg_buf = kmalloc(DEBUG_MSG_SIZE, GFP_KERNEL);
+	if (!db.dbg_buf)
+		return -ENOMEM;
+
+	db.msg = db.dbg_buf;
+	*db.msg = '\0';
+	db.int_buf = db.dbg_buf + PRINT_MSG_SIZE + DEBUG_MSG_BUF;
+	db.int_msg = db.int_buf;
+	*db.int_msg = '\0';
+	db.msg_cnt = db.int_cnt = 0;
+	db.last_msg_line = 1;
+	db.last_int_line = 1;
+	db.lock = 0;
+
+	INIT_WORK(&db.dbg_print, dbg_print_work);
+
+	/* 100 ms timeout */
+	ksz_init_timer(&db.dbg_timer_info, 100 * HZ / 1000, dbg_monitor, &db);
+	ksz_start_timer(&db.dbg_timer_info, db.dbg_timer_info.period);
+
+	return 0;
+}  /* init_dbg */
+
+static void exit_dbg(void)
+{
+	if (db.dbg_buf) {
+		ksz_stop_timer(&db.dbg_timer_info);
+		flush_work(&db.dbg_print);
+
+		if (db.msg != db.dbg_buf)
+			printk(KERN_DEBUG "%s\n", db.dbg_buf);
+		if (db.int_msg != db.int_buf)
+			printk(KERN_DEBUG "%s\n", db.int_buf);
+		kfree(db.dbg_buf);
+		db.dbg_buf = NULL;
+	}
+}  /* exit_dbg */
+#endif
+
+static void dbg_msg(char *fmt, ...)
+{
+#ifdef DEBUG_MSG
+	va_list args;
+	char **msg;
+	int *dbg_cnt;
+	int left;
+	int in_intr = in_interrupt();
+	int n;
+
+	dbg_cnt = &db.msg_cnt;
+	msg = &db.msg;
+	left = PRINT_MSG_SIZE - db.msg_cnt - 1;
+	if (left <= 0) {
+		db.last_msg_line = 1;
+		return;
+	}
+
+	/* Called within interrupt routines. */
+	if (in_intr) {
+		/*
+		 * If not able to get lock then put in the interrupt message
+		 * buffer.
+		 */
+		if (test_bit(1, &db.lock)) {
+			dbg_cnt = &db.int_cnt;
+			msg = &db.int_msg;
+			left = PRINT_INT_SIZE - db.int_cnt - 1;
+			in_intr = 0;
+		}
+	} else
+		set_bit(1, &db.lock);
+	va_start(args, fmt);
+	n = vsnprintf(*msg, left + 1, fmt, args);
+	va_end(args);
+	if (n > 0) {
+		if (left > n)
+			left = n;
+		*dbg_cnt += left;
+		*msg += left;
+	}
+	if (!in_intr)
+		clear_bit(1, &db.lock);
+#endif
+}  /* dbg_msg */
+
+/* -------------------------------------------------------------------------- */
+
+static inline void dbp_mac_addr(u8 *addr)
+{
+	dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x",
+		addr[0], addr[1], addr[2],
+		addr[3], addr[4], addr[5]);
+}  /* dbp_mac_addr */
+
+static inline void dbp_pkt(struct sk_buff *skb, char first, char *msg, int hdr)
+{
+	int i;
+	int len = skb->len;
+	u8 *data = (u8 *) skb->data;
+
+	if (!first || first != data[0]) {
+		if (msg)
+			dbg_msg(msg);
+		if (hdr && len > 0x50)
+			len = 0x50;
+		for (i = 0; i < len; i++) {
+			dbg_msg("%02x ", data[i]);
+			if ((i % 16) == 15)
+				dbg_msg("\n");
+		}
+		if ((i % 16))
+			dbg_msg("\n");
+	}
+}  /* dbp_pkt */
+
+static int get_num_val(const char *buf)
+{
+	int num = -1;
+
+	if ('0' == buf[0] && 'x' == buf[1])
+		sscanf(&buf[2], "%x", (unsigned int *) &num);
+	else if ('0' == buf[0] && 'b' == buf[1]) {
+		int i = 2;
+
+		num = 0;
+		while (buf[i]) {
+			num <<= 1;
+			num |= buf[i] - '0';
+			i++;
+		}
+	} else if ('0' == buf[0] && 'd' == buf[1])
+		sscanf(&buf[2], "%u", &num);
+	else
+		sscanf(buf, "%d", &num);
+	return num;
+}  /* get_num_val */
+
diff --git a/drivers/net/ethernet/micrel/ksz_common.h b/drivers/net/ethernet/micrel/ksz_common.h
new file mode 100644
index 0000000..c4b2bdd
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_common.h
@@ -0,0 +1,83 @@
+/**
+ * Micrel Ethernet driver common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2011 Micrel, Inc.
+ *
+ * This file contains shared structure definitions to be used between network
+ * and switch drivers.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_COMMON_H
+#define KSZ_COMMON_H
+
+
+/* Used to indicate type of flow control support. */
+enum {
+	PHY_NO_FLOW_CTRL,
+	PHY_FLOW_CTRL,
+	PHY_TX_ONLY,
+	PHY_RX_ONLY
+};
+
+/* Used to indicate link connection state. */
+enum {
+	media_connected,
+	media_disconnected,
+	media_unknown
+};
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * struct ksz_timer_info - Timer information data structure
+ * @timer:	Kernel timer.
+ * @cnt:	Running timer counter.
+ * @max:	Number of times to run timer; -1 for infinity.
+ * @period:	Timer period in jiffies.
+ */
+struct ksz_timer_info {
+	struct timer_list timer;
+	int cnt;
+	int max;
+	int period;
+};
+
+/**
+ * struct ksz_counter_info - OS dependent counter information data structure
+ * @counter:	Wait queue to wakeup after counters are read.
+ * @time:	Next time in jiffies to read counter.
+ * @read:	Indication of counters read in full or not.
+ */
+struct ksz_counter_info {
+	wait_queue_head_t counter;
+	unsigned long time;
+	int read;
+};
+
+#define DEV_NAME_SIZE			32
+
+/**
+ * struct ksz_dev_attr - Sysfs data structure
+ * @dev_attr:	Device attribute.
+ * @dev_name:	Attribute name.
+ */
+struct ksz_dev_attr {
+	struct device_attribute dev_attr;
+	char dev_name[DEV_NAME_SIZE];
+};
+
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz_dlr.c b/drivers/net/ethernet/micrel/ksz_dlr.c
new file mode 100644
index 0000000..fd39eba
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_dlr.c
@@ -0,0 +1,5404 @@
+/**
+ * Micrel DLR code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 1
+#define DROP_BEACON_1	1
+#else
+#define DROP_BEACON_1	0
+#endif
+#if 1
+#define DROP_BEACON_0
+
+#endif
+#if 0
+#define DLR_NO_ACL_TIMEOUT
+#endif
+
+
+static u8 MAC_ADDR_BEACON[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x01 };
+static u8 MAC_ADDR_SIGNON[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x02 };
+static u8 MAC_ADDR_ANNOUNCE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x03 };
+static u8 MAC_ADDR_ADVERTISE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x04 };
+static u8 MAC_ADDR_LEARNING_UPDATE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x05 };
+
+
+#define BEACON_TICK			5
+#define BEACON_INTERVAL			(BEACON_TICK * 10)
+
+enum {
+	DLR_ANNOUNCE_NODE,
+	DLR_BEACON_NODE,
+	DLR_SUPERVISOR,
+	DLR_ACTIVE_SUPERVISOR,
+};
+
+enum {
+	DLR_BEGIN,
+
+	DLR_IDLE_STATE,
+	DLR_FAULT_STATE,
+	DLR_NORMAL_STATE,
+	DLR_ACTIVE_STATE,
+	DLR_ACTIVE_FAULT_STATE,
+	DLR_ACTIVE_NORMAL_STATE,
+	DLR_BACKUP_STATE,
+	DLR_PREPARE_STATE,
+	DLR_RESTART_STATE,
+};
+
+#ifdef CONFIG_HAVE_ACL_HW
+
+#define DLR_TIMEOUT_ACL_ENTRY		14
+
+static void setup_acl_beacon(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = DLR_TIMEOUT_ACL_ENTRY - 2;
+	int first_rule = i;
+	int ruleset = (3 << i);
+
+	if (info->overrides & DLR_BEACON_LEAK_HACK)
+		first_rule = 0;
+	acl = &cfg->acl_info[i];
+	mutex_lock(&sw->acllock);
+if (0 == info->attrib.active_super_addr.addr[5])
+dbg_msg("  empty addr!\n");
+	if (!memcmp(acl->mac, info->attrib.active_super_addr.addr, ETH_ALEN) &&
+	    acl->first_rule == first_rule)
+		goto done;
+	acl->first_rule = first_rule;
+	acl->ruleset = ruleset;
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 1;
+	memcpy(acl->mac, info->attrib.active_super_addr.addr, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	sw_w_acl_table(sw, port, i, acl);
+	i++;
+	acl = &cfg->acl_info[i];
+	if (!memcmp(acl->mac, MAC_ADDR_BEACON, ETH_ALEN) &&
+	    acl->first_rule == first_rule)
+		goto done;
+	acl->first_rule = first_rule;
+	acl->ruleset = ruleset;
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, MAC_ADDR_BEACON, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	sw_w_acl_table(sw, port, i, acl);
+
+done:
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_beacon */
+
+static void setup_acl_beacon_timeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = DLR_TIMEOUT_ACL_ENTRY + 1;
+	int ruleset;
+	int first_rule = i;
+	u8 *addr = MAC_ADDR_BEACON;
+
+	i = DLR_TIMEOUT_ACL_ENTRY;
+	ruleset = 1 << i;
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->first_rule = first_rule;
+	acl->ruleset = ruleset;
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_COUNT;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, addr, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	if (info->beacon_timeout > ACL_CNT_M) {
+		int cnt = info->beacon_timeout + 500;
+
+		cnt /= 1000;
+		acl->cnt = cnt;
+		acl->msec = 1;
+	} else {
+		acl->cnt = info->beacon_timeout;
+		acl->msec = 0;
+	}
+	acl->intr_mode = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	++i;
+	acl = &cfg->acl_info[i];
+	acl->map_mode = ACL_MAP_MODE_DISABLE;
+	acl->ports = 0;
+	sw_w_acl_action(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_beacon_timeout */
+
+static void setup_acl_beacon_drop(struct ksz_dlr_info *info, int drop)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i = DLR_TIMEOUT_ACL_ENTRY - 2;
+	int p;
+
+if (0 == info->attrib.active_super_addr.addr[5])
+dbg_msg("  %s\n", __func__);
+	for (p = 0; p < 2; p++)
+		setup_acl_beacon(info, info->ports[p]);
+	if (info->overrides & DLR_BEACON_LEAK_HACK)
+		i = 0;
+	mutex_lock(&sw->acllock);
+	for (p = 0; p < 2; p++) {
+		cfg = &sw->info->port_cfg[info->ports[p]];
+
+		acl = &cfg->acl_info[i];
+		acl->map_mode = ACL_MAP_MODE_REPLACE;
+		acl->ports = drop & info->member;
+		if (!drop)
+			acl->map_mode = ACL_MAP_MODE_DISABLE;
+		sw_w_acl_action(sw, info->ports[p], i, acl);
+	}
+	mutex_unlock(&sw->acllock);
+
+	if (drop)
+		return;
+
+	/* Ring state may be changed in the beacon. */
+	memset(&info->beacon_info[0].last, 0, sizeof(struct ksz_dlr_beacon));
+	memset(&info->beacon_info[1].last, 0, sizeof(struct ksz_dlr_beacon));
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 0;
+}  /* setup_acl_beacon_drop */
+
+static void setup_vlan_table(struct ksz_dlr_info *info, u16 vid, int set)
+{
+	struct ksz_vlan_table vlan;
+	struct ksz_sw *sw = info->sw_dev;
+
+	memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+	vlan.vid = vid;
+	if (set) {
+		vlan.ports = sw->PORT_MASK;
+		vlan.valid = true;
+	}
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+}  /* setup_vlan_table */
+#endif
+
+#ifdef CONFIG_HAVE_DLR_HW
+static void dlr_hw_set_state(struct ksz_sw *sw, u8 node, u8 ring)
+{
+	u8 data;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_STATE__1);
+	data &= ~DLR_RING_STATE_NORMAL;
+	data &= ~(DLR_NODE_STATE_M << DLR_NODE_STATE_S);
+	node &= DLR_NODE_STATE_M;
+	node <<= DLR_NODE_STATE_S;
+	ring &= DLR_RING_STATE_NORMAL;
+	data |= node | ring;
+	SW_W(sw, REG_DLR_STATE__1, data);
+}
+
+static void dlr_hw_set_supervisor(struct ksz_sw *sw, u8 super)
+{
+	u8 data;
+	u8 saved;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	saved = data;
+	data &= ~DLR_BEACON_TX_ENABLE;
+	data &= ~DLR_BACKUP_AUTO_ON;
+	if (DLR_ACTIVE_SUPERVISOR == super)
+		data |= DLR_BEACON_TX_ENABLE;
+	if (DLR_SUPERVISOR == super && (saved & DLR_BACKUP_AUTO_ON)) {
+
+		/* Turn off previous automatic start first. */
+		SW_W(sw, REG_DLR_CTRL__1, data);
+dbg_msg("  reset backup: %x\n", saved);
+		data |= DLR_BACKUP_AUTO_ON;
+	}
+	data |= DLR_ASSIST_ENABLE;
+	SW_W(sw, REG_DLR_CTRL__1, data);
+dbg_msg("%s %x\n", __func__, data);
+}
+
+static void dlr_hw_set_dest_addr(struct ksz_sw *sw, u8 *addr)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w(sw, REG_DLR_DEST_ADDR_0, addr, ETH_ALEN);
+}
+
+static void dlr_hw_set_ip_addr(struct ksz_sw *sw, u32 ip_addr)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_IP_ADDR__4, ip_addr);
+}
+
+static void dlr_hw_reset_seq_id(struct ksz_sw *sw)
+{
+	u8 data;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	SW_W(sw, REG_DLR_CTRL__1, data | DLR_RESET_SEQ_ID);
+	SW_W(sw, REG_DLR_CTRL__1, data & ~DLR_RESET_SEQ_ID);
+}
+
+static void dlr_hw_set_port_map(struct ksz_sw *sw, u32 ports)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_PORT_MAP__4, ports);
+}
+
+static void dlr_hw_set_precedence(struct ksz_sw *sw, u8 precedence)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w8(sw, REG_DLR_PRECEDENCE__1, precedence);
+}
+
+static void dlr_hw_set_interval(struct ksz_sw *sw, u16 interval)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_BEACON_INTERVAL__4, interval);
+}
+
+static void dlr_hw_set_timeout(struct ksz_sw *sw, u32 timeout)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_BEACON_TIMEOUT__4, timeout);
+}
+
+static void dlr_hw_set_vlan_id(struct ksz_sw *sw, u16 vid)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w16(sw, REG_DLR_VLAN_ID__2, vid);
+}
+#endif
+
+
+#define DLR_LEARNING_ENTRY		3
+#define DLR_BEACON_ENTRY		4
+#define DLR_ANNOUNCE_ENTRY		5
+#define DLR_SIGNON_ENTRY		6
+#define DLR_SUPERVISOR_ENTRY		7
+
+static void sw_setup_dlr(struct ksz_sw *sw)
+{
+	int p;
+	struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+	/* Enable 802.1p priority to give highest priority to beacon frame. */
+	sw_ena_802_1p(sw, dlr->ports[0]);
+	sw_ena_802_1p(sw, dlr->ports[1]);
+	sw_ena_802_1p(sw, sw->HOST_PORT);
+
+	/* DLR ports only communicate with host port. */
+	sw_cfg_port_base_vlan(sw, dlr->ports[0], sw->HOST_MASK | dlr->member);
+	sw_cfg_port_base_vlan(sw, dlr->ports[1], sw->HOST_MASK | dlr->member);
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (p == sw->HOST_PORT)
+			continue;
+		if (p == dlr->ports[0] || p == dlr->ports[1])
+			continue;
+		sw_cfg_port_base_vlan(sw, p, sw->PORT_MASK & ~dlr->member);
+	}
+
+	/* Need to create VLAN id 0 entry in the VLAN table. */
+	port_cfg(sw, dlr->ports[0], REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+		true);
+	port_cfg(sw, dlr->ports[1], REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+		true);
+
+	/* Need to receive beacon frame with changed VID. */
+	sw->reg->w32(sw, REG_SW_LUE_UNK_VID_CTRL__4,
+		sw->HOST_MASK | SW_UNK_VID_ENABLE);
+
+#ifdef CONFIG_HAVE_DLR_HW
+	dlr_hw_set_dest_addr(sw, MAC_ADDR_BEACON);
+	dlr_hw_set_port_map(sw, dlr->member);
+#endif
+	sw->ops->release(sw);
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		false, false, 0);
+	if (DLR_ANNOUNCE_NODE == sw->info->dlr.node)
+		sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON,
+			dlr->member, false, false, 0);
+	sw->ops->acquire(sw);
+}  /* sw_setup_dlr */
+
+enum {
+	DEV_DLR_CHK_HW,
+	DEV_DLR_CLR_SUPER,
+	DEV_DLR_SETUP_DIR,
+	DEV_DLR_SETUP_DROP,
+	DEV_DLR_SETUP_TIMEOUT,
+	DEV_DLR_SETUP_VID,
+	DEV_DLR_FLUSH,
+	DEV_DLR_STOP,
+	DEV_DLR_UPDATE,
+	DEV_DLR_RX,
+	DEV_DLR_START_ANNOUNCE,
+	DEV_DLR_TX_ANNOUNCE,
+	DEV_DLR_TX_LOCATE,
+	DEV_DLR_TX_SIGNON,
+	DEV_DLR_TX_REQ,
+	DEV_DLR_TX_RESP,
+	DEV_DLR_TX_STATUS,
+	DEV_DLR_TX_ADVERTISE,
+	DEV_DLR_TX_FLUSH_TABLES,
+	DEV_DLR_TX_LEARNING_UPDATE,
+};
+
+static void wait_for_timeout(u32 microsec)
+{
+#if 0
+	microsec *= 1000;
+#endif
+#if 0
+dbg_msg("%s %u %lx\n", __func__, microsec, jiffies);
+#endif
+	if (microsec >= 20000) {
+		microsec /= 1000;
+		delay_milli(microsec);
+dbg_msg(" W %lx\n", jiffies);
+	} else
+		delay_micro(microsec);
+}  /* wait_for_timeout */
+
+static void dlr_set_addr(struct ksz_dlr_active_node *node, u32 ip_addr,
+	u8 *addr)
+{
+	node->ip_addr = ip_addr;
+	memcpy(node->addr, addr, ETH_ALEN);
+}  /* dlr_set_addr */
+
+static int dlr_xmit(struct ksz_dlr_info *info, u16 ports)
+{
+	int rc;
+	struct sk_buff *skb;
+	u8 *frame = info->tx_frame;
+	int len = info->len;
+	const struct net_device_ops *ops = info->dev->netdev_ops;
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(info->dev) || !netif_carrier_ok(info->dev))
+		return 0;
+
+	/* Do not send to port if its link is lost. */
+	if ((ports & (1 << info->ports[0])) && info->p1_down)
+		ports &= ~(1 << info->ports[0]);
+	if ((ports & (1 << info->ports[1])) && info->p2_down)
+		ports &= ~(1 << info->ports[1]);
+
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, info->tx_frame, len);
+
+	skb_put(skb, len);
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	skb->protocol = htons(DLR_TAG_TYPE);
+	skb->dev = info->dev;
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(info->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* dlr_xmit */
+
+static int prep_dlr_beacon(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_BEACON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_BEACON;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid_beacon++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.beacon.ring_state = dlr->ring_state;
+	frame->data.beacon.precedence = dlr->precedence;
+	frame->data.beacon.interval = htonl(dlr->beacon_interval);
+	frame->data.beacon.timeout = htonl(dlr->beacon_timeout);
+	return sizeof(struct ksz_dlr_beacon);
+}  /* prep_dlr_beacon */
+
+static int prep_dlr_announce(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_ANNOUNCE;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.announce.ring_state = dlr->ring_state;
+	return sizeof(struct ksz_dlr_announce);
+}  /* prep_dlr_announce */
+
+static int prep_dlr_link_status(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, dlr->attrib.active_super_addr.addr,
+		ETH_ALEN);
+	frame->hdr.frame_type = DLR_LINK_STATUS;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.status.port1_active = 1;
+	frame->data.status.port2_active = 1;
+
+	/* Link down has higher priority. */
+	if (dlr->p1_down || dlr->p2_down) {
+		frame->data.status.neighbor = 0;
+		if (dlr->p1_down)
+			frame->data.status.port1_active = 0;
+		if (dlr->p2_down)
+			frame->data.status.port2_active = 0;
+	} else {
+		frame->data.status.neighbor = 1;
+		if (dlr->p1_lost)
+			frame->data.status.port1_active = 0;
+		if (dlr->p2_lost)
+			frame->data.status.port2_active = 0;
+	}
+	return sizeof(struct ksz_dlr_status);
+}  /* prep_dlr_link_status */
+
+static int prep_dlr_locate_fault(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_LOCATE_FAULT;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	return 0;
+}  /* prep_dlr_locate_fault */
+
+static int prep_dlr_neigh_chk_req(struct ksz_dlr_info *dlr, int port)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_SIGNON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_NEIGH_CHK_REQ;
+	frame->hdr.src_port = port ? DLR_PORT_2 : DLR_PORT_1;
+	dlr->seqid_chk[port] = dlr->seqid++;
+	if (1 == dlr->port_chk[port])
+		dlr->seqid_first[port] = dlr->seqid_chk[port];
+	frame->hdr.seqid = htonl(dlr->seqid_chk[port]);
+
+	memset(frame->data.reserved, 0, 30);
+	return 0;
+}  /* prep_dlr_neigh_chk_req */
+
+static int prep_dlr_neigh_chk_resp(struct ksz_dlr_info *dlr, u32 seqid, u8 in,
+	int out)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_SIGNON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_NEIGH_CHK_RESP;
+	frame->hdr.src_port = out ? DLR_PORT_2 : DLR_PORT_1;
+	frame->hdr.seqid = htonl(seqid);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.neigh_chk_resp.src_port = in;
+	return sizeof(struct ksz_dlr_neigh_chk_resp);
+}  /* prep_dlr_neigh_chk_resp */
+
+static int prep_dlr_signon(struct ksz_dlr_info *dlr, int len)
+{
+	int i;
+	struct ksz_dlr_tx_frame *base = (struct ksz_dlr_tx_frame *)
+		dlr->signon_frame;
+	struct ksz_dlr_frame *frame = &base->body;
+	u16 num = ntohs(frame->data.signon.num);
+	struct ksz_dlr_node *node = frame->data.signon.node;
+	int space = 1000;
+
+	/* The very first signon frame. */
+	if (!len) {
+		memcpy(base->vlan.h_dest, dlr->signon_addr, ETH_ALEN);
+		frame->hdr.frame_type = DLR_SIGN_ON;
+		frame->hdr.src_port = dlr->tx_port;
+		dlr->seqid_signon = dlr->seqid;
+		frame->hdr.seqid = htonl(dlr->seqid++);
+		num = 0;
+		len = sizeof(struct vlan_ethhdr) +
+			sizeof(struct ksz_dlr_hdr) +
+			sizeof(struct ksz_dlr_signon) -
+			sizeof(struct ksz_dlr_node);
+		if ((dlr->overrides & DLR_TEST) && dlr->signon_space)
+			space = dlr->signon_space + 1;
+	}
+	memcpy(base->vlan.h_source, dlr->src_addr, ETH_ALEN);
+	if (len + sizeof(struct ksz_dlr_node) > 1500) {
+		memcpy(base->vlan.h_dest, dlr->attrib.active_super_addr.addr,
+			ETH_ALEN);
+		frame->hdr.src_port = dlr->rx_port;
+		dlr->signon_port = dlr->rx_port;
+		return len;
+	}
+	for (i = 0; i < num; i++)
+		node++;
+	do {
+		num++;
+		memcpy(node->addr, dlr->src_addr, ETH_ALEN);
+		node->ip_addr = htonl(dlr->ip_addr);
+		len += sizeof(struct ksz_dlr_node);
+		node++;
+	} while (len + sizeof(struct ksz_dlr_node) * space <= 1500);
+	frame->data.signon.num = htons(num);
+
+	dlr->signon_port = dlr->tx_port;
+	return len;
+}  /* prep_dlr_signon */
+
+static int prep_dlr_advertise(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ADVERTISE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_ADVERTISE;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.advertise.state = DLR_GW_ACTIVE_LISTEN_STATE;
+	frame->data.advertise.precedence = dlr->precedence;
+	frame->data.advertise.interval = htonl(dlr->beacon_interval);
+	frame->data.advertise.timeout = htonl(dlr->beacon_timeout);
+	frame->data.advertise.learning_update_enable = 1;
+	return sizeof(struct ksz_dlr_advertise);
+}  /* prep_dlr_advertise */
+
+static int prep_dlr_flush_tables(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_FLUSH_TABLES;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.flush.learning_update_enable = 1;
+	return sizeof(struct ksz_dlr_flush_tables);
+}  /* prep_dlr_flush_tables */
+
+static int prep_dlr_learning_update(struct ksz_dlr_info *dlr)
+{
+	dlr->update_frame.hdr.seqid = htonl(dlr->seqid++);
+
+	return sizeof(struct ksz_dlr_update_frame);
+}  /* prep_dlr_learning_update */
+
+static void dlr_tx_beacon(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_beacon(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_beacon */
+
+static int dbg_ann;
+static void dlr_tx_announce(struct ksz_dlr_info *dlr)
+{
+	int rc;
+	u16 ports = dlr->member;
+
+	if (RING_NORMAL_STATE == dlr->ring_state && dlr->state != DLR_ACTIVE_NORMAL_STATE)
+dbg_msg("  tx ann ?\n");
+if (dbg_ann > 0) {
+dbg_msg(" tx ann: %d %d %x %lx\n", dlr->state, dlr->ring_state, dlr->seqid,
+jiffies);
+dbg_ann--;
+}
+	if (RING_NORMAL_STATE == dlr->ring_state)
+		ports = 1 << dlr->ports[dlr->port];
+
+	dlr->len = prep_dlr_announce(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, ports);
+
+	/* First delayed announce from initial fault state sent. */
+	if (dlr->ann_delay)
+		dlr->ann_delay = 0;
+}  /* dlr_tx_announce */
+
+static void dlr_tx_chk_req(struct ksz_dlr_info *dlr, int port)
+{
+	int rc;
+
+	dlr->len = prep_dlr_neigh_chk_req(dlr, port);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, (1 << dlr->ports[port]));
+}  /* dlr_tx_chk_req */
+
+static void dlr_tx_chk_resp(struct ksz_dlr_info *dlr, int port)
+{
+	int rc;
+
+	dlr->len = prep_dlr_neigh_chk_resp(dlr, dlr->seqid_rcv[port],
+		dlr->port_rcv[port], port);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, (1 << dlr->ports[port]));
+}  /* dlr_tx_chk_resp */
+
+static void dlr_tx_status(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_link_status(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, 0);
+}  /* dlr_tx_status */
+
+static void dlr_tx_signon(struct ksz_dlr_info *dlr, int len)
+{
+	int rc;
+
+#if 0
+dbg_msg("%s\n", __func__);
+#endif
+	dlr->len = prep_dlr_signon(dlr, len);
+	dlr->tx_frame = dlr->signon_frame;
+	rc = dlr_xmit(dlr, (1 << dlr->ports[dlr->signon_port]));
+	dlr->tx_frame = (u8 *) &dlr->frame;
+}  /* dlr_tx_signon */
+
+static void dlr_tx_locate_fault(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_locate_fault(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_locate_fault */
+
+static void dlr_tx_learning_update(struct ksz_dlr_info *dlr)
+{
+	int rc;
+	u16 ports = dlr->member;
+
+	dlr->len = prep_dlr_learning_update(dlr);
+	dlr->tx_frame = (u8 *) &dlr->update_frame;
+
+	/* Attempt to notify the other supervisor about incoming beacons. */
+	if (dlr->rogue_super) {
+		memcpy(dlr->update_frame.eth.h_dest,
+			&dlr->rogue_super->prec_addr[1], ETH_ALEN);
+		ports = (1 << dlr->ports[dlr->rogue_super->port]);
+	}
+	rc = dlr_xmit(dlr, ports);
+
+	/* Reset default Learning_Update address. */
+	if (dlr->rogue_super) {
+		memcpy(dlr->update_frame.eth.h_dest, MAC_ADDR_LEARNING_UPDATE,
+			ETH_ALEN);
+		dlr->rogue_super = NULL;
+	}
+	dlr->tx_frame = (u8 *) &dlr->frame;
+}  /* dlr_tx_learning_update */
+
+static void dlr_tx_advertise(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_advertise(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_advertise */
+
+static void dlr_tx_flush_tables(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_flush_tables(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_flush_tables */
+
+static void flushMacTable(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+#if 0
+dbg_msg("%s\n", __func__);
+#endif
+	sw->ops->acquire(sw);
+	sw->ops->flush_table(sw, sw->mib_port_cnt);
+	sw->ops->release(sw);
+#if 0
+dbg_msg(" %s.\n", __func__);
+#endif
+}
+
+static void enableOnePort(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	int block_port = (info->port + 1) & 1;
+
+#if 0
+dbg_msg("%s %d\n", __func__, info->port);
+#endif
+	sw->ops->acquire(sw);
+	port_set_stp_state(sw, info->ports[block_port], STP_STATE_BLOCKED);
+	sw->ops->release(sw);
+}
+
+static void enableBothPorts(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+#if 0
+dbg_msg("%s\n", __func__);
+#endif
+	sw->ops->acquire(sw);
+	port_set_stp_state(sw, info->ports[0], STP_STATE_FORWARDING);
+	port_set_stp_state(sw, info->ports[1], STP_STATE_FORWARDING);
+	sw->ops->release(sw);
+}
+
+static void dlr_set_state(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u8 node;
+	u8 ring;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (DLR_IDLE_STATE == info->state)
+		node = DLR_NODE_STATE_IDLE;
+	else if (DLR_NORMAL_STATE == info->state ||
+		 DLR_ACTIVE_NORMAL_STATE == info->state)
+		node = DLR_NODE_STATE_NORMAL;
+	else
+		node = DLR_NODE_STATE_FAULT;
+	ring = RING_FAULT_STATE == info->ring_state ?
+		DLR_RING_STATE_FAULT : DLR_RING_STATE_NORMAL;
+
+	/* Backup supervisor needs to send ring fault with the first beacon. */
+	if (DLR_ACTIVE_SUPERVISOR != info->node ||
+	    DLR_RESTART_STATE == info->state)
+		ring = DLR_RING_STATE_FAULT;
+	sw->ops->acquire(sw);
+	dlr_hw_set_state(sw, node, ring);
+	sw->ops->release(sw);
+#if 0
+dbg_msg(" set state: s=%d r=%d; N=%d R=%d\n", info->state,
+info->ring_state, node, ring);
+#endif
+
+	/* Make sure beacon is sent with new ring state. */
+	if (DLR_ACTIVE_SUPERVISOR == info->node)
+		wait_for_timeout(info->beacon_interval * 2);
+#endif
+}  /* dlr_set_state */
+
+static void dlr_chk_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u32 data;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->ops->acquire(sw);
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	if (data & DLR_BEACON_TX_ENABLE) {
+dbg_msg("%s %x\n", __func__, data);
+		if (info->node != DLR_ACTIVE_SUPERVISOR) {
+			data &= ~DLR_BEACON_TX_ENABLE;
+			SW_W(sw, REG_DLR_CTRL__1, data);
+dbg_msg(" tx off\n");
+		}
+	} else if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			data |= DLR_BEACON_TX_ENABLE;
+			SW_W(sw, REG_DLR_CTRL__1, data);
+dbg_msg(" tx on\n");
+	}
+	sw->ops->release(sw);
+#endif
+	info->chk_hw = 0;
+}  /* dlr_chk_supervisor */
+
+static void dlr_set_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u8 node;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (info->node < DLR_SUPERVISOR)
+		node = DLR_BEACON_NODE;
+	else if (DLR_ACTIVE_SUPERVISOR == info->node)
+		node = DLR_ACTIVE_SUPERVISOR;
+	else
+		node = DLR_SUPERVISOR;
+	sw->ops->acquire(sw);
+	dlr_hw_set_supervisor(sw, node);
+dbg_msg("%s %d %d\n", __func__, node, info->node);
+	sw->ops->release(sw);
+#endif
+}  /* dlr_set_supervisor */
+
+static void setupBeacons(struct ksz_dlr_info *info)
+{
+	int use_hw = false;
+
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (sw->features & REDUNDANCY_SUPPORT) {
+		use_hw = true;
+	}
+#endif
+	if (use_hw) {
+		dlr_set_state(info);
+	} else {
+		info->interval = 0;
+		dlr_tx_beacon(info);
+	}
+}  /* setupBeacons */
+
+static void disableSupervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+dbg_msg("%s\n", __func__);
+	sw->ops->acquire(sw);
+	sw->ops->cfg_src_filter(sw, 1);
+	sw->ops->release(sw);
+	dlr_set_supervisor(info);
+	info->start = 0;
+	sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE, 0, false,
+		false, 0);
+	sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON, 0, false,
+		false, 0);
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		false, false, 0);
+	sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY, MAC_ADDR_LEARNING_UPDATE, 0,
+		false, false, 0);
+}  /* disableSupervisor */
+
+static void enableSupervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+#if 0
+dbg_msg("%s\n", __func__);
+#endif
+	sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE, 0, true,
+		false, 0);
+	sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON, sw->HOST_MASK,
+		!DROP_BEACON_1, false, 0);
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		true, false, 0);
+	sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY, MAC_ADDR_LEARNING_UPDATE, 0,
+		true, false, 0);
+	sw->ops->acquire(sw);
+	sw->ops->cfg_src_filter(sw, 0);
+#ifdef CONFIG_HAVE_DLR_HW
+	dlr_hw_reset_seq_id(sw);
+	dlr_hw_set_precedence(sw, info->precedence);
+	dlr_hw_set_interval(sw, info->beacon_interval);
+	dlr_hw_set_timeout(sw, info->beacon_timeout);
+	dlr_hw_set_vlan_id(sw, info->vid);
+	dlr_hw_set_ip_addr(sw, info->ip_addr);
+#endif
+	sw->ops->release(sw);
+	info->start = 1;
+	dlr_set_supervisor(info);
+}  /* enableSupervisor */
+
+static void disableAnnounce(struct ksz_dlr_info *info)
+{
+dbg_msg("%s\n", __func__);
+	cancel_delayed_work_sync(&info->announce_tx);
+}
+
+static void startAnnounce(struct ksz_dlr_info *info)
+{
+dbg_msg("%s\n", __func__);
+	cancel_delayed_work_sync(&info->announce_tx);
+	schedule_delayed_work(&info->announce_tx, 100);
+	dlr_tx_announce(info);
+}  /* startAnnounce */
+
+static void enableAnnounce(struct ksz_dlr_info *info, int delay)
+{
+#if 1
+dbg_msg("%s %d %d %lx\n", __func__, delay, info->ann_delay,
+jiffies);
+#endif
+	if (1 == delay) {
+
+		/* Wait for 2 * beacon timeout before sending announce. */
+		if (!info->ann_delay) {
+			info->ann_delay = 1;
+			cancel_delayed_work_sync(&info->announce_tx);
+			schedule_delayed_work(&info->announce_tx, 0);
+		}
+
+	/* Wait until first announce is sent. */
+	} else if (!info->ann_delay)
+		startAnnounce(info);
+}
+
+static void disableAnnounceTimeout(struct ksz_dlr_info *info)
+{
+dbg_msg("%s\n", __func__);
+	ksz_stop_timer(&info->announce_timeout_timer_info);
+}
+
+static void disableNeighChkTimers(struct ksz_dlr_info *info)
+{
+#if 0
+dbg_msg("%s\n", __func__);
+#endif
+	info->p1_lost = info->p2_lost = 0;
+	info->port_chk[0] = info->port_chk[1] = 0;
+	ksz_stop_timer(&info->neigh_chk_timer_info);
+	info->neigh_chk = 0;
+}
+
+static void disableSignOnTimer(struct ksz_dlr_info *info)
+{
+#if 0
+dbg_msg("%s\n", __func__);
+#endif
+	ksz_stop_timer(&info->signon_timer_info);
+	info->signon_start = 0;
+}  /* disableSignOnTimer */
+
+static void updateValues(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_sw *sw = info->sw_dev;
+	int vid_change = false;
+
+dbg_msg("%s\n", __func__);
+	if (info->vid != attrib->super_cfg.vid) {
+		vid_change = true;
+		setup_vlan_table(info, info->vid, false);
+	}
+	sw->ops->acquire(sw);
+	if (info->precedence != attrib->super_cfg.prec) {
+		info->precedence = attrib->super_cfg.prec;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_precedence(sw, info->precedence);
+#endif
+	}
+	if (info->beacon_interval != attrib->super_cfg.beacon_interval) {
+dbg_msg("%s %u %u\n", __func__, info->beacon_interval, attrib->super_cfg.beacon_interval);
+		info->beacon_interval = attrib->super_cfg.beacon_interval;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_interval(sw, info->beacon_interval);
+#endif
+	}
+	if (info->beacon_timeout != attrib->super_cfg.beacon_timeout) {
+		info->beacon_timeout = attrib->super_cfg.beacon_timeout;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_timeout(sw, info->beacon_timeout);
+#endif
+	}
+	if (info->vid != attrib->super_cfg.vid) {
+		info->vid = attrib->super_cfg.vid;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_vlan_id(sw, info->vid);
+#endif
+	}
+	sw->ops->release(sw);
+	if (vid_change)
+		setup_vlan_table(info, info->vid, true);
+	info->new_val = 0;
+	info->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+	memcpy(info->signon_frame, &info->frame, sizeof(struct vlan_ethhdr));
+}  /* updateValues */
+
+static void dlr_flush(struct ksz_dlr_info *info)
+{
+	flushMacTable(info);
+}
+
+static void setupDir(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	int cmp;
+
+	/* Do not change entry of own address. */
+	cmp = memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+		ETH_ALEN);
+	if (!cmp)
+		return;
+	if (port >= 0)
+		port = 1 << info->ports[port];
+	else
+		port = 0;
+	info->active_port = port;
+	sw->ops->cfg_mac(sw, DLR_SUPERVISOR_ENTRY,
+		info->attrib.active_super_addr.addr, port, false, false, 0);
+dbg_msg("%s %x %02x:%02x:%02x\n", __func__, port,
+info->attrib.active_super_addr.addr[3],
+info->attrib.active_super_addr.addr[4],
+info->attrib.active_super_addr.addr[5]);
+}
+
+static void proc_dlr_cmd(struct ksz_dlr_info *dlr, struct dlr_work *parent);
+
+static int inside_state;
+static int inside_cmd;
+static int proc_dlr_hw_access(struct ksz_dlr_info *dlr, int cmd, int subcmd,
+	int option, struct sk_buff *skb)
+{
+	struct dlr_work *work;
+	int ret = 0;
+
+if (inside_state) {
+dbg_msg(" ? %d ", subcmd);
+inside_cmd = subcmd;
+}
+	work = &dlr->work_info.works[dlr->work_info.tail];
+	if (work->used) {
+		pr_alert("work full\n");
+		return -EFAULT;
+	}
+	work->skb = skb;
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	work->used = true;
+#if 0
+if (work->prev->used && !in_interrupt())
+dbg_msg(" used: %d %d %d %x %d; %p %p\n", dlr->work_info.tail, subcmd, work->prev->subcmd,
+work->prev->index, dlr->work_info.ready, work, work->prev);
+#endif
+	dlr->work_info.tail++;
+	dlr->work_info.tail &= DLR_WORK_LAST;
+	if (!work->prev->used) {
+dlr->work_info.ready = 1;
+		schedule_work(&dlr->work_info.work);
+}
+	return ret;
+}  /* proc_dlr_hw_access */
+
+#define announcedState		(info->ring_state)
+#define fromRingState		(RING_FAULT_STATE == info->ring_state ?	\
+	DLR_FAULT_STATE : DLR_NORMAL_STATE)
+#define announceRcvd		(info->ann_rcvd)
+#define announceTimeout		(info->ann_timeout)
+#define oneBeaconRcvd		(info->one_rcvd)
+#define twoBeaconsRcvd		(info->both_rcvd)
+#define oneBeaconTimeout	(info->one_timeout)
+#define twoBeaconsTimeout	(info->both_timeout)
+#define newSupervisor		(info->new_supervisor)
+#define newValue		(info->new_val)
+#define backupSupervisor	(DLR_SUPERVISOR == info->node)
+#define faultState		(RING_FAULT_STATE == info->ring_state)
+#define linkDown		(info->both_down)
+#define linkLoss		(info->one_down)
+#define linkStatus		(info->p1_lost || info->p2_lost)
+
+
+static int dbg_bcn;
+static void acceptBeacons_(struct ksz_dlr_info *dlr)
+{
+dbg_msg("%s\n", __func__);
+dbg_bcn += 4;
+	dlr->skip_beacon = false;
+#ifdef CONFIG_HAVE_ACL_HW
+#if (0 == DROP_BEACON_1)
+	if (dlr->node != DLR_ACTIVE_SUPERVISOR)
+#endif
+	setup_acl_beacon_drop(dlr, 0);
+#endif
+	dlr->drop_beacon = false;
+}  /* acceptBeacons_ */
+
+static void acceptBeacons(struct ksz_dlr_info *info)
+{
+dbg_msg("  %d %d; ", info->beacon_info[0].timeout, info->beacon_info[1].timeout);
+dbg_msg("%s\n", __func__);
+	info->skip_beacon = false;
+	proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_SETUP_DROP, 0,
+		NULL);
+	info->drop_beacon = false;
+}  /* acceptBeacons */
+
+#ifdef DROP_BEACON_0
+static void dropBeacons(struct ksz_dlr_info *info)
+{
+	int drop = info->member;
+
+	if (info->node == DLR_ACTIVE_SUPERVISOR)
+		drop = 0x8000;
+dbg_msg("%s\n", __func__);
+	proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_SETUP_DROP, drop,
+		NULL);
+	info->drop_beacon = true;
+}  /* dropBeacons */
+#endif
+
+static void setupBeaconTimeout(struct ksz_dlr_info *info, int port)
+{
+#ifndef DLR_NO_ACL_TIMEOUT
+dbg_msg("  setup timeout: %d\n", port);
+	proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_SETUP_TIMEOUT, port,
+		NULL);
+#endif
+}  /* setupBeanconTimeout */
+
+static int seq_ahead(u32 start, u32 num)
+{
+	u32 ahead;
+	u32 behind;
+
+	ahead = num - start;
+	behind = start - num;
+	return ahead < behind;
+}
+
+static int seq_in(u32 start, u32 end, u32 num)
+{
+	end -= start;
+	num -= start;
+	return 0 <= num && num <= end;
+}
+
+static struct ksz_dlr_node_info *find_dlr_node(struct ksz_dlr_info *info,
+	u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < info->attrib.participants_cnt; i++)
+		if (!memcpy(info->nodes[i].signon.addr, addr, ETH_ALEN))
+			return &info->nodes[i];
+	return NULL;
+}  /* find_dlr_node */
+
+static int bcn_cnt;
+static int dbg_leak = 5;
+
+static void dbg_dlr(struct ksz_dlr_info *info, char *msg)
+{
+	dbg_msg(" %s: %d %d ", msg, info->state, info->ring_state);
+	dbg_msg("d=%d:%d D=%d:%d l=%d:%d r=%d:%d R=%d:%d t=%d:%d T=%d:%d\n",
+		info->p1_down, info->p2_down,
+		info->one_down, info->both_down,
+		info->p1_lost, info->p2_lost,
+		info->p1_rcvd, info->p2_rcvd,
+		info->one_rcvd, info->both_rcvd,
+		info->p1_timeout, info->p2_timeout,
+		info->one_timeout, info->both_timeout);
+}
+
+static void dlr_print(struct ksz_dlr_info *info, char *msg)
+{
+	if (info->overrides & DLR_TEST)
+		printk(KERN_INFO "%s\n", msg);
+}
+
+static int dbg_active;
+static void dlr_chk_beacon_timeout(struct ksz_dlr_info *info, int p,
+	struct ksz_dlr_frame *beacon,
+	struct ksz_dlr_super_info *active,
+	struct ksz_dlr_super_info *super)
+{
+	u32 crc;
+	int i;
+	struct ksz_dlr_super_info *next;
+	struct ksz_dlr_super_info *first = NULL;
+	struct ksz_dlr_super_info *found = NULL;
+
+	crc = ether_crc(ETH_ALEN + 1, super->prec_addr);
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (!next->crc && !first)
+			first = next;
+		if (next->crc == crc) {
+			found = next;
+			break;
+		}
+	}
+	if (!found)
+		found = first;
+	if (found) {
+		found->cnt++;
+		found->port = p;
+
+		/* First time. */
+		if (!found->crc) {
+dbg_msg("  %x=%02x:%02x:%02x:%02x:%02x:%02x  %x=%02x:%02x:%02x:%02x:%02x:%02x\n",
+active->prec_addr[0],
+active->prec_addr[1],
+active->prec_addr[2],
+active->prec_addr[3],
+active->prec_addr[4],
+active->prec_addr[5],
+active->prec_addr[6],
+super->prec_addr[0],
+super->prec_addr[1],
+super->prec_addr[2],
+super->prec_addr[3],
+super->prec_addr[4],
+super->prec_addr[5],
+super->prec_addr[6]);
+dbg_msg("cnt: %d %x\n", found->cnt, crc);
+			found->crc = crc;
+			memcpy(found->prec_addr, super->prec_addr,
+				ETH_ALEN + 1);
+		}
+		found->timeout[p] += ntohl(beacon->data.beacon.interval);
+		if (found->timeout[p] > info->beacon_timeout && !found->sent) {
+			if (!info->rogue_super) {
+				info->rogue_super = found;
+				found->sent = 1;
+				proc_dlr_hw_access(info, DEV_CMD_PUT,
+					DEV_DLR_TX_LEARNING_UPDATE, 0, NULL);
+			}
+		}
+	}
+}  /* dlr_chk_beacon_timeout */
+
+static void dbg_supervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_super_info *next;
+	int i;
+
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (next->crc) {
+dbg_msg(" super %d %x=%02x:%02x:%02x\n",
+next->cnt,
+next->prec_addr[0],
+next->prec_addr[4],
+next->prec_addr[5],
+next->prec_addr[6]);
+		}
+	}
+}  /* dbg_supervisor */
+
+static void dlr_clr_supervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_super_info *next;
+	int i;
+
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (next->crc) {
+			if (next->last_cnt == next->cnt) {
+dbg_msg(" clr %d %u:%u %x=%02x:%02x:%02x\n",
+next->cnt, next->timeout[0], next->timeout[1],
+next->prec_addr[0],
+next->prec_addr[4],
+next->prec_addr[5],
+next->prec_addr[6]);
+				memset(next, 0,
+					sizeof(struct ksz_dlr_super_info));
+			} else
+				next->last_cnt = next->cnt;
+		}
+	}
+}  /* dlr_clr_supervisor */
+
+static int handleBeacon(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	u32 interval;
+	u32 timeout;
+	u16 vid;
+	int cmp = 0;
+	int update = false;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *beacon = frame->body;
+	u32 seqid = ntohl(beacon->hdr.seqid);
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+struct ksz_dlr_super_info active;
+struct ksz_dlr_super_info super;
+
+	if (info->state && (info->overrides & DLR_TEST_SEQ) &&
+	    (info->seqid_last[port] + 1 != seqid) && !info->seqid_cnt) {
+		info->seqid_cnt = 1000;
+		dbg_msg("bcn seq %d: %d %d\n", port,
+			info->seqid_last[port], seqid);
+	}
+	if (info->seqid_cnt > 0)
+		info->seqid_cnt--;
+	info->seqid_last[port] = seqid;
+
+	/* Announce node does not accept beacons. */
+	if (DLR_ANNOUNCE_NODE == info->node)
+		return update;
+
+#if 1
+/*
+ * THa  2015/11/03
+ * Hardware generates wrong ring state.
+ */
+	if (0 == beacon->data.beacon.ring_state)
+		beacon->data.beacon.ring_state = 2;
+#endif
+
+	/* Ignore own beacon if stopped. */
+	/*
+	 * Only active supervisor can receive its own beacons because of
+	 * self-address filtering.
+	 */
+	if (!memcmp(info->src_addr, vlan->h_source, ETH_ALEN)) {
+#if 0
+if (port == info->rx_port)
+dbg_msg(" * ");
+#endif
+#if (1 == DROP_BEACON_1)
+if (info->skip_beacon && info->drop_beacon && dbg_leak)
+dbg_msg(" ^ ");
+#endif
+#if 0
+		if (DLR_ACTIVE_SUPERVISOR != info->node && !info->start)
+			info->chk_hw = 1;
+#endif
+		if (!info->start)
+			return update;
+	}
+
+	/* Determine precedence level. */
+	super.prec_addr[0] = beacon->data.beacon.precedence;
+	memcpy(&super.prec_addr[1], vlan->h_source, ETH_ALEN);
+
+	/* Compare own address first if supervisor capable. */
+	if (info->node >= DLR_SUPERVISOR) {
+		active.prec_addr[0] = attrib->super_cfg.prec;
+		memcpy(&active.prec_addr[1], info->src_addr, ETH_ALEN);
+		cmp = memcmp(&super, &active, ETH_ALEN + 1);
+	}
+
+	if (cmp >= 0) {
+		active.prec_addr[0] = attrib->active_super_prec;
+		memcpy(&active.prec_addr[1], attrib->active_super_addr.addr,
+			ETH_ALEN);
+		cmp = memcmp(&super, &active, ETH_ALEN + 1);
+	}
+
+	/* Ignore lower precedence beacon. */
+	if (cmp < 0) {
+		/* Simulate beacon timeout as hardware cannot catch that. */
+		dlr_chk_beacon_timeout(info, port, beacon, &active, &super);
+
+#if 0
+/* Need to accept own beacons to check for timeout. */
+if (info->skip_beacon) {
+dbg_msg(" - ");
+		acceptBeacons(info);
+}
+#endif
+		return update;
+	} else if (cmp > 0) {
+dbg_msg("new %d %02x:%02x:%02x:%02x:%02x:%02x  %02x:%02x:%02x:%02x:%02x:%02x\n",
+cmp,
+attrib->active_super_addr.addr[0],
+attrib->active_super_addr.addr[1],
+attrib->active_super_addr.addr[2],
+attrib->active_super_addr.addr[3],
+attrib->active_super_addr.addr[4],
+attrib->active_super_addr.addr[5],
+vlan->h_source[0],
+vlan->h_source[1],
+vlan->h_source[2],
+vlan->h_source[3],
+vlan->h_source[4],
+vlan->h_source[5]);
+if (info->new_supervisor)
+dbg_msg("  prev super not proc\n");
+		update = true;
+		dlr_set_addr(&attrib->active_super_addr,
+			ntohl(beacon->hdr.ip_addr), vlan->h_source);
+		info->new_supervisor = 1;
+		info->p1_set = info->p2_set = 1;
+		info->p1_rcvd = info->p2_rcvd =
+		info->one_rcvd = info->both_rcvd = 0;
+		info->p1_timeout = info->p2_timeout =
+		info->one_timeout = info->both_timeout = 0;
+dbg_msg("  new sup 2: %d %d %d %d %d\n", beacon->data.beacon.precedence,
+attrib->active_super_prec, info->precedence, attrib->super_cfg.prec,
+beacon->data.beacon.ring_state);
+
+		/* Set in following code. */
+		info->LastBcnRcvPort = 0;
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+dbg_active = 1;
+			dbg_dlr(info, "stop being active");
+dbg_bcn = 6;
+		}
+		memset(&info->beacon_info[0].last, 0,
+			sizeof(struct ksz_dlr_beacon));
+		memset(&info->beacon_info[1].last, 0,
+			sizeof(struct ksz_dlr_beacon));
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+
+	/* Process accepted beacon. */
+
+	interval = ntohl(beacon->data.beacon.interval);
+	timeout = ntohl(beacon->data.beacon.timeout);
+
+	/* Used to determine beacon timeout in software simulation. */
+	beacon_info->rcv_once = 1;
+	beacon_info->timeout_start = 0;
+
+#ifdef DROP_BEACON_0
+#ifdef CONFIG_HAVE_DLR_HW
+	if (!update && info->skip_beacon /*&& info->node < DLR_ACTIVE_SUPERVISOR*/) {
+if (dbg_leak > 0) {
+dbg_msg(" ??: %d %d %d %x %02x:%02x:%02x:%02x:%02x:%02x %02x:%02x:%02x:%02x:%02x:%02x\n", port, info->skip_beacon, info->node, seqid,
+vlan->h_dest[0],
+vlan->h_dest[1],
+vlan->h_dest[2],
+vlan->h_dest[3],
+vlan->h_dest[4],
+vlan->h_dest[5],
+vlan->h_source[0],
+vlan->h_source[1],
+vlan->h_source[2],
+vlan->h_source[3],
+vlan->h_source[4],
+vlan->h_source[5]
+);
+--dbg_leak;
+}
+}
+#endif
+	if (!update && info->skip_beacon)
+{
+if (dbg_active)
+dbg_msg("  r3\n");
+		return update;
+}
+	else if (beacon_info->timeout && !info->drop_beacon) {
+		beacon_info->timeout += interval;
+		if (beacon_info->timeout > timeout) {
+if (RING_NORMAL_STATE != beacon->data.beacon.ring_state ||
+    RING_NORMAL_STATE != info->ring_state)
+dbg_msg(" drop: %d %d; %d %d\n", beacon->data.beacon.ring_state, info->ring_state,
+info->beacon_info[0].timeout, info->beacon_info[1].timeout);
+			info->beacon_info[0].timeout =
+			info->beacon_info[1].timeout = 0;
+			dropBeacons(info);
+			return update;
+		}
+	}
+	if (info->skip_beacon) {
+dbg_msg(" update: %d\n", update);
+		acceptBeacons(info);
+	}
+#endif
+
+	/* Try to process as few beacons as possible. */
+	if (memcmp(&beacon_info->last, &beacon->data.beacon,
+	    sizeof(struct ksz_dlr_beacon))) {
+		memcpy(&beacon_info->last, &beacon->data.beacon,
+			sizeof(struct ksz_dlr_beacon));
+		info->seqid_accept[port] = seqid;
+	} else {
+#if 0
+if (dbg_active || dbg_bcn)
+dbg_msg("  r4: %d %02x:%02x:%02x\n", port,
+vlan->h_source[3],
+vlan->h_source[4],
+vlan->h_source[5]);
+#endif
+if (dbg_bcn)
+--dbg_bcn;
+if (update)
+dbg_msg("  ??? update\n");
+		if (beacon_info->timeout)
+			return update;
+	}
+
+	/* Not running as supervisor. */
+	if ((DLR_ACTIVE_SUPERVISOR != info->node &&
+	    info->ring_state != beacon->data.beacon.ring_state) ||
+	    update) {
+int ring_state = info->ring_state;
+		info->ring_state = beacon->data.beacon.ring_state;
+
+		/* Set in following code. */
+		info->LastBcnRcvPort = 0;
+		info->p1_rcvd = info->p2_rcvd =
+		info->one_rcvd = info->both_rcvd = 0;
+		beacon_info->timeout = 0;
+
+		if (RING_FAULT_STATE == info->ring_state) {
+			dbg_dlr(info, "ring fault");
+		}
+dbg_msg(" R: %d %d %d %d\n", info->node, info->ring_state, ring_state, update);
+	}
+	if (1 == port) {
+		info->p2_rcvd = 1;
+		info->p2_timeout = 0;
+		info->p2_down = 0;
+	} else {
+		info->p1_rcvd = 1;
+		info->p1_timeout = 0;
+		info->p1_down = 0;
+	}
+	if (!info->p1_timeout && !info->p2_timeout)
+		info->one_timeout = 0;
+	info->both_timeout = 0;
+
+	/* Change down state as beacon can be received before link status. */
+	if (!info->p1_down && !info->p2_down)
+		info->one_down = 0;
+	info->both_down = 0;
+
+	if (DLR_IDLE_STATE == info->state) {
+#if 0
+		int last = info->LastBcnRcvPort;
+#endif
+
+		if (1 == port) {
+			info->LastBcnRcvPort = 2;
+		} else {
+			info->LastBcnRcvPort = 1;
+		}
+#if 0
+		if (last != info->LastBcnRcvPort) {
+dbg_msg("L: %d %d.\n", port, info->LastBcnRcvPort);
+if (dbg_bcn > 2)
+++dbg_bcn;
+}
+#endif
+	} else if (info->state != DLR_NORMAL_STATE &&
+		   info->state != DLR_ACTIVE_NORMAL_STATE) {
+#if 0
+		int last = info->LastBcnRcvPort;
+#endif
+
+		if (1 == port) {
+			if (info->LastBcnRcvPort & 1) {
+				info->LastBcnRcvPort = 3;
+			} else
+				info->LastBcnRcvPort = 2;
+		} else {
+			if (info->LastBcnRcvPort & 2) {
+				info->LastBcnRcvPort = 3;
+			} else
+				info->LastBcnRcvPort = 1;
+		}
+#if 0
+		if (last != info->LastBcnRcvPort && 3 == info->LastBcnRcvPort) {
+dbg_msg("L: %d 3 %d.\n", port, info->ring_state);
+if (dbg_bcn > 2)
+++dbg_bcn;
+}
+#endif
+	}
+	if (info->p1_rcvd && info->p2_rcvd) {
+
+		/* Running as supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR == info->node ||
+		    RING_NORMAL_STATE == info->ring_state) {
+			if (!info->both_rcvd)
+				update = true;
+			info->both_rcvd = 1;
+			info->one_rcvd = 0;
+		}
+
+		/* Start the beacon drop process. */
+		if (!beacon_info->timeout &&
+		    !info->skip_beacon && !info->drop_beacon &&
+		    RING_NORMAL_STATE == beacon->data.beacon.ring_state &&
+		    RING_NORMAL_STATE == info->ring_state &&
+		    (DLR_NORMAL_STATE == info->state ||
+		    DLR_ACTIVE_NORMAL_STATE == info->state))
+{
+			beacon_info->timeout = 1;
+dbg_msg("  ready to drop: %d\n", beacon->data.beacon.ring_state);
+}
+	} else {
+		if (!info->one_rcvd)
+			update = true;
+		info->one_rcvd = 1;
+		beacon_info->timeout = 0;
+	}
+	if (attrib->active_super_prec != beacon->data.beacon.precedence) {
+		attrib->active_super_prec = beacon->data.beacon.precedence;
+	}
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		vid = ntohs(vlan->h_vlan_TCI) & ((1 << VLAN_PRIO_SHIFT) - 1);
+		if (info->vid != vid) {
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_SETUP_VID, info->vid, NULL);
+			info->vid = vid;
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_SETUP_VID,
+				0x80000000 | info->vid, NULL);
+
+			/* Use current VID . */
+			if (attrib->super_cfg.enable)
+				attrib->super_cfg.vid = vid;
+			info->frame.vlan.h_vlan_TCI =
+				htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+			memcpy(info->signon_frame, &info->frame,
+				sizeof(struct vlan_ethhdr));
+		}
+	}
+	if (info->beacon_interval != interval) {
+dbg_msg("%s %u %u\n", __func__, info->beacon_interval, interval);
+		info->beacon_interval = interval;
+
+		/* Use current beacon interval. */
+		if (attrib->super_cfg.enable)
+			attrib->super_cfg.beacon_interval = interval;
+	}
+	if (info->beacon_timeout != timeout) {
+		info->beacon_timeout = timeout;
+
+		/* Use current beacon timeout. */
+		if (attrib->super_cfg.enable)
+			attrib->super_cfg.beacon_timeout = timeout;
+		info->p1_set = info->p2_set = 1;
+	}
+#if 1
+if (update || dbg_bcn > 0)
+#endif
+dbg_msg("b: %d=%d:%d %x %d; p=%d:%d r=%d:%d; %u %u %lx %d %lx %d\n", port,
+beacon->data.beacon.ring_state, info->ring_state, ntohs(vlan->h_vlan_TCI), beacon_info->timeout,
+info->p1_rcvd, info->p2_rcvd, info->one_rcvd, info->both_rcvd,
+beacon_info->interval, bcn_cnt, seqid, info->new_supervisor,
+jiffies, dbg_bcn);
+if (dbg_bcn)
+--dbg_bcn;
+	beacon_info->interval = 0;
+	if (info->p1_rcvd && info->p1_set) {
+		info->p1_set = 0;
+		setupBeaconTimeout(info, 0);
+	}
+	if (info->p2_rcvd && info->p2_set) {
+		info->p2_set = 0;
+		setupBeaconTimeout(info, 1);
+	}
+	return update;
+}  /* handleBeacon */
+
+static int handleSignOn(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	int i;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *signon = frame->body;
+	u16 num = ntohs(signon->data.signon.num);
+	struct ksz_dlr_node *node = signon->data.signon.node;
+
+	/* Ignore if not NORMAL_STATE. */
+	if (info->state != DLR_ACTIVE_NORMAL_STATE &&
+	    info->state != DLR_NORMAL_STATE &&
+	    !(info->overrides & DLR_TEST))
+{
+dbg_dlr(info, " ?signon");
+dbg_bcn = 10;
+		return false;
+}
+dbg_bcn = 2;
+	if (DLR_ACTIVE_SUPERVISOR == info->node &&
+	    !memcmp(node->addr, info->src_addr, ETH_ALEN)) {
+		struct ksz_dlr_node_info *cur;
+		u32 seqid = ntohl(signon->hdr.seqid);
+
+		if (seqid != info->seqid_signon)
+dbg_msg("!seqid: %08x %08x\n", seqid, info->seqid_signon);
+dbg_msg("signon: %d; %d; %02x\n", num, port, vlan->h_dest[0]);
+dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x\n",
+node->addr[0],
+node->addr[1],
+node->addr[2],
+node->addr[3],
+node->addr[4],
+node->addr[5]);
+		for (i = 0; i < num; i++, node++) {
+			if (i && !memcmp(node->addr, info->src_addr, ETH_ALEN))
+				continue;
+dbg_msg("%d %02x:%02x:%02x:%02x:%02x:%02x\n", i,
+node->addr[0],
+node->addr[1],
+node->addr[2],
+node->addr[3],
+node->addr[4],
+node->addr[5]);
+			cur = &info->nodes[info->attrib.participants_cnt];
+			memcpy(&cur->signon, node,
+				sizeof(struct ksz_dlr_node));
+			cur->p1_down = cur->p2_down =
+			cur->p1_lost = cur->p2_lost = 0;
+			info->attrib.participants_cnt++;
+		}
+
+		/* Addressed to the supervisor instead of multicast address. */
+		if (!memcmp(info->src_addr, vlan->h_dest, ETH_ALEN)) {
+			if (info->overrides & DLR_TEST)
+dbg_msg("send next\n");
+			memcpy(info->signon_addr, vlan->h_source,
+				ETH_ALEN);
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_TX_SIGNON, 0, NULL);
+		} else
+			disableSignOnTimer(info);
+	} else {
+		int len;
+		struct ksz_dlr_tx_frame *tx = (struct ksz_dlr_tx_frame *)
+			info->signon_frame;
+
+		if ((info->overrides & DLR_TEST) && info->ignore_req) {
+			++info->req_cnt[0];
+			if (info->req_cnt[0] <= info->ignore_req)
+				printk(KERN_INFO
+					"ignore SignOn: %d\n",
+					info->req_cnt[0]);
+			if (info->req_cnt[0] <= info->ignore_req)
+				return false;
+			info->req_cnt[0] = 0;
+		}
+		len = sizeof(struct ksz_dlr_hdr) +
+			sizeof(struct ksz_dlr_signon) +
+			(num - 1) * sizeof(struct ksz_dlr_node);
+		memcpy(info->signon_frame, vlan, ETH_ALEN * 2);
+		memcpy(&tx->body, signon, len);
+		len += sizeof(struct vlan_ethhdr);
+		info->rx_port = port;
+		info->tx_port = (port + 1) & 1;
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_SIGNON, len,
+			NULL);
+		if (info->active_port != (1 << info->ports[info->rx_port]))
+			setupDir(info, port);
+	}
+	return false;
+}  /* handleSignOn */
+
+static int handleLocateFault(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+{
+dbg_msg("%s\n", __func__);
+		return false;
+}
+	if (info->node != DLR_ACTIVE_SUPERVISOR &&
+	    !memcmp(vlan->h_source, info->attrib.active_super_addr.addr,
+	    ETH_ALEN)) {
+		if (linkDown)
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_TX_STATUS, 0, NULL);
+		else if (!info->neigh_chk) {
+			if (info->neigh_chk_timer_info.max) {
+dbg_msg(" !! %s %d\n", __func__, info->neigh_chk_timer_info.max);
+				return false;
+			}
+			info->neigh_chk = 1;
+			ksz_start_timer(&info->neigh_chk_timer_info,
+				info->neigh_chk_timer_info.period);
+			info->neigh_chk_timer_info.max = 3;
+			info->p1_lost = info->p2_lost = 0;
+			info->port_chk[0] = info->port_chk[1] = 1;
+			proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_REQ,
+				0, NULL);
+			proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_REQ,
+				1, NULL);
+		}
+	} else {
+dbg_msg("%s ignored\n", __func__);
+	}
+	return false;
+}  /* handleLocateFault */
+
+static int handleAnnounce(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	u32 seqid;
+	u16 vid;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *announce = frame->body;
+	int new = 0;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if (RING_NORMAL_STATE == announce->data.announce.ring_state &&
+	    DLR_NORMAL_STATE == info->state &&
+	    (!info->active_port || info->rx_port != port)) {
+dbg_msg("%s %d %x dir %x %lx\n", __func__, port, info->active_port,
+ntohl(announce->hdr.seqid), jiffies);
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_SETUP_DIR, port,
+			NULL);
+	}
+	if (announce->data.announce.ring_state != info->ring_state) {
+dbg_msg(" ann: %d %d %d\n", port, info->ring_state, announce->data.announce.ring_state);
+} else if (RING_NORMAL_STATE == info->ring_state && info->state != DLR_NORMAL_STATE)
+dbg_msg(" ann ring normal: %d\n", port);
+#if 1
+	/* Rely on Announce frame to determine ring state. */
+	if (info->skip_beacon) {
+		if (!memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		    ETH_ALEN) && info->ring_state !=
+		    announce->data.announce.ring_state) {
+			info->ring_state = announce->data.announce.ring_state;
+			acceptBeacons(info);
+dbg_bcn += 4;
+			return true;
+		}
+	}
+#endif
+
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+dbg_msg("%s ignored %d %d %d\n", __func__, port,
+announce->data.announce.ring_state, info->skip_beacon);
+		return false;
+	}
+	if (DLR_ANNOUNCE_NODE != info->node)
+		return false;
+
+info->rx_port = port;
+info->tx_port = (port + 1) & 1;
+	seqid = ntohl(announce->hdr.seqid);
+
+	if (memcmp(vlan->h_source, attrib->active_super_addr.addr, ETH_ALEN) ||
+	    info->ann_timeout) {
+		dlr_set_addr(&attrib->active_super_addr,
+			ntohl(announce->hdr.ip_addr), vlan->h_source);
+		info->new_supervisor = 1;
+		new = 1;
+	} else {
+		/* Check valid sequence number. */
+		if (!seq_ahead(info->seqid_announce, seqid))
+			return false;
+	}
+	info->seqid_announce = seqid;
+	if (announce->data.announce.ring_state != info->ring_state) {
+		info->ring_state = announce->data.announce.ring_state;
+		info->ann_rcvd = 1;
+		new = 1;
+	}
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		vid = ntohs(vlan->h_vlan_TCI) & ((1 << VLAN_PRIO_SHIFT) - 1);
+		if (vid != info->vid) {
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_SETUP_VID, info->vid, NULL);
+			info->vid = vid;
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_SETUP_VID,
+				0x80000000 | info->vid, NULL);
+			info->frame.vlan.h_vlan_TCI =
+				htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+			memcpy(info->signon_frame, &info->frame,
+				sizeof(struct vlan_ethhdr) +
+				sizeof(struct ksz_dlr_hdr));
+			new = 1;
+		}
+	}
+	info->ann_timeout = 0;
+
+	ksz_stop_timer(&info->announce_timeout_timer_info);
+	ksz_start_timer(&info->announce_timeout_timer_info,
+		info->announce_timeout_timer_info.period);
+	info->announce_timeout_timer_info.max = 1;
+	return new;
+}  /* handleAnnounce */
+
+static int handleNeighChkReq(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *req = frame->body;
+
+#if 0
+dbg_msg("req: %08x %d %d\n",
+ntohl(req->hdr.seqid), req->hdr.src_port, port);
+#endif
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    info->state != DLR_ACTIVE_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO "req: p=%d s=%08x %d; %lx\n",
+			port, ntohl(req->hdr.seqid), req->hdr.src_port,
+			jiffies);
+	}
+	if ((info->overrides & DLR_TEST) && info->ignore_req) {
+		++info->req_cnt[port];
+		if (info->req_cnt[port] <= info->ignore_req)
+			return false;
+		info->req_cnt[port] = 0;
+	}
+	info->port_rcv[port] = req->hdr.src_port;
+	info->seqid_rcv[port] = ntohl(req->hdr.seqid);
+	proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_RESP, port, NULL);
+	return false;
+}  /* handleNeighChkReq */
+
+static int handleNeighChkResp(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *resp = frame->body;
+	int src_port = port ? DLR_PORT_2 : DLR_PORT_1;
+
+#if 0
+dbg_msg("resp: %08x %08x; %d %d\n",
+ntohl(resp->hdr.seqid), info->seqid_chk[port],
+resp->data.neigh_chk_resp.src_port, port);
+#endif
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    info->state != DLR_ACTIVE_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO "resp: p=%d s=%08x %d; %08x - %08x; %lx\n",
+			port, ntohl(resp->hdr.seqid),
+			resp->data.neigh_chk_resp.src_port,
+			info->seqid_first[port],
+			info->seqid_chk[port], jiffies);
+	}
+	if (src_port == resp->data.neigh_chk_resp.src_port) {
+		u32 seqid = ntohl(resp->hdr.seqid);
+
+		if (seq_in(info->seqid_first[port], info->seqid_chk[port],
+		    seqid)) {
+			if (port)
+				info->p2_down = info->p2_lost = 0;
+			else
+				info->p1_down = info->p1_lost = 0;
+			info->port_chk[port] = 0;
+			if (!info->port_chk[(port + 1) & 1]) {
+				ksz_stop_timer(&info->neigh_chk_timer_info);
+				info->neigh_chk = 0;
+			}
+		}
+	}
+	return false;
+}  /* handleNeighChkResp */
+
+static int handleFlushTables(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *flush = frame->body;
+
+	/* Ignore if not in NORMAL_STATE or FAULT_STATE. */
+	if (info->state <= DLR_IDLE_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_FLUSH, 0, NULL);
+	if (flush->data.flush.learning_update_enable)
+		proc_dlr_hw_access(info, DEV_CMD_PUT,
+			DEV_DLR_TX_LEARNING_UPDATE, 0, NULL);
+
+	return false;
+}  /* handleFlushTables */
+
+static int handleLinkStatus(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *status = frame->body;
+
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO
+			"link: %02x:%02x:%02x:%02x:%02x:%02x %d:%d %d\n",
+			vlan->h_source[0], vlan->h_source[1], vlan->h_source[2],
+			vlan->h_source[3], vlan->h_source[4], vlan->h_source[5],
+			status->data.status.port1_active,
+			status->data.status.port2_active,
+			status->data.status.neighbor);
+	}
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		int i;
+		struct ksz_dlr_node_info *node;
+		struct ksz_dlr_node_info *prev;
+		struct ksz_dlr_node_info *next;
+
+dbg_msg("link: %02x:%02x:%02x:%02x:%02x:%02x %d:%d %d\n",
+vlan->h_source[0],
+vlan->h_source[1],
+vlan->h_source[2],
+vlan->h_source[3],
+vlan->h_source[4],
+vlan->h_source[5],
+status->data.status.port1_active,
+status->data.status.port2_active,
+status->data.status.neighbor);
+		prev = &info->nodes[0];
+		for (i = 1; i < info->attrib.participants_cnt; i++) {
+			node = &info->nodes[i];
+			if (!memcmp(node->signon.addr, vlan->h_source,
+			    ETH_ALEN)) {
+				if (i + 1 < info->attrib.participants_cnt)
+					next = node + 1;
+				else
+					next = &info->nodes[0];
+				if (status->data.status.neighbor) {
+					if (status->data.status.port1_active) {
+						node->p1_down = 0;
+						node->p1_lost = 0;
+					} else {
+						node->p1_lost = 1;
+						prev->p2_lost = 1;
+					}
+					if (status->data.status.port2_active) {
+						node->p2_down = 0;
+						node->p2_lost = 0;
+					} else {
+						node->p2_lost = 1;
+						next->p1_lost = 1;
+					}
+				} else {
+					if (status->data.status.port1_active) {
+						node->p1_down = 0;
+					} else {
+						node->p1_down = 1;
+					}
+					if (status->data.status.port2_active) {
+						node->p2_down = 0;
+					} else {
+						node->p2_down = 1;
+					}
+				}
+			}
+			prev = node;
+		}
+		if (!status->data.status.port1_active ||
+		    !status->data.status.port2_active) {
+			dlr_set_addr(&info->attrib.last_active[port],
+				ntohl(status->hdr.ip_addr), vlan->h_source);
+			if (1 == port)
+				info->p2_lost = 1;
+			else
+				info->p1_lost = 1;
+			return true;
+		}
+	} else {
+dbg_msg("%s ignored\n", __func__);
+	}
+	return false;
+}  /* handleLinkStatus */
+
+static int handleLearningUpdate(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+#if 0
+	struct ksz_dlr_frame *update = frame->body;
+#endif
+
+dbg_msg("%s %d %d\n", __func__, info->node, port);
+	if (!memcmp(info->src_addr, vlan->h_dest, ETH_ALEN)) {
+dbg_msg(" to self!\n");
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_CHK_HW, 0, NULL);
+	}
+	if (DLR_SUPERVISOR <= info->node) {
+		if (!memcmp(info->src_addr, vlan->h_source, ETH_ALEN)) {
+dbg_msg("%s %d\n", __func__, info->node);
+		}
+	}
+	return false;
+}  /* handleLearningUpdate */
+
+static int handleBeaconTimeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+
+	if (DLR_NORMAL_STATE == info->state ||
+	    DLR_ACTIVE_NORMAL_STATE == info->state ||
+	    info->state_change) {
+		info->state_change = 0;
+		if (1 == port)
+			info->LastBcnRcvPort = 1;
+		else
+			info->LastBcnRcvPort = 2;
+#if 0
+dbg_msg("L1: %d %d\n", port, info->LastBcnRcvPort);
+#endif
+	} else if (info->state > DLR_IDLE_STATE) {
+		int last = info->LastBcnRcvPort;
+
+		if (1 == port) {
+			if (2 == info->LastBcnRcvPort) {
+				info->LastBcnRcvPort = 0;
+			}
+			if (DLR_SUPERVISOR <= info->node) {
+				info->LastBcnRcvPort &= ~2;
+				info->LastBcnRcvPort &= ~8;
+			}
+		} else {
+			if (1 == info->LastBcnRcvPort) {
+				info->LastBcnRcvPort = 0;
+			}
+			if (DLR_SUPERVISOR <= info->node) {
+				info->LastBcnRcvPort &= ~1;
+				info->LastBcnRcvPort &= ~4;
+			}
+		}
+#if 0
+dbg_msg("L2: %d %d\n", port, info->LastBcnRcvPort);
+#endif
+		if (last != info->LastBcnRcvPort &&
+		    0 == info->LastBcnRcvPort) {
+			if (DLR_SUPERVISOR == info->node)
+dbg_msg("become active\n");
+			else if (DLR_ACTIVE_SUPERVISOR != info->node)
+dbg_msg("become idle\n");
+		}
+	}
+	if (1 == port) {
+		info->p2_rcvd = 0;
+		info->p2_timeout = 1;
+	} else {
+		info->p1_rcvd = 0;
+		info->p1_timeout = 1;
+	}
+	memset(&beacon_info->last, 0, sizeof(struct ksz_dlr_beacon));
+	beacon_info->timeout = 0;
+	if (info->p1_rcvd || info->p2_rcvd)
+		info->one_rcvd = 1;
+	else
+		info->one_rcvd = 0;
+	info->both_rcvd = 0;
+	if ((info->p1_timeout && info->p2_timeout) /*|| !info->one_rcvd*/) {
+		info->both_timeout = 1;
+		info->one_timeout = 0;
+		info->chk_hw = 1;
+#if 0
+if (DLR_ACTIVE_SUPERVISOR == info->node) {
+dbg_msg(" chk hw\n");
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_CHK_HW, 0, NULL);
+}
+#endif
+	} else
+		info->one_timeout = 1;
+dbg_msg("timeout: r=%d:%d; t=%d:%d; R=%d:%d; T=%d:%d\n",
+info->p1_rcvd, info->p2_rcvd, info->p1_timeout, info->p2_timeout,
+info->one_rcvd, info->both_rcvd, info->one_timeout, info->both_timeout);
+	if (info->both_timeout)
+		info->p1_timeout = info->p2_timeout = 0;
+
+	/* Automatic backup supervisor will start. */
+	if (info->both_timeout && DLR_SUPERVISOR == info->node)
+		info->start = 1;
+	return true;
+}  /* handleBeaconTimeout */
+
+static int handleLinkChange(struct ksz_dlr_info *info, int link1, int link2)
+{
+	int change[2];
+	int down[2];
+	int update = false;
+	int going_up = false;
+
+	down[0] = down[1] = 0;
+	change[0] = !link1 ^ info->p1_down;
+	change[1] = !link2 ^ info->p2_down;
+	if (link1)
+		info->p1_down = 0;
+	else
+		info->p1_down = 1;
+	if (link2)
+		info->p2_down = 0;
+	else
+		info->p2_down = 1;
+	if ((!change[0] || link1) && (!change[1] || link2))
+		going_up = true;
+	if (info->p1_down && info->p2_down) {
+		if (!info->both_down)
+			update = true;
+		info->both_down = 1;
+		info->one_down = 0;
+		info->one_rcvd = 0;
+	} else if (info->p1_down || info->p2_down) {
+		if (!info->one_down)
+			update = true;
+		info->both_down = 0;
+		info->one_down = 1;
+		if (info->both_rcvd)
+			info->one_rcvd = 1;
+	} else {
+		info->one_down = 0;
+		info->both_down = 0;
+	}
+	if (info->p1_down) {
+		info->p1_rcvd = 0;
+		info->both_rcvd = 0;
+	}
+	if (info->p2_down) {
+		info->p2_rcvd = 0;
+		info->both_rcvd = 0;
+	}
+	down[0] = info->p1_down;
+	down[1] = info->p2_down;
+	if (info->node != DLR_ANNOUNCE_NODE) {
+		info->beacon_info[0].timer = !down[0];
+		info->beacon_info[1].timer = !down[1];
+	}
+	if (going_up)
+		update = false;
+	if ((info->p1_down || info->p2_down) && !going_up) {
+		int p;
+
+dbg_msg("Lo: %d:%d %d\n", down[0], down[1], info->LastBcnRcvPort);
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+			for (p = 0; p < 2; p++) {
+				if (down[p]) {
+					dlr_set_addr(&attrib->last_active[p],
+						info->ip_addr, info->src_addr);
+				}
+			}
+		}
+
+		/* Reset last beacon in case timeout is not processed. */
+		for (p = 0; p < 2; p++) {
+			if (down[p]) {
+				memset(&info->beacon_info[p].last, 0,
+					sizeof(struct ksz_dlr_beacon));
+				info->beacon_info[p].timeout = 0;
+			}
+		}
+	}
+	return update;
+}  /* handleLinkChange */
+
+static void LocateFault(struct ksz_dlr_info *info)
+{
+	if (info->node != DLR_ACTIVE_SUPERVISOR)
+		return;
+#if 1
+	dlr_tx_locate_fault(info);
+#else
+	proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_LOCATE, 0, NULL);
+#endif
+}  /* LocateFault */
+
+static void NeighborCheck(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if (!info->p1_down || !info->p2_down) {
+		if (info->neigh_chk_timer_info.max) {
+dbg_msg(" !! %s %d %d\n", __func__, info->neigh_chk_timer_info.max,
+info->neigh_chk);
+			return;
+		}
+		info->neigh_chk = 1;
+		ksz_start_timer(&info->neigh_chk_timer_info,
+			info->neigh_chk_timer_info.period);
+		info->neigh_chk_timer_info.max = 3;
+	}
+	if (info->p1_down) {
+		dlr_set_addr(&attrib->last_active[0],
+			info->ip_addr, info->src_addr);
+	} else {
+		info->p1_lost = 0;
+		info->port_chk[0] = 1;
+#if 1
+		dlr_tx_chk_req(info, 0);
+#else
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_REQ, 0, NULL);
+#endif
+	}
+	if (info->p2_down) {
+		dlr_set_addr(&attrib->last_active[1],
+			info->ip_addr, info->src_addr);
+	} else {
+		info->p2_lost = 0;
+		info->port_chk[1] = 1;
+#if 1
+		dlr_tx_chk_req(info, 1);
+#else
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_REQ, 1, NULL);
+#endif
+	}
+}  /* NeighborCheck */
+
+static void startSignOn(struct ksz_dlr_info *info, int now)
+{
+	if (info->signon_timer_info.max) {
+dbg_msg(" !!! %s %d\n", __func__, info->signon_timer_info.max);
+		return;
+	}
+	info->attrib.participants_cnt = 0;
+	memcpy(info->signon_addr, MAC_ADDR_SIGNON, ETH_ALEN);
+	ksz_start_timer(&info->signon_timer_info,
+		info->signon_timer_info.period);
+	if (!info->signon_delay || !info->ann_delay) {
+		info->signon_delay = 0;
+		if (now)
+			dlr_tx_signon(info, 0);
+		else
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_TX_SIGNON, 0, NULL);
+if (inside_cmd == DEV_DLR_TX_SIGNON)
+dbg_msg(" %s\n", __func__);
+	}
+	else
+dbg_msg("%s %d %d %lx\n", __func__, info->signon_delay, info->ann_delay,
+jiffies);
+}  /* startSignOn */
+
+static void sendLinkStatus(struct ksz_dlr_info *info)
+{
+	/* Supervisor is known. */
+	if (info->attrib.active_super_addr.addr[0] ||
+	    info->attrib.active_super_addr.addr[1])
+		dlr_tx_status(info);
+}  /* sendLinkStatus */
+
+static void dlr_clear(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_beacon_info *beacon_info;
+	int p;
+
+	info->p1_rcvd = info->p2_rcvd =
+	info->one_rcvd = info->both_rcvd =
+	info->p1_timeout = info->p2_timeout =
+	info->one_timeout = info->both_timeout = 0;
+	for (p = 0; p < 2; p++ ) {
+		beacon_info = &info->beacon_info[p];
+		beacon_info->timer =
+		beacon_info->rcv_once =
+		beacon_info->timeout_start =
+		beacon_info->timeout_stop = 0;
+		beacon_info->timeout = 0;
+		memset(&beacon_info->last, 0, sizeof(struct ksz_dlr_beacon));
+	}
+	memset(info->supers, 0, sizeof(struct ksz_dlr_super_info) *
+		DLR_SUPERVISOR_NUM);
+}  /* dlr_clear */
+
+struct dlr_state {
+	int change;
+	int delay_ann;
+	int new_state;
+};
+
+static void dlr_idle_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	dbg_dlr(info, "idle");
+	dlr_print(info, "idle");
+	oneBeaconRcvd = twoBeaconsRcvd = 0;
+	oneBeaconTimeout = twoBeaconsTimeout = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_topology = DLR_TOPOLOGY_LINEAR;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_NO_SUPERVISOR;
+	memset(&attrib->active_super_addr, 0,
+		sizeof(struct ksz_dlr_active_node));
+	attrib->active_super_prec = 0;
+
+	if (info->skip_beacon)
+		acceptBeacons_(info);
+	flushMacTable(info);
+	dlr_set_state(info);
+}  /* dlr_idle_init */
+
+static void dlr_idle_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsRcvd && !faultState) {
+		state->new_state = DLR_NORMAL_STATE;
+	}
+	if (oneBeaconRcvd || (twoBeaconsRcvd && faultState)) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+}  /* dlr_idle_next */
+
+static void dlr_ann_idle_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	dbg_dlr(info, "idle");
+	dlr_print(info, "idle");
+	announceRcvd = 0;
+	announcedState = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_LINEAR;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_NO_SUPERVISOR;
+	memset(&attrib->active_super_addr, 0,
+		sizeof(struct ksz_dlr_active_node));
+	attrib->active_super_prec = 0;
+}  /* dlr_ann_idle_init */
+
+static void dlr_ann_idle_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+}  /* dlr_ann_idle_next */
+
+static void dlr_fault_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+dbg_msg(" %d %d; %d %d;  ", info->seqid_accept[0], info->seqid_last[0],
+info->seqid_accept[1], info->seqid_last[1]);
+	dbg_dlr(info, "fault");
+	dlr_print(info, "fault");
+	newSupervisor = 0;
+	announcedState = RING_FAULT_STATE;
+dbg_msg("timeout %d %d; %d\n", info->beacon_info[0].timeout, info->beacon_info[1].timeout, info->drop_beacon);
+if (info->drop_beacon) {
+dbg_msg("  !!!\n");
+	info->drop_beacon = false;
+}
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_RING_FAULT;
+	if (DLR_BEACON_NODE >= info->node)
+		attrib->super_status = DLR_STAT_RING_NODE;
+	else
+		attrib->super_status = DLR_STAT_BACKUP_SUPERVISOR;
+
+	flushMacTable(info);
+	dlr_set_state(info);
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+#if 0
+	if (info->chk_hw) {
+		info->chk_hw = 0;
+		dlr_chk_supervisor(info);
+	}
+#endif
+}  /* dlr_fault_init */
+
+static void dlr_fault_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsRcvd && !faultState) {
+		newSupervisor = 0;
+		disableNeighChkTimers(info);
+		state->new_state = DLR_NORMAL_STATE;
+	}
+	if (twoBeaconsTimeout) {
+#if 0
+dbg_msg("to active\n");
+#endif
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+		else
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (oneBeaconTimeout && !oneBeaconRcvd) {
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkDown) {
+dbg_msg(" s linkDown\n");
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkLoss) {
+		linkLoss = 0;
+dbg_msg(" s linkLoss\n");
+		sendLinkStatus(info);
+	}
+	if (newSupervisor) {
+dbg_msg("new super\n");
+		newSupervisor = 0;
+		flushMacTable(info);
+	}
+
+	/* Apply only to supervisor. */
+	if (newValue) {
+		updateValues(info);
+	}
+}  /* dlr_fault_next */
+
+static void dlr_ann_fault_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	dbg_dlr(info, "fault");
+	dlr_print(info, "fault");
+	announceRcvd = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_RING_FAULT;
+	attrib->super_status = DLR_STAT_RING_NODE;
+
+	flushMacTable(info);
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+}  /* dlr_ann_fault_init */
+
+static void dlr_ann_fault_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceTimeout) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkDown) {
+		disableAnnounceTimeout(info);
+		disableNeighChkTimers(info);
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+	if (newSupervisor) {
+		newSupervisor = 0;
+		disableNeighChkTimers(info);
+	}
+}  /* dlr_ann_fault_next */
+
+static void dlr_normal_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+dbg_msg(" %d %d; %d %d;  ", info->seqid_accept[0], info->seqid_last[0],
+info->seqid_accept[1], info->seqid_last[1]);
+	dbg_dlr(info, "normal");
+	dlr_print(info, "normal");
+	newSupervisor = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_NORMAL;
+	if (DLR_BEACON_NODE >= info->node)
+		attrib->super_status = DLR_STAT_RING_NODE;
+	else
+		attrib->super_status = DLR_STAT_BACKUP_SUPERVISOR;
+
+	flushMacTable(info);
+	dlr_set_state(info);
+}  /* dlr_normal_init */
+
+static void dlr_normal_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsTimeout) {
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+		else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (newSupervisor || faultState || oneBeaconTimeout) {
+
+		/* Change from normal to fault. */
+		if (faultState)
+			info->state_change = 1;
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (linkDown) {
+dbg_msg(" s linkDown\n");
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkLoss) {
+dbg_msg(" s linkLoss 2\n");
+		state->new_state = DLR_FAULT_STATE;
+	}
+
+	/* Apply only to supervisor. */
+	if (newValue) {
+		updateValues(info);
+	}
+	if (state->new_state) {
+		setupDir(info, -1);
+		if (info->skip_beacon)
+			acceptBeacons_(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+	if (DLR_FAULT_STATE == state->new_state) {
+		struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+		attrib->fault_cnt++;
+	}
+}  /* dlr_normal_next */
+
+static void dlr_ann_normal_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	dbg_dlr(info, "normal");
+	dlr_print(info, "normal");
+	announceRcvd = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_RING_NODE;
+
+	flushMacTable(info);
+	disableNeighChkTimers(info);
+}  /* dlr_ann_normal_init */
+
+static void dlr_ann_normal_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceTimeout) {
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+	if (linkLoss) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (state->new_state)
+		setupDir(info, -1);
+	if (DLR_FAULT_STATE == state->new_state) {
+		struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+		attrib->fault_cnt++;
+	}
+}  /* dlr_ann_normal_next */
+
+static void dlr_active_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	dbg_dlr(info, "active");
+	dlr_clear(info);
+	info->beacon_info[0].timer =
+	info->beacon_info[1].timer = 1;
+
+	/* Reset incoming and outgoing ports. */
+	info->tx_port = info->port;
+	info->rx_port = (info->tx_port + 1) & 1;
+	info->LastBcnRcvPort = 0;
+	info->node = DLR_ACTIVE_SUPERVISOR;
+	info->interval = info->beacon_interval;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->super_status = DLR_STAT_ACTIVE_SUPERVISOR;
+	dlr_set_addr(&attrib->active_super_addr,
+		info->ip_addr, info->src_addr);
+	attrib->active_super_prec = attrib->super_cfg.prec;
+
+	/* Supervisor source address may change. */
+	info->p1_set = info->p2_set = 1;
+	enableSupervisor(info);
+	if (state->change > 0)
+		state->change--;
+}  /* dlr_active_init */
+
+static void dlr_active_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	state->delay_ann = 1;
+	state->new_state = DLR_ACTIVE_FAULT_STATE;
+	if (newSupervisor)
+		state->new_state = DLR_BACKUP_STATE;
+dbg_bcn = 4;
+}  /* dlr_active_next */
+
+static void dlr_active_fault_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+dbg_msg("  %d %d; ", info->beacon_info[0].timeout, info->beacon_info[1].timeout);
+	dbg_dlr(info, "active fault");
+	dlr_print(info, "active fault");
+	dbg_supervisor(info);
+	linkLoss = 0;
+	newSupervisor = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_status = DLR_NET_RING_FAULT;
+	memset(&attrib->last_active[0], 0,
+		sizeof(struct ksz_dlr_active_node));
+	memset(&attrib->last_active[1], 0,
+		sizeof(struct ksz_dlr_active_node));
+
+dbg_ann = 4;
+	flushMacTable(info);
+	enableBothPorts(info);
+	setupBeacons(info);
+	enableAnnounce(info, state->delay_ann);
+	state->delay_ann = 0;
+dbg_bcn = 3;
+}  /* dlr_active_fault_init */
+
+static void dlr_active_fault_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (oneBeaconTimeout || twoBeaconsTimeout) {
+		oneBeaconTimeout = twoBeaconsTimeout = 0;
+		if (!info->neigh_chk) {
+			LocateFault(info);
+			NeighborCheck(info);
+		}
+else
+dbg_msg("chk in progress\n");
+		if (info->start && info->chk_hw)
+			dlr_chk_supervisor(info);
+	}
+	if (twoBeaconsRcvd) {
+#if 0
+		dbg_dlr(info, "to active normal");
+#endif
+		disableNeighChkTimers(info);
+		state->new_state = DLR_ACTIVE_NORMAL_STATE;
+	}
+	if (newSupervisor) {
+dbg_msg("to backup\n");
+		disableNeighChkTimers(info);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+	if (newValue) {
+		state->new_state = DLR_RESTART_STATE;
+	}
+#if 0
+	if (info->chk_hw) {
+		info->chk_hw = 0;
+		dlr_chk_supervisor(info);
+	}
+#endif
+}  /* dlr_active_fault_next */
+
+static void dlr_active_normal_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	dbg_dlr(info, "active normal");
+	dlr_print(info, "active normal");
+	announcedState = RING_NORMAL_STATE;
+
+	attrib->net_status = DLR_NET_NORMAL;
+
+	/* Allow timeout notification. */
+	info->beacon_timeout_ports = 0;
+dbg_ann = 4;
+	enableOnePort(info);
+	flushMacTable(info);
+	setupBeacons(info);
+	enableAnnounce(info, 0);
+
+	/*
+	 * Need to wait until the normal beacons are
+	 * sent.
+	 */
+	info->signon_delay = 1;
+	if (!info->signon_start) {
+		info->signon_start = 1;
+		startSignOn(info, true);
+	}
+dbg_bcn += 8;
+}  /* dlr_active_normal_init */
+
+static void dlr_active_normal_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if (oneBeaconTimeout || twoBeaconsTimeout) {
+		dbg_dlr(info, "active timeout");
+		disableSignOnTimer(info);
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			if (twoBeaconsTimeout && info->start)
+				dlr_chk_supervisor(info);
+			state->new_state = DLR_ACTIVE_FAULT_STATE;
+		} else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (linkDown) {
+dbg_msg(" s linkDown\n");
+		disableNeighChkTimers(info);
+		state->new_state = DLR_ACTIVE_FAULT_STATE;
+	}
+	if (linkLoss || linkStatus) {
+		dbg_dlr(info, "active loss");
+		twoBeaconsRcvd = 0;
+		disableSignOnTimer(info);
+		if (DLR_ACTIVE_SUPERVISOR == info->node)
+			state->new_state = DLR_ACTIVE_FAULT_STATE;
+		else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (newSupervisor) {
+dbg_msg("to backup\n");
+		dbg_dlr(info, "to back");
+dbg_active = 1;
+		twoBeaconsRcvd = 0;
+dbg_bcn += 4;
+		disableSignOnTimer(info);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+	if (newValue) {
+		state->new_state = DLR_RESTART_STATE;
+	}
+	if (DLR_ACTIVE_FAULT_STATE == state->new_state)
+		attrib->fault_cnt++;
+	if (state->new_state) {
+		if (info->skip_beacon)
+			acceptBeacons_(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+}  /* dlr_active_normal_next */
+
+static void dlr_backup_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	dbg_dlr(info, "backup");
+	newSupervisor = 0;
+	info->node = DLR_SUPERVISOR;
+	disableSupervisor(info);
+	enableBothPorts(info);
+	state->new_state = DLR_FAULT_STATE;
+}  /* dlr_backup_init */
+
+static void dlr_backup_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+}
+
+static void dlr_prepare_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+dbg_dlr(info, "prepare");
+	wait_for_timeout(info->beacon_timeout);
+	info->wait_done = 1;
+}  /* dlr_prepare_init */
+
+static void dlr_prepare_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (info->wait_done) {
+dbg_dlr(info, "to active");
+		info->wait_done = 0;
+		state->new_state = DLR_ACTIVE_STATE;
+	}
+	if (newSupervisor)
+		state->new_state = DLR_BACKUP_STATE;
+}  /* dlr_prepare_init */
+
+static void dlr_restart_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	dbg_dlr(info,"restart");
+
+	/* Disable timeout notification. */
+	info->beacon_timeout_ports = 3;
+	info->wait_done = 0;
+	info->node = DLR_SUPERVISOR;
+	disableSupervisor(info);
+	disableAnnounce(info);
+	dlr_clear(info);
+dbg_bcn += 5;
+
+	/* Reset to ring fault state. */
+	dlr_set_state(info);
+	wait_for_timeout(info->beacon_timeout * 2);
+	info->wait_done = 1;
+}  /* dlr_restart_init */
+
+static void dlr_restart_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (info->wait_done) {
+	if (oneBeaconRcvd || twoBeaconsRcvd)
+dbg_msg("  have beacon 0\n");
+		info->wait_done = 0;
+		updateValues(info);
+		info->beacon_info[0].timer =
+		info->beacon_info[1].timer = 1;
+		state->new_state = DLR_ACTIVE_STATE;
+	if (oneBeaconRcvd || twoBeaconsRcvd)
+dbg_msg("  have beacon 1\n");
+	}
+	if (oneBeaconRcvd || twoBeaconsRcvd) {
+		enableBothPorts(info);
+
+		/* If not updated in previous code. */
+		if (newValue) {
+			updateValues(info);
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 1;
+		}
+		state->new_state = DLR_FAULT_STATE;
+	}
+}  /* dlr_restart_next */
+
+static int dlr_proc_state(struct ksz_dlr_info *info, struct dlr_state *state,
+	void (*state_init)(struct ksz_dlr_info *info, struct dlr_state *state),
+	void (*state_next)(struct ksz_dlr_info *info, struct dlr_state *state))
+{
+	if (state->new_state) {
+		state->new_state = 0;
+		state_init(info, state);
+		if (1 == state->change)
+			return 1;
+	}
+	state_next(info, state);
+	return 0;
+}  /* dlr_proc_state */
+
+static void RingSupervisor_state(struct ksz_dlr_info *info)
+{
+	struct dlr_state state_info;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	state_info.change = 1;
+	state_info.delay_ann = 0;
+	state_info.new_state = 0;
+#if 0
+dbg_msg(" %s %d\n", __func__, info->state);
+#endif
+inside_state = 1;
+dbg_leak = 5;
+	do {
+		state_info.change--;
+		switch (info->state) {
+		case DLR_BEGIN:
+			dlr_clear(info);
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 1;
+			info->LastBcnRcvPort = 0;
+			memset(&attrib->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attrib->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			attrib->participants_cnt = 0;
+			if (info->skip_beacon)
+				acceptBeacons_(info);
+			if (DLR_BEACON_NODE >= info->node)
+				state_info.new_state = DLR_IDLE_STATE;
+			else
+				state_info.new_state = DLR_ACTIVE_STATE;
+			state_info.change = 1;
+			break;
+		case DLR_IDLE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_idle_init, dlr_idle_next))
+				goto done;
+			break;
+		case DLR_FAULT_STATE:
+dbg_active = 0;
+			if (dlr_proc_state(info, &state_info,
+			    dlr_fault_init, dlr_fault_next))
+				goto done;
+			break;
+		case DLR_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_normal_init, dlr_normal_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_init, dlr_active_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_fault_init, dlr_active_fault_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_normal_init, dlr_active_normal_next))
+				goto done;
+			break;
+		case DLR_BACKUP_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_backup_init, dlr_backup_next))
+				goto done;
+			break;
+		case DLR_PREPARE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_prepare_init, dlr_prepare_next))
+				goto done;
+			break;
+		case DLR_RESTART_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_restart_init, dlr_restart_next))
+				goto done;
+			break;
+		}
+		if (info->reset) {
+			info->reset = 0;
+			state_info.new_state = 0;
+			info->state = DLR_BEGIN;
+			state_info.change++;
+		}
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			info->state = state_info.new_state;
+			state_info.change++;
+		}
+	} while (state_info.change);
+
+done:
+inside_state = 0;
+#if 0
+dbg_msg("  %s %d. \n", __func__, info->state);
+#endif
+}  /* RingSupervisor_state */
+
+static void AnnounceRingNode_state(struct ksz_dlr_info *info)
+{
+	struct dlr_state state_info;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	state_info.change = 1;
+	state_info.delay_ann = 0;
+	state_info.new_state = 0;
+inside_state = 1;
+	do {
+		state_info.change--;
+		switch (info->state) {
+		case DLR_BEGIN:
+			dlr_clear(info);
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 0;
+			announceRcvd = 0;
+			announcedState = 0;
+			memset(&attrib->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attrib->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			attrib->participants_cnt = 0;
+			if (info->skip_beacon)
+				acceptBeacons_(info);
+			state_info.new_state = DLR_IDLE_STATE;
+			state_info.change = 1;
+			break;
+		case DLR_IDLE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_idle_init, dlr_ann_idle_next))
+				goto done;
+			break;
+		case DLR_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_fault_init, dlr_ann_fault_next))
+				goto done;
+			break;
+		case DLR_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_normal_init, dlr_ann_normal_next))
+				goto done;
+			break;
+		}
+		if (info->reset) {
+			info->reset = 0;
+			state_info.new_state = 0;
+			info->state = DLR_BEGIN;
+			state_info.change++;
+		}
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			info->state = state_info.new_state;
+			state_info.change++;
+		}
+	} while (state_info.change);
+
+done:
+inside_state = 0;
+}  /* AnnounceRingNode_state */
+
+static void *check_dlr_frame(u8 *data)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+#if 0
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+
+			ptr += VLAN_HLEN;
+			vlan = (struct vlan_ethhdr *) ptr;
+		}
+#endif
+		if (vlan->h_vlan_encapsulated_proto == htons(DLR_TAG_TYPE))
+			return vlan + 1;
+	}
+
+	/* VLAN tag can be removed by the switch. */
+	if (vlan->h_vlan_proto == htons(DLR_TAG_TYPE)) {
+		struct ethhdr *eth = (struct ethhdr *) data;
+
+		return eth + 1;
+	}
+	return NULL;
+}  /* check_dlr_frame */
+
+static int dlr_rcv(struct ksz_dlr_info *info, struct sk_buff *skb, int port)
+{
+	struct ksz_dlr_rx_frame frame;
+	struct ksz_dlr_frame *body;
+	int update = false;
+int inside = inside_state;
+
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		port = 0;
+	else if (port == info->ports[1])
+		port = 1;
+	if (port > 1)
+		return 1;
+	body = check_dlr_frame(skb->data);
+	if (body) {
+inside_state = 0;
+		frame.vlan = (struct vlan_ethhdr *) skb->data;
+		frame.body = body;
+		switch (body->hdr.frame_type) {
+		case DLR_SIGN_ON:
+		case DLR_NEIGH_CHK_REQ:
+		case DLR_NEIGH_CHK_RESP:
+		case DLR_LOCATE_FAULT:
+		case DLR_FLUSH_TABLES:
+			/* Need to process later after state is changed. */
+			proc_dlr_hw_access(info, DEV_CMD_GET, DEV_DLR_RX,
+				port, skb);
+inside_state = inside;
+			return 0;
+			break;
+		case DLR_BEACON:
+			update = handleBeacon(info, &frame, port);
+			break;
+		case DLR_ANNOUNCE:
+			update = handleAnnounce(info, &frame, port);
+			break;
+		case DLR_LINK_STATUS:
+			update = handleLinkStatus(info, &frame, port);
+			break;
+		case DLR_LEARNING_UPDATE:
+			update = handleLearningUpdate(info, &frame, port);
+			break;
+		}
+		if (update) {
+#if 0
+dbg_msg("%s %d\n", __func__, body->hdr.frame_type);
+#endif
+			proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_UPDATE,
+				0, NULL);
+		}
+inside_state = inside;
+		dev_kfree_skb_irq(skb);
+		return 0;
+	}
+	return 1;
+}  /* dlr_rcv */
+
+static void dlr_link_change(struct ksz_dlr_info *info, int link1, int link2)
+{
+	if (handleLinkChange(info, link1, link2))
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_UPDATE, 0, NULL);
+}  /* dlr_link_change */
+
+static void proc_beacon_timeout(struct work_struct *work)
+{
+	int p;
+	struct ksz_dlr_info *dlr =
+		container_of(work, struct ksz_dlr_info, beacon_rx_timeout);
+int inside = inside_state;
+inside_state = 0;
+
+	for (p = 0; p < 2; p++) {
+		if (dlr->beacon_timeout_ports & (1 << p)) {
+			handleBeaconTimeout(dlr, p);
+		}
+	}
+	if (dlr->beacon_timeout_ports) {
+		proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_UPDATE, 0, NULL);
+		dlr->beacon_timeout_ports = 0;
+	}
+inside_state = inside;
+}  /* proc_beacon_timeout */
+
+static void dlr_timeout(struct ksz_dlr_info *info, int port)
+{
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		port = 0;
+	else if (port == info->ports[1])
+		port = 1;
+	if (port > 1)
+		return;
+	if (!(info->beacon_timeout_ports & (1 << port))) {
+		info->beacon_timeout_ports |= (1 << port);
+		schedule_work(&info->beacon_rx_timeout);
+	}
+}  /* dlr_timeout */
+
+static void dlr_stop(struct ksz_dlr_info *info)
+{
+	if (info->skip_beacon)
+		acceptBeacons_(info);
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 0;
+	info->node = DLR_BEACON_NODE;
+	disableSupervisor(info);
+	enableBothPorts(info);
+	disableAnnounce(info);
+	disableSignOnTimer(info);
+}  /* dlr_stop */
+
+static void dlr_update(struct ksz_dlr_info *info)
+{
+	switch (info->node) {
+	case DLR_ANNOUNCE_NODE:
+		AnnounceRingNode_state(info);
+		break;
+	default:
+		RingSupervisor_state(info);
+	}
+}  /* dlr_update */
+
+static void dlr_rx(struct ksz_dlr_info *info, int port, struct sk_buff *skb)
+{
+	struct ksz_dlr_rx_frame frame;
+	int update = false;
+
+	frame.vlan = (struct vlan_ethhdr *) skb->data;
+	frame.body = check_dlr_frame(skb->data);
+	switch (frame.body->hdr.frame_type) {
+	case DLR_SIGN_ON:
+		update = handleSignOn(info, &frame, port);
+		break;
+	case DLR_NEIGH_CHK_REQ:
+		update = handleNeighChkReq(info, &frame, port);
+		break;
+	case DLR_NEIGH_CHK_RESP:
+		update = handleNeighChkResp(info, &frame, port);
+		break;
+	case DLR_LOCATE_FAULT:
+		update = handleLocateFault(info, &frame, port);
+		break;
+	case DLR_FLUSH_TABLES:
+		update = handleFlushTables(info, &frame, port);
+		break;
+	default:
+dbg_msg("%s %d\n", __func__, frame.body->hdr.frame_type);
+	}
+	if (update) {
+dbg_msg("%s %d\n", __func__, frame.body->hdr.frame_type);
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_UPDATE,
+			0, NULL);
+	}
+	kfree_skb(skb);
+}  /* dlr_rx */
+
+static void setup_dir(struct ksz_dlr_info *info, int port)
+{
+dbg_msg("%s %d\n", __func__, port);
+	info->rx_port = port;
+	info->tx_port = (port + 1) & 1;
+	setupDir(info, port);
+}  /* setup_dir */
+
+static void proc_dlr_cmd(struct ksz_dlr_info *dlr, struct dlr_work *parent)
+{
+#if 0
+dbg_msg("%s %d\n", __func__, parent->index);
+#endif
+	switch (parent->cmd) {
+	case DEV_CMD_PUT:
+if (inside_cmd == parent->subcmd)
+inside_cmd = 0;
+		switch (parent->subcmd) {
+		case DEV_DLR_CHK_HW:
+			dlr_chk_supervisor(dlr);
+			break;
+		case DEV_DLR_CLR_SUPER:
+			dlr_clr_supervisor(dlr);
+			break;
+		case DEV_DLR_SETUP_DIR:
+			setup_dir(dlr, parent->option);
+			break;
+		case DEV_DLR_SETUP_DROP:
+			if (!dlr->drop_beacon && parent->option)
+dbg_msg(" ignore drop: %d %d\n", parent->option, dlr->skip_beacon);
+			if (!dlr->drop_beacon && parent->option)
+				break;
+#ifdef CONFIG_HAVE_ACL_HW
+#if (0 == DROP_BEACON_1)
+			if (dlr->node != DLR_ACTIVE_SUPERVISOR)
+#endif
+			setup_acl_beacon_drop(dlr, parent->option);
+#endif
+			dlr->skip_beacon = !!parent->option;
+			break;
+		case DEV_DLR_SETUP_TIMEOUT:
+#ifdef CONFIG_HAVE_ACL_HW
+			setup_acl_beacon_timeout(dlr,
+				dlr->ports[parent->option]);
+#endif
+			break;
+		case DEV_DLR_SETUP_VID:
+			setup_vlan_table(dlr, parent->option & ~0x80000000,
+				!!(parent->option & 0x80000000));
+			break;
+		case DEV_DLR_FLUSH:
+			dlr_flush(dlr);
+			break;
+			break;
+		case DEV_DLR_STOP:
+			dlr_stop(dlr);
+			break;
+		case DEV_DLR_UPDATE:
+			if (parent->option) {
+				dlr->reset = 1;
+			}
+			dlr_update(dlr);
+			break;
+		case DEV_DLR_START_ANNOUNCE:
+			startAnnounce(dlr);
+			break;
+		case DEV_DLR_TX_ANNOUNCE:
+			dlr_tx_announce(dlr);
+			break;
+		case DEV_DLR_TX_LOCATE:
+			dlr_tx_locate_fault(dlr);
+			break;
+		case DEV_DLR_TX_SIGNON:
+			dlr_tx_signon(dlr, parent->option);
+			break;
+		case DEV_DLR_TX_REQ:
+			dlr_tx_chk_req(dlr, parent->option);
+			break;
+		case DEV_DLR_TX_RESP:
+			dlr_tx_chk_resp(dlr, parent->option);
+			break;
+		case DEV_DLR_TX_STATUS:
+			dlr_tx_status(dlr);
+			break;
+		case DEV_DLR_TX_ADVERTISE:
+			dlr_tx_advertise(dlr);
+			break;
+		case DEV_DLR_TX_FLUSH_TABLES:
+			dlr_tx_flush_tables(dlr);
+			break;
+		case DEV_DLR_TX_LEARNING_UPDATE:
+			dlr_tx_learning_update(dlr);
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_DLR_SETUP_TIMEOUT:
+			break;
+		case DEV_DLR_RX:
+			dlr_rx(dlr, parent->option, parent->skb);
+			parent->skb = NULL;
+			break;
+		}
+		break;
+	}
+	parent->used = false;
+#if 0
+dbg_msg(" cmd:%d %p\n", parent->index, parent);
+#endif
+}  /* proc_dlr_cmd */
+
+static void proc_dlr_work(struct work_struct *work)
+{
+	struct dlr_work_info *info =
+		container_of(work, struct dlr_work_info, work);
+	struct ksz_dlr_info *dlr =
+		container_of(info, struct ksz_dlr_info, work_info);
+	struct dlr_work *cmd;
+
+	cmd = &info->works[info->head];
+	while (cmd->used) {
+		proc_dlr_cmd(dlr, cmd);
+		info->head++;
+		info->head &= DLR_WORK_LAST;
+		cmd = &info->works[info->head];
+	}
+dlr->work_info.ready = 0;
+}  /* proc_dlr_work */
+
+static void init_dlr_work(struct ksz_dlr_info *dlr)
+{
+	struct dlr_work_info *info = &dlr->work_info;
+	struct dlr_work *work;
+	int i;
+
+	for (i = 0; i < DLR_WORK_NUM; i++) {
+		work = &info->works[i];
+		work->index = i;
+		work->prev = &info->works[(i - 1) & DLR_WORK_LAST];
+	}
+	info->head = info->tail = 0;
+	INIT_WORK(&info->work, proc_dlr_work);
+}  /* init_dlr_work */
+
+static void prep_dlr_addr(struct ksz_dlr_info *dlr, u8 *src)
+{
+	memcpy(dlr->src_addr, src, ETH_ALEN);
+	memcpy(dlr->frame.vlan.h_source, src, ETH_ALEN);
+	memcpy(dlr->update_frame.eth.h_source, src, ETH_ALEN);
+	if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+		memcpy(dlr->attrib.active_super_addr.addr, src, ETH_ALEN);
+}  /* prep_dlr_addr */
+
+static void dlr_change_addr(struct ksz_dlr_info *dlr, u8 *addr)
+{
+	struct ksz_dlr_gateway_capable *attrib = &dlr->attrib;
+	struct ksz_sw *sw = dlr->sw_dev;
+
+dbg_msg("%s\n", __func__);
+	/* Do not do anything if device is not ready. */
+	if (!dlr->dev || !netif_running(dlr->dev))
+		return;
+	if (!memcmp(dlr->src_addr, addr, ETH_ALEN))
+		return;
+dbg_msg("%s.\n", __func__);
+	sw->ops->cfg_mac(sw, 3, dlr->src_addr, 0, false, false, 0);
+	prep_dlr_addr(dlr, addr);
+	memcpy(dlr->signon_frame, &dlr->frame, sizeof(struct vlan_ethhdr) +
+		sizeof(struct ksz_dlr_hdr));
+	sw->ops->cfg_mac(sw, 3, dlr->src_addr, sw->HOST_MASK, false, false, 0);
+	if (DLR_SUPERVISOR == dlr->node &&
+	    dlr->attrib.super_cfg.prec == attrib->active_super_prec) {
+		int cmp = memcmp(dlr->src_addr, attrib->active_super_addr.addr,
+			ETH_ALEN);
+
+		if (cmp > 0) {
+			proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_UPDATE, 0,
+				NULL);
+		}
+	}
+}  /* dlr_change_addr */
+
+static void prep_dlr(struct ksz_dlr_info *dlr, struct net_device *dev, u8 *src)
+{
+	dlr->dev = dev;
+	prep_dlr_addr(dlr, src);
+	dlr->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | dlr->vid);
+	memcpy(dlr->signon_frame, &dlr->frame, sizeof(struct vlan_ethhdr) +
+		sizeof(struct ksz_dlr_hdr));
+
+	dlr->active_port = 0;
+	dlr->seqid = 0;
+#if 1
+	dlr->seqid = 0xffffffe0;
+#endif
+	dlr->seqid_beacon = 0;
+	dlr->state = DLR_BEGIN;
+	proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_UPDATE, 0, NULL);
+	do {
+		struct ksz_sw *sw = dlr->sw_dev;
+
+		sw->ops->cfg_mac(sw, 3, dlr->src_addr, sw->HOST_MASK, false,
+			false, 0);
+	} while (0);
+	setup_vlan_table(dlr, dlr->vid, true);
+}  /* prep_dlr */
+
+static unsigned long last_jiffies;
+
+#ifndef CONFIG_HAVE_DLR_HW
+static void chk_beacon_timeout(struct ksz_dlr_info *dlr)
+{
+	int p;
+
+	for (p = 0; p < 2; p++) {
+		struct ksz_dlr_beacon_info *info = &dlr->beacon_info[p];
+
+		/* Beacon timeout timer enabled. */
+		if (info->timer) {
+printk("  !!!!\n");
+			info->interval += BEACON_INTERVAL;
+			if (info->rcv_once) {
+				info->rcv_once = 0;
+				info->timeout_start = 1;
+				info->timeout_stop = 0;
+				info->interval = 0;
+			} else if (info->timeout_start)
+				info->timeout_stop = 1;
+			if (info->interval >= dlr->beacon_timeout &&
+			    info->timeout_stop) {
+#if 1
+dbg_msg("  Timeout: %d %d %d %d %d %d %lx\n", p, info->rcv_once, info->timeout_start,
+info->timeout_stop, info->interval, bcn_cnt, jiffies);
+#endif
+dbg_bcn = 3;
+				info->timeout_start = info->timeout_stop = 0;
+				dlr_timeout(dlr, dlr->ports[p]);
+			}
+		}
+	}
+}  /* chk_beacon_timeout */
+#endif
+
+static void beacon_monitor(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_dlr_info *dlr =
+		container_of(dwork, struct ksz_dlr_info, beacon_tx);
+
+++bcn_cnt;
+if (jiffies - last_jiffies > BEACON_TICK * 2)
+dbg_msg("%lx %lx\n", last_jiffies, jiffies);
+last_jiffies = jiffies;
+	/* Simulate beacon interval. */
+	dlr->interval += BEACON_INTERVAL;
+	if (dlr->interval >= dlr->beacon_interval) {
+		dlr->interval = 0;
+
+		/* Send beacons for supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+			dlr_tx_beacon(dlr);
+		}
+	}
+
+#ifndef CONFIG_HAVE_DLR_HW
+	chk_beacon_timeout(dlr);
+#endif
+	schedule_delayed_work(&dlr->beacon_tx, BEACON_TICK);
+}  /* beacon_monitor */
+
+static void announce_monitor(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_dlr_info *dlr =
+		container_of(dwork, struct ksz_dlr_info, announce_tx);
+int inside = inside_state;
+
+inside_state = 0;
+	proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_CLR_SUPER, 0, NULL);
+#if 0
+	if (dlr->chk_hw) {
+		dlr->chk_hw = 0;
+		proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_CHK_HW, 0, NULL);
+	}
+#endif
+inside_state = inside;
+	if (DLR_ACTIVE_SUPERVISOR != dlr->node)
+		return;
+
+	if (dlr->ann_delay) {
+		wait_for_timeout(dlr->beacon_timeout * 2);
+	}
+inside_state = 0;
+if (dbg_ann)
+dbg_msg(" sched ann: %d\n", dlr->signon_delay);
+	proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_TX_ANNOUNCE, 0, NULL);
+	if (dlr->signon_delay) {
+dbg_msg("delay signon: %lx\n", jiffies);
+		proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_TX_SIGNON, 0,
+			NULL);
+		dlr->signon_delay = 0;
+	}
+inside_state = inside;
+	schedule_delayed_work(&dlr->announce_tx, 100);
+}  /* announce_monitor */
+
+static void announce_timeout_monitor(unsigned long ptr)
+{
+	struct ksz_dlr_info *info = (struct ksz_dlr_info *) ptr;
+int inside = inside_state;
+inside_state = 0;
+
+	info->ann_timeout = 1;
+	if (DLR_ANNOUNCE_NODE == info->node)
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_UPDATE, 0, NULL);
+inside_state = inside;
+	ksz_update_timer(&info->announce_timeout_timer_info);
+}  /* announce_timeout_monitor */
+
+static void neigh_chk_monitor(unsigned long ptr)
+{
+	int p;
+	struct ksz_dlr_info *dlr = (struct ksz_dlr_info *) ptr;
+	int checking = 0;
+	int lost = false;
+
+	/* Neighbor_Check_Request timeout. */
+	for (p = 0; p < 2; p++) {
+
+		/* This port has sent a Neighbor_Check_Request frame. */
+		if (dlr->port_chk[p]) {
+			++checking;
+			++dlr->port_chk[p];
+			if (dlr->port_chk[p] > 3) {
+				if (p)
+					dlr->p2_lost = 1;
+				else
+					dlr->p1_lost = 1;
+				lost = true;
+				--checking;
+			} else
+				proc_dlr_hw_access(dlr, DEV_CMD_PUT,
+					DEV_DLR_TX_REQ, p, NULL);
+		}
+	}
+	if (lost) {
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+			struct ksz_dlr_node_info *node;
+
+			node = find_dlr_node(dlr, dlr->src_addr);
+			if (node) {
+				if (dlr->p1_lost)
+					node->p1_lost = 1;
+				if (dlr->p2_lost)
+					node->p2_lost = 1;
+			}
+		} else
+			proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_TX_STATUS,
+				0, NULL);
+	}
+	ksz_update_timer(&dlr->neigh_chk_timer_info);
+	dlr->neigh_chk = !!dlr->neigh_chk_timer_info.max;
+}  /* neigh_chk_monitor */
+
+static void signon_monitor(unsigned long ptr)
+{
+	struct ksz_dlr_info *info = (struct ksz_dlr_info *) ptr;
+
+dbg_msg("%s\n", __func__);
+	if (DLR_ACTIVE_SUPERVISOR == info->node)
+		proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_SIGNON, 0,
+			NULL);
+	else
+		info->signon_timer_info.max = 1;
+	ksz_update_timer(&info->signon_timer_info);
+}  /* signon_monitor */
+
+static void dlr_reset_attrib(struct ksz_dlr_info *dlr)
+{
+	memset(&dlr->attrib, 0, sizeof(struct ksz_dlr_gateway_capable));
+#if 0
+	dlr->attrib.super_status = 0xFF;
+	dlr->attrib.participants_cnt = 0xFFFF;
+#endif
+	switch (dlr->node) {
+	case DLR_ANNOUNCE_NODE:
+		dlr->attrib.cap = DLR_CAP_ANNOUNCE_BASED;
+		dlr->attrib.super_status = DLR_STAT_RING_NODE;
+		break;
+	case DLR_BEACON_NODE:
+		dlr->attrib.cap = DLR_CAP_BEACON_BASED;
+		dlr->attrib.cap |= DLR_CAP_SUPERVISOR_CAPABLE;
+		dlr->attrib.super_status = DLR_STAT_RING_NODE;
+		dlr->attrib.super_cfg.beacon_interval = 400;
+		dlr->attrib.super_cfg.beacon_timeout = 1960;
+		dlr->attrib.super_cfg.beacon_interval *= 2;
+		dlr->attrib.super_cfg.beacon_timeout *= 1;
+		break;
+	default:
+		dlr->attrib.cap = DLR_CAP_BEACON_BASED;
+		dlr->attrib.cap |= DLR_CAP_SUPERVISOR_CAPABLE;
+		dlr->attrib.super_status = DLR_STAT_ACTIVE_SUPERVISOR;
+		dlr->attrib.super_cfg.enable = true;
+		dlr->attrib.super_cfg.beacon_interval = 400;
+		dlr->attrib.super_cfg.beacon_timeout = 1960;
+		dlr->attrib.super_cfg.beacon_interval *= 2;
+		dlr->attrib.super_cfg.beacon_timeout *= 1;
+	}
+}  /* dlr_reset_attrib */
+
+static void setup_dlr(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	dlr_reset_attrib(dlr);
+
+	dlr->ip_addr = 0;
+	dlr->vid = 0;
+	dlr->precedence = 0;
+	dlr->beacon_interval = 400;
+	dlr->beacon_timeout = 1960;
+	dlr->beacon_interval *= 2;
+	dlr->beacon_timeout *= 1;
+
+	dlr->frame.vlan.h_vlan_proto = htons(ETH_P_8021Q);
+	dlr->frame.vlan.h_vlan_encapsulated_proto = htons(DLR_TAG_TYPE);
+	frame->hdr.ring_subtype = DLR_RING_SUBTYPE;
+	frame->hdr.ring_protocol_version = 1;
+	frame->hdr.ip_addr = htonl(dlr->ip_addr);
+	dlr->tx_frame = (u8 *) &dlr->frame;
+
+	memcpy(dlr->update_frame.eth.h_dest, MAC_ADDR_LEARNING_UPDATE,
+		ETH_ALEN);
+	dlr->update_frame.eth.h_proto = htons(DLR_TAG_TYPE);
+	dlr->update_frame.hdr.ring_subtype = DLR_RING_SUBTYPE;
+	dlr->update_frame.hdr.ring_protocol_version = 1;
+	dlr->update_frame.hdr.frame_type = DLR_LEARNING_UPDATE;
+	dlr->update_frame.hdr.src_port = DLR_PORT_NONE;
+	memset(dlr->update_frame.reserved, 0, 34);
+
+	dlr->p1_set = dlr->p2_set = 1;
+
+	dlr->port = 0;
+	dlr->tx_port = dlr->port;
+	dlr->rx_port = (dlr->tx_port + 1) & 1;
+
+	ksz_init_timer(&dlr->announce_timeout_timer_info, 1200 * HZ / 1000,
+		announce_timeout_monitor, dlr);
+	ksz_init_timer(&dlr->neigh_chk_timer_info, 100 * HZ / 1000,
+		neigh_chk_monitor, dlr);
+	ksz_init_timer(&dlr->signon_timer_info, 60000 * HZ / 1000,
+		signon_monitor, dlr);
+
+}  /* setup_dlr */
+
+static int dlr_get_attrib(struct ksz_dlr_info *dlr, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_dlr_node_info *cur;
+	struct ksz_dlr_active_node *node;
+	int i;
+	struct ksz_dlr_gateway_capable *attr = &dlr->attrib;
+	u8 svc = (u8)(subcmd >> CIP_SVC_S);
+	u8 class = (u8)(subcmd >> CIP_CLASS_S);
+	u8 code = (u8)(subcmd >> CIP_ATTR_S);
+	u8 id = (u8) subcmd;
+
+	*len = 0;
+	*output = 0;
+	if (class != CLASS_DLR_OBJECT)
+		return DEV_IOC_INVALID_CMD;
+	if (svc != SVC_GET_ATTRIBUTES_ALL &&
+	    svc != SVC_GET_ATTRIBUTE_SINGLE &&
+	    svc != SVC_GET_MEMBER)
+		return DEV_IOC_INVALID_CMD;
+	if (CIP_INSTANCE_ATTRIBUTES == code) {
+		if (SVC_GET_ATTRIBUTES_ALL == svc) {
+			*len = sizeof(struct ksz_dlr_super_capable_2);
+			if (attr->cap & DLR_CAP_GATEWAY_CAPABLE)
+				*len = sizeof(struct ksz_dlr_gateway_capable);
+			memcpy(data, attr, *len);
+		} else if (SVC_GET_ATTRIBUTE_SINGLE == svc) {
+			union dlr_data *attrib = (union dlr_data *) data;
+
+			switch (id) {
+			case DLR_GET_NETWORK_TOPOLOGY:
+				*len = 1;
+				attrib->byte = attr->net_topology;
+				break;
+			case DLR_GET_NETWORK_STATUS:
+				*len = 1;
+				attrib->byte = attr->net_status;
+				break;
+			case DLR_GET_RING_SUPERVISOR_STATUS:
+				*len = 1;
+				attrib->byte = attr->super_status;
+				break;
+			case DLR_SET_RING_SUPERVISOR_CONFIG:
+				*len = sizeof(struct ksz_dlr_super_cfg);
+				memcpy(&attrib->super_cfg, &attr->super_cfg,
+					*len);
+				break;
+			case DLR_SET_RING_FAULT_COUNT:
+				*len = 2;
+				attrib->word = attr->fault_cnt;
+				break;
+			case DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,	&attr->last_active[0],
+					*len);
+				break;
+			case DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,	&attr->last_active[1],
+					*len);
+				break;
+			case DLR_GET_RING_PARTICIPANTS_COUNT:
+				*len = 2;
+				attrib->word = attr->participants_cnt;
+				break;
+			case DLR_GET_ACTIVE_SUPERVISOR_ADDRESS:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,
+					&attr->active_super_addr, *len);
+				break;
+			case DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE:
+				*len = 1;
+				attrib->byte = attr->active_super_prec;
+				break;
+			case DLR_GET_CAPABILITY_FLAGS:
+				*len = 4;
+				attrib->dword = attr->cap;
+				break;
+			case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = sizeof(struct ksz_dlr_gateway_cfg);
+				memcpy(&attrib->gateway_cfg, &attr->gateway_cfg,
+					*len);
+				break;
+			case DLR_GET_REDUNDANT_GATEWAY_STATUS:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = 1;
+				attrib->byte = attr->gateway_status;
+				break;
+			case DLR_GET_ACTIVE_GATEWAY_ADDRESS:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,
+					&attr->active_gateway_addr, *len);
+				break;
+			case DLR_GET_ACTIVE_GATEWAY_PRECEDENCE:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = 1;
+				attrib->byte = attr->active_gateway_prec;
+				break;
+			case DLR_GET_RING_PARTICIPANTS_LIST:
+				*len = sizeof(struct ksz_dlr_active_node);
+				if (attr->super_status !=
+				    DLR_STAT_ACTIVE_SUPERVISOR) {
+					*output = STATUS_OBJECT_STATE_CONFLICT;
+					break;
+				}
+				if (attr->participants_cnt > 1) {
+					*output = STATUS_REPLY_DATA_TOO_LARGE;
+					break;
+				}
+				node = (struct ksz_dlr_active_node *) data;
+				*len *= attr->participants_cnt;
+				for (i = 0; i < attr->participants_cnt; i++) {
+					cur = &dlr->nodes[i];
+					node->ip_addr = cur->signon.ip_addr;
+					memcpy(&node->addr, cur->signon.addr,
+						ETH_ALEN);
+					node++;
+				}
+				break;
+			}
+		} else if (SVC_GET_MEMBER == svc) {
+			switch (id) {
+			case DLR_GET_RING_PARTICIPANTS_LIST:
+				*len = sizeof(struct ksz_dlr_active_node);
+				if (attr->super_status !=
+				    DLR_STAT_ACTIVE_SUPERVISOR) {
+					*output = STATUS_OBJECT_STATE_CONFLICT;
+					break;
+				}
+				node = (struct ksz_dlr_active_node *) data;
+				i = 0;
+				cur = &dlr->nodes[i];
+				node->ip_addr = cur->signon.ip_addr;
+				memcpy(&node->addr, cur->signon.addr,
+					ETH_ALEN);
+				break;
+			}
+		}
+	} else if (CIP_CLASS_ATTRIBUTES == code) {
+		union dlr_data *attrib = (union dlr_data *) data;
+
+		if (DLR_GET_REVISION == id) {
+			*len = 2;
+			attrib->word = DLR_REVISION;
+		}
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	return DEV_IOC_OK;
+}  /* dlr_get_attrib */
+
+static int dlr_change_cfg(struct ksz_dlr_info *dlr,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	struct ksz_dlr_gateway_capable *attrib = &dlr->attrib;
+	struct ksz_dlr_super_cfg *super = &attrib->super_cfg;
+int inside = inside_state;
+
+	if (cfg->beacon_interval < 100 || cfg->beacon_interval > 100000 ||
+	    cfg->beacon_timeout < 1000 || cfg->beacon_timeout > 2000000)
+		return STATUS_INVALID_ATTRIB_VALUE;
+inside_state = 0;
+	if (cfg->enable != super->enable) {
+		disableAnnounceTimeout(dlr);
+		if (super->enable) {
+			dlr->node = DLR_BEACON_NODE;
+			proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_STOP, 0,
+				NULL);
+		} else
+			dlr->node = DLR_SUPERVISOR;
+		super->enable = cfg->enable;
+		proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_UPDATE, 1, NULL);
+	} else if (super->enable) {
+		u8 prec = super->prec;
+
+		if (cfg->prec != prec && DLR_SUPERVISOR == dlr->node) {
+			int cmp = memcmp(dlr->src_addr,
+				attrib->active_super_addr.addr, ETH_ALEN);
+
+			if (cfg->prec < attrib->active_super_prec ||
+			    (cfg->prec == attrib->active_super_prec &&
+			    cmp < 0)) {
+				prec = cfg->prec;
+			}
+		}
+		if (cfg->prec != prec) {
+			super->prec = cfg->prec;
+			dlr->new_val = 1;
+		}
+		if (cfg->beacon_interval != super->beacon_interval) {
+			super->beacon_interval = cfg->beacon_interval;
+			dlr->new_val = 1;
+		}
+		if (cfg->beacon_timeout != super->beacon_timeout) {
+			super->beacon_timeout = cfg->beacon_timeout;
+			dlr->new_val = 1;
+		}
+		if (cfg->vid != super->vid) {
+			super->vid = cfg->vid;
+			dlr->new_val = 1;
+		}
+	}
+	if (dlr->new_val)
+		proc_dlr_hw_access(dlr, DEV_CMD_PUT, DEV_DLR_UPDATE, 0, NULL);
+inside_state = inside;
+	return 0;
+}  /* dlr_change_cfg */
+
+static int dlr_set_attrib(struct ksz_dlr_info *dlr, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_dlr_gateway_capable *attr = &dlr->attrib;
+	int len = 0;
+	u8 svc = (u8)(subcmd >> CIP_SVC_S);
+	u8 class = (u8)(subcmd >> CIP_CLASS_S);
+	u8 code = (u8)(subcmd >> CIP_ATTR_S);
+	u8 id = (u8) subcmd;
+	union dlr_data *attrib = (union dlr_data *) data;
+
+	*output = 0;
+	if (class != CLASS_DLR_OBJECT)
+		return DEV_IOC_INVALID_CMD;
+	switch (svc) {
+	case SVC_SET_ATTRIBUTE_SINGLE:
+		if (CIP_INSTANCE_ATTRIBUTES != code)
+			return DEV_IOC_INVALID_CMD;
+		switch (id) {
+		case DLR_SET_RING_SUPERVISOR_CONFIG:
+			len = sizeof(struct ksz_dlr_super_cfg);
+			break;
+		case DLR_SET_RING_FAULT_COUNT:
+			len = 2;
+			break;
+		case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+			if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+				break;
+			len = sizeof(struct ksz_dlr_gateway_cfg);
+			break;
+		}
+		if (!len)
+			return DEV_IOC_INVALID_CMD;
+		if (size < len) {
+			*req_size = len + SIZEOF_ksz_request;
+			return DEV_IOC_INVALID_LEN;
+		}
+		switch (id) {
+		case DLR_SET_RING_SUPERVISOR_CONFIG:
+			*output = dlr_change_cfg(dlr, &attrib->super_cfg);
+			break;
+		case DLR_SET_RING_FAULT_COUNT:
+			if (attrib->word) {
+				*output = STATUS_INVALID_ATTRIB_VALUE;
+				break;
+			}
+			attr->fault_cnt = attrib->word;
+			break;
+		case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+			if (memcmp(&attr->gateway_cfg,&attrib->gateway_cfg,
+			    len)) {
+				memcpy(&attr->gateway_cfg, &attrib->gateway_cfg,
+					len);
+			}
+			break;
+		}
+		break;
+	case SVC_DLR_VERIFY_FAULT_LOCATION:
+		if (attr->super_status !=
+		    DLR_STAT_ACTIVE_SUPERVISOR ||
+		    dlr->state != DLR_ACTIVE_FAULT_STATE) {
+			*output = STATUS_OBJECT_STATE_CONFLICT;
+			memset(&attr->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attr->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			break;
+		}
+		proc_dlr_hw_access(dlr, DEV_CMD_PUT,
+			DEV_DLR_TX_LOCATE, 0, NULL);
+		break;
+	case SVC_DLR_CLEAR_RAPID_FAULTS:
+		break;
+	case SVC_DLR_RESTART_SIGN_ON:
+		if (attr->super_status !=
+		    DLR_STAT_ACTIVE_SUPERVISOR ||
+		    dlr->state != DLR_ACTIVE_NORMAL_STATE) {
+			*output = STATUS_OBJECT_STATE_CONFLICT;
+			break;
+		}
+
+		/* SignOn timer not started. */
+		if (!dlr->signon_start) {
+			dlr->signon_start = 1;
+			startSignOn(dlr, false);
+		}
+		break;
+	case SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT:
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+}  /* dlr_set_attrib */
+
+static int _dlr_dev_req(struct ksz_dlr_info *dlr, char *arg)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	int err = 0;
+	int result = 0;
+	size_t param_size;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	req_size = req->size;
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	maincmd = req->cmd;
+	subcmd = req->subcmd;
+	output = req->output;
+	len = req_size - SIZEOF_ksz_request;
+
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+#if 0
+				data[4] = mrp->version;
+				data[5] = mrp->ports;
+#endif
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+						6) ||
+						copy_to_user(req->param.data,
+						data, 6)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+#if 0
+				result = proc_mrp_hw_access(mrp,
+					maincmd, subcmd, 0,
+					data, 6, &output, true);
+#endif
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+#if 0
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, 0, &output, true);
+#endif
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		memcpy(data, &req->param, len);
+		result = dlr_set_attrib(dlr, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		req->output = output;
+		break;
+	case DEV_CMD_GET:
+		result = dlr_get_attrib(dlr, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		memcpy(req->param.data, data, param_size);
+		req_size = param_size + SIZEOF_ksz_request;
+		req->output = output;
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	req->size = req_size;
+	req->result = result;
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* _dlr_dev_req */
+
+static int dlr_dev_req(struct ksz_dlr_info *dlr, char *arg)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	int err = 0;
+	int result = 0;
+	size_t param_size;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+#if 0
+	if (!access_ok(VERIFY_READ, &req->param, 4) ||
+	    copy_from_user(data, &req->param, 4)) {
+		result = -EFAULT;
+		goto dev_ioctl_resp;
+	}
+#endif
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+#if 0
+				data[4] = mrp->version;
+				data[5] = mrp->ports;
+#endif
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+						6) ||
+						copy_to_user(req->param.data,
+						data, 6)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+#if 0
+				result = proc_mrp_hw_access(mrp,
+					maincmd, subcmd, 0,
+					data, 6, &output, true);
+#endif
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+#if 0
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, 0, &output, true);
+#endif
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data))
+			goto dev_ioctl_resp;
+		result = dlr_set_attrib(dlr, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		__put_user(output, &req->output);
+		break;
+	case DEV_CMD_GET:
+		result = dlr_get_attrib(dlr, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		if (!access_ok(VERIFY_WRITE, req->param.data, param_size) ||
+		    copy_to_user(req->param.data, data, param_size)) {
+			err = -EFAULT;
+			goto dev_ioctl_done;
+		}
+		req_size = param_size + SIZEOF_ksz_request;
+		__put_user(output, &req->output);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* dlr_dev_req */
+
+static void set_dlr_req(void *ptr, int cmd,
+	u8 svc, u8 class, u8 code, u8 id, void *dlr, size_t dlr_size)
+{
+	struct ksz_request *req = ptr;
+
+	req->size = SIZEOF_ksz_request;
+	req->size += dlr_size;
+	req->cmd = cmd;
+	req->subcmd = (svc << CIP_SVC_S) | (class << CIP_CLASS_S) |
+		(code << CIP_ATTR_S) | id;
+	req->output = 0;
+	if (dlr)
+		memcpy(&req->param, dlr, dlr_size);
+}  /* set_dlr_req */
+
+static int get_dlr_revision(void *fd,
+	u16 *rev)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_CLASS_ATTRIBUTES, DLR_GET_REVISION,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*rev = data->word;
+	}
+	return rc;
+}  /* get_dlr_revision */
+
+static int get_dlr_all(void *fd,
+	struct ksz_dlr_gateway_capable *capable)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTES_ALL,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES, 0,
+		NULL, sizeof(struct ksz_dlr_gateway_capable));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(capable, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_all */
+
+static int get_dlr_topology(void *fd,
+	u8 *topology)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_NETWORK_TOPOLOGY,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*topology = data->byte;
+	}
+	return rc;
+}  /* get_dlr_topology */
+
+static int get_dlr_network(void *fd,
+	u8 *network)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_NETWORK_STATUS,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*network = data->byte;
+	}
+	return rc;
+}  /* get_dlr_network */
+
+static int get_dlr_super_status(void *fd,
+	u8 *status)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_SUPERVISOR_STATUS,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*status = data->byte;
+	}
+	return rc;
+}  /* get_dlr_super_status */
+
+static int get_dlr_super_cfg(void *fd,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_SUPERVISOR_CONFIG,
+		NULL, sizeof(struct ksz_dlr_super_cfg));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(cfg, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_super_cfg */
+
+static int set_dlr_super_cfg(void *fd,
+	struct ksz_dlr_super_cfg *cfg, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_SET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_SUPERVISOR_CONFIG,
+		cfg, sizeof(struct ksz_dlr_super_cfg));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_super_cfg */
+
+static int get_dlr_ring_fault_cnt(void *fd,
+	u16 *cnt)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_FAULT_COUNT,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*cnt = data->word;
+	}
+	return rc;
+}  /* get_dlr_ring_fault_cnt */
+
+static int set_dlr_ring_fault_cnt(void *fd,
+	u16 cnt, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_SET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_FAULT_COUNT,
+		&cnt, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_ring_fault_cnt */
+
+static int get_dlr_active_node(void *fd,
+	u8 port, struct ksz_dlr_active_node *node)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+	u8 id;
+
+	if (1 == port)
+		id = DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2;
+	else
+		id = DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1;
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		id,
+		NULL, sizeof(struct ksz_dlr_active_node));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(node, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_active_node */
+
+static int get_dlr_ring_part_cnt(void *fd,
+	u16 *cnt)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_PARTICIPANTS_COUNT,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*cnt = data->word;
+	}
+	return rc;
+}  /* get_dlr_ring_part_cnt */
+
+static int get_dlr_ring_part_list(void *fd,
+	struct ksz_dlr_active_node *node, u16 *size, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_PARTICIPANTS_LIST,
+		NULL, *size);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+		*size = req.size - SIZEOF_ksz_request;
+		memcpy(node, &req.param, *size);
+	}
+	return rc;
+}  /* get_dlr_ring_part_list */
+
+static int get_dlr_active_super_addr(void *fd,
+	struct ksz_dlr_active_node *node)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_ACTIVE_SUPERVISOR_ADDRESS,
+		NULL, sizeof(struct ksz_dlr_active_node));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(node, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_active_super_addr */
+
+static int get_dlr_active_super_prec(void *fd,
+	u8 *prec)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*prec = data->byte;
+	}
+	return rc;
+}  /* get_dlr_active_super_prec */
+
+static int get_dlr_cap(void *fd,
+	u32 *flags)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_CAPABILITY_FLAGS,
+		NULL, sizeof(u32));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*flags = data->dword;
+	}
+	return rc;
+}  /* get_dlr_cap */
+
+static int set_dlr_verify_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_VERIFY_FAULT_LOCATION,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_verify_fault */
+
+static int set_dlr_clear_rapid_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_CLEAR_RAPID_FAULTS,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_clear_rapid_fault */
+
+static int set_dlr_restart_sign_on(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_RESTART_SIGN_ON,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_restart_sign_on */
+
+static int set_dlr_clear_gateway_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_clear_gateway_fault */
+
+enum {
+	PROC_GET_DLR_INFO,
+	PROC_SET_DLR_NODE,
+	PROC_SET_DLR_PRECEDENCE,
+	PROC_SET_DLR_INTERVAL,
+	PROC_SET_DLR_TIMEOUT,
+	PROC_SET_DLR_VID,
+	PROC_SET_DLR_STATE,
+	PROC_SET_DLR_PORT,
+	PROC_SET_DLR_TEST,
+	PROC_SET_DLR_NEIGH_CHK_REQ,
+	PROC_SET_DLR_NEIGH_CHK_RESP,
+	PROC_SET_DLR_LINK_BREAK,
+	PROC_SET_DLR_LEARNING_UPDATE,
+	PROC_SET_DLR_LOCATE_FAULT,
+	PROC_SET_DLR_SIGN_ON,
+	PROC_SET_DLR_CLEAR_FAULT,
+	PROC_SET_DLR_CLEAR_GATEWAY_FAULT,
+
+	PROC_GET_DLR_ALL,
+	PROC_GET_DLR_REVISION,
+	PROC_GET_DLR_NETWORK_TOPOLOGY,
+	PROC_GET_DLR_NETWORK_STATUS,
+	PROC_GET_DLR_RING_SUPERVISOR_STATUS,
+	PROC_SET_DLR_RING_SUPERVISOR_CONFIG,
+	PROC_SET_DLR_RING_FAULT_COUNT,
+	PROC_GET_DLR_LAST_ACTIVE_NODE_1,
+	PROC_GET_DLR_LAST_ACTIVE_NODE_2,
+	PROC_GET_DLR_RING_PARTICIPANTS_COUNT,
+	PROC_GET_DLR_RING_PARTICIPANTS_LIST,
+	PROC_GET_DLR_ACTIVE_SUPERVISOR_ADDRESS,
+	PROC_GET_DLR_ACTIVE_SUPERVISOR_PRECEDENCE,
+	PROC_GET_DLR_CAPABILITIES,
+	PROC_SET_DLR_PORT_1,
+	PROC_SET_DLR_PORT_2,
+};
+
+static void display_faults(struct ksz_dlr_info *dlr)
+{
+	int i;
+	struct ksz_dlr_node_info *node;
+	struct ksz_dlr_node *signon;
+
+	for (i = 0; i < dlr->attrib.participants_cnt; i++) {
+		node = &dlr->nodes[i];
+		signon = &node->signon;
+		if (!node->p1_down && !node->p2_down &&
+		    !node->p1_lost && !node->p2_lost)
+			continue;
+		dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x %u.%u.%u.%u\n",
+			signon->addr[0], signon->addr[1], signon->addr[2],
+			signon->addr[3], signon->addr[4], signon->addr[5],
+			(u8)(signon->ip_addr >> 24),
+			(u8)(signon->ip_addr >> 16),
+			(u8)(signon->ip_addr >> 8),
+			(u8) signon->ip_addr);
+		dbg_msg("%u:%u %u:%u\n",
+			node->p1_down, node->p2_down,
+			node->p1_lost, node->p2_lost);
+	}
+}  /* display_faults */
+
+static void display_nodes(struct ksz_dlr_info *dlr)
+{
+	int i;
+	struct ksz_dlr_node_info *node;
+	struct ksz_dlr_node *signon;
+
+	for (i = 0; i < dlr->attrib.participants_cnt; i++) {
+		node = &dlr->nodes[i];
+		signon = &node->signon;
+		dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x  %u.%u.%u.%u  ",
+			signon->addr[0], signon->addr[1], signon->addr[2],
+			signon->addr[3], signon->addr[4], signon->addr[5],
+			(u8)(signon->ip_addr >> 24),
+			(u8)(signon->ip_addr >> 16),
+			(u8)(signon->ip_addr >> 8),
+			(u8) signon->ip_addr);
+		dbg_msg("%u:%u %u:%u\n",
+			node->p1_down, node->p2_down,
+			node->p1_lost, node->p2_lost);
+	}
+}  /* display_nodes */
+
+static ssize_t show_dlr_err(ssize_t len, char *buf, u8 err)
+{
+	if (buf)
+		len += sprintf(buf + len, "!0x%02x\n", err);
+	else
+		printk(KERN_INFO "!0x%02x\n", err);
+	return len;
+}  /* show_dlr_err */
+
+static ssize_t show_dlr_attrib(ssize_t len, char *buf, u8 err, char *format,
+	u32 data)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len, format, data);
+	return len;
+}  /* show_dlr_attrib */
+
+static ssize_t show_dlr_attrib_super(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len, "E:%u  P:%u  I:%u  T:%u  V:%u\n",
+			cfg->enable,
+			cfg->prec,
+			cfg->beacon_interval,
+			cfg->beacon_timeout,
+			cfg->vid);
+	return len;
+}  /* show_dlr_attrib_super */
+
+static ssize_t show_dlr_attrib_node(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_active_node *node)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len,
+			"%u.%u.%u.%u  %02x:%02x:%02x:%02x:%02x:%02x\n",
+			(u8)(node->ip_addr >> 24),
+			(u8)(node->ip_addr >> 16),
+			(u8)(node->ip_addr >> 8),
+			(u8) node->ip_addr,
+			node->addr[0],
+			node->addr[1],
+			node->addr[2],
+			node->addr[3],
+			node->addr[4],
+			node->addr[5]);
+	return len;
+}  /* show_dlr_attrib_node */
+
+static ssize_t show_dlr_attrib_all(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_gateway_capable *capable)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else {
+		len = show_dlr_attrib(len, buf, 0,
+			"top:%u  ", capable->net_topology);
+		len = show_dlr_attrib(len, buf, 0,
+			"net:%u  ", capable->net_status);
+		len = show_dlr_attrib(len, buf, 0,
+			"sup:%u  ", capable->super_status);
+		len = show_dlr_attrib(len, buf, 0,
+			"fault:%u\n", capable->fault_cnt);
+		len = show_dlr_attrib_super(len, buf, 0,
+			&capable->super_cfg);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->last_active[0]);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->last_active[1]);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->active_super_addr);
+		len = show_dlr_attrib(len, buf, 0,
+			"cnt:%u  ", capable->participants_cnt);
+		len = show_dlr_attrib(len, buf, 0,
+			"prec:%u  ", capable->active_super_prec);
+		len = show_dlr_attrib(len, buf, 0,
+			"cap:%08x\n", capable->cap);
+	}
+	return len;
+}  /* show_dlr_attrib_all */
+
+static ssize_t sysfs_dlr_read(struct ksz_dlr_info *dlr, int proc_num,
+	ssize_t len, char *buf)
+{
+	struct ksz_dlr_active_node node;
+	struct ksz_dlr_active_node nodes[10];
+	struct ksz_dlr_super_cfg super;
+	struct ksz_dlr_gateway_capable capable;
+	int rc;
+	u32 dword;
+	u16 word;
+	u8 byte;
+	u8 err = 0;
+
+	switch (proc_num) {
+	case PROC_GET_DLR_INFO:
+		break;
+	case PROC_SET_DLR_NODE:
+		len += sprintf(buf + len, "%u\n", dlr->node);
+		break;
+	case PROC_SET_DLR_PRECEDENCE:
+		len += sprintf(buf + len, "%u\n", dlr->precedence);
+		break;
+	case PROC_SET_DLR_INTERVAL:
+		len += sprintf(buf + len, "%u\n", dlr->beacon_interval);
+		break;
+	case PROC_SET_DLR_TIMEOUT:
+		len += sprintf(buf + len, "%u\n", dlr->beacon_timeout);
+		break;
+	case PROC_SET_DLR_VID:
+		len += sprintf(buf + len, "0x%03x\n", dlr->vid);
+		break;
+	case PROC_SET_DLR_STATE:
+		len += sprintf(buf + len, "%u\n", dlr->ring_state);
+		break;
+	case PROC_SET_DLR_PORT:
+		len += sprintf(buf + len, "%u\n", dlr->port);
+		break;
+	case PROC_SET_DLR_TEST:
+		len += sprintf(buf + len, "%u\n",
+			(dlr->overrides & DLR_TEST) ? 1 : 0);
+		break;
+	case PROC_SET_DLR_LOCATE_FAULT:
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+			display_faults(dlr);
+		break;
+	case PROC_SET_DLR_SIGN_ON:
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+			display_nodes(dlr);
+		else
+			len += sprintf(buf + len, "%u\n", dlr->ignore_req);
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_REQ:
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_RESP:
+		len += sprintf(buf + len, "%u\n", dlr->ignore_req);
+		break;
+	case PROC_SET_DLR_LINK_BREAK:
+		len += sprintf(buf + len, "0x%x\n", dlr->link_break);
+		break;
+	case PROC_GET_DLR_REVISION:
+		rc = get_dlr_revision(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_ALL:
+		rc = get_dlr_all(dlr, &capable);
+		if (!rc)
+			len = show_dlr_attrib_all(len, buf, err, &capable);
+		break;
+	case PROC_GET_DLR_NETWORK_TOPOLOGY:
+		rc = get_dlr_topology(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_NETWORK_STATUS:
+		rc = get_dlr_network(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_RING_SUPERVISOR_STATUS:
+		rc = get_dlr_super_status(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_SET_DLR_RING_SUPERVISOR_CONFIG:
+		rc = get_dlr_super_cfg(dlr, &super);
+		if (!rc)
+			len = show_dlr_attrib_super(len, buf, err, &super);
+		break;
+	case PROC_SET_DLR_RING_FAULT_COUNT:
+		rc = get_dlr_ring_fault_cnt(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_LAST_ACTIVE_NODE_1:
+		rc = get_dlr_active_node(dlr, 0, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_LAST_ACTIVE_NODE_2:
+		rc = get_dlr_active_node(dlr, 1, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_RING_PARTICIPANTS_COUNT:
+		rc = get_dlr_ring_part_cnt(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_RING_PARTICIPANTS_LIST:
+		word = sizeof(nodes);
+		rc = get_dlr_ring_part_list(dlr, nodes, &word, &err);
+		if (!rc) {
+			int i;
+
+			if (err) {
+				len = show_dlr_err(len, buf, err);
+				break;
+			}
+			word /= sizeof(struct ksz_dlr_active_node);
+			for (i = 0; i < word; i++)
+				len = show_dlr_attrib_node(len, buf, 0,
+					&nodes[i]);
+		}
+		break;
+	case PROC_GET_DLR_ACTIVE_SUPERVISOR_ADDRESS:
+		rc = get_dlr_active_super_addr(dlr, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_ACTIVE_SUPERVISOR_PRECEDENCE:
+		rc = get_dlr_active_super_prec(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_CAPABILITIES:
+		rc = get_dlr_cap(dlr, &dword);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%08x\n", dword);
+		break;
+	case PROC_SET_DLR_PORT_1:
+		len += sprintf(buf + len, "%u\n", dlr->ports[0]);
+		break;
+	case PROC_SET_DLR_PORT_2:
+		len += sprintf(buf + len, "%u\n", dlr->ports[1]);
+		break;
+	}
+	return len;
+}  /* sysfs_dlr_read */
+
+static void sysfs_dlr_write(struct ksz_dlr_info *info, int proc_num, int num,
+	const char *buf)
+{
+	int rc;
+	u8 err;
+	struct ksz_dlr_super_cfg super;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_sw *sw = info->sw_dev;
+
+	switch (proc_num) {
+	case PROC_SET_DLR_NODE:
+		if (!(DLR_ANNOUNCE_NODE <= num &&
+		    num <= DLR_ACTIVE_SUPERVISOR))
+			break;
+		if (num == info->node)
+			break;
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR <= info->node) {
+			int prev_node = info->node;
+
+			info->node = DLR_BEACON_NODE;
+			disableSupervisor(info);
+			if (DLR_ACTIVE_SUPERVISOR == prev_node) {
+				enableBothPorts(info);
+				disableAnnounce(info);
+				disableSignOnTimer(info);
+			}
+		} else if (DLR_NORMAL_STATE == info->state) {
+			setupDir(info, -1);
+		}
+		if (info->skip_beacon)
+			acceptBeacons_(info);
+		if (DLR_ANNOUNCE_NODE == info->node)
+			sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY,
+				MAC_ADDR_BEACON, 0, false, false, 0);
+		if (num == info->node)
+			break;
+		info->node = (u8) num;
+		dlr_reset_attrib(info);
+		info->attrib.super_cfg.beacon_interval = info->beacon_interval;
+		info->attrib.super_cfg.beacon_timeout = info->beacon_timeout;
+dbg_bcn = 4;
+		do {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (DLR_ANNOUNCE_NODE == info->node)
+				sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY,
+					MAC_ADDR_BEACON, info->member,
+					false, false, 0);
+			proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_UPDATE,
+				1, NULL);
+		} while (0);
+		break;
+	case PROC_SET_DLR_PRECEDENCE:
+		/* Value can only be set through supervisor. */
+		if (info->node < DLR_SUPERVISOR)
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.prec = (u8) num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_INTERVAL:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num < 200) {
+			printk("too small\n");
+			break;
+		} else if (num * 4 > info->beacon_timeout) {
+			printk("too large\n");
+			break;
+		}
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.beacon_interval = num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_TIMEOUT:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num < info->beacon_interval * 4) {
+			printk("too small\n");
+			break;
+		} else if (num >= ACL_CNT_M * 1000) {
+			printk("too large\n");
+			break;
+		}
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.beacon_timeout = num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_VID:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num >= 0x400)
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.vid = (u16) num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_STATE:
+		if (1 <= num && num <= 2) {
+			info->ring_state = (u8) num;
+			dlr_set_state(info);
+		}
+		break;
+	case PROC_SET_DLR_PORT:
+		if (0 <= num && num <= 1) {
+			info->port = (u8) num;
+			info->tx_port = (u8) num;
+			info->rx_port = (info->tx_port + 1) & 1;
+		}
+		break;
+	case PROC_SET_DLR_TEST:
+		if (num & 1)
+			info->overrides |= DLR_TEST;
+		else
+			info->overrides &= ~DLR_TEST;
+		if (num & 2)
+			info->overrides |= DLR_TEST_SEQ;
+		else
+			info->overrides &= ~DLR_TEST_SEQ;
+		if (num & 4)
+			info->overrides |= DLR_BEACON_LEAK_HACK;
+		else
+			info->overrides &= ~DLR_BEACON_LEAK_HACK;
+		break;
+	case PROC_SET_DLR_LEARNING_UPDATE:
+		dlr_tx_learning_update(info);
+		break;
+	case PROC_SET_DLR_LOCATE_FAULT:
+		rc = set_dlr_verify_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_SIGN_ON:
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			info->signon_space = num;
+		} else {
+			info->ignore_req = (u8) num;
+			info->req_cnt[0] = info->req_cnt[1] = 0;
+			if (info->overrides & DLR_TEST)
+				break;
+		}
+		rc = set_dlr_restart_sign_on(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_REQ:
+		if (num & 3) {
+			if (!info->neigh_chk) {
+				info->neigh_chk = 1;
+				ksz_start_timer(&info->neigh_chk_timer_info,
+					info->neigh_chk_timer_info.period);
+			}
+			info->neigh_chk_timer_info.max = 3;
+		}
+		if (num & 1) {
+			info->port_chk[0] = 1;
+			proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_REQ,
+				0, NULL);
+		}
+		if (num & 2) {
+			info->port_chk[1] = 1;
+			proc_dlr_hw_access(info, DEV_CMD_PUT, DEV_DLR_TX_REQ,
+				1, NULL);
+		}
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_RESP:
+		info->ignore_req = (u8) num;
+		info->req_cnt[0] = info->req_cnt[1] = 0;
+		break;
+	case PROC_SET_DLR_LINK_BREAK:
+		info->link_break = num;
+		if (info->link_break & 1)
+			info->p1_down = 1;
+		else
+			info->p1_down = 0;
+		if (info->link_break & 2)
+			info->p2_down = 1;
+		else
+			info->p2_down = 0;
+		if (info->link_break & 4)
+			info->p1_lost = 1;
+		else
+			info->p1_lost = 0;
+		if (info->link_break & 8)
+			info->p2_lost = 1;
+		else
+			info->p2_lost = 0;
+		if (info->node != DLR_ACTIVE_SUPERVISOR)
+			proc_dlr_hw_access(info, DEV_CMD_PUT,
+				DEV_DLR_TX_STATUS, 0, NULL);
+		else if (attrib->participants_cnt > 0) {
+			struct ksz_dlr_node_info *node;
+
+			node = &info->nodes[0];
+			if (attrib->participants_cnt > 1) {
+				struct ksz_dlr_node_info *prev;
+				struct ksz_dlr_node_info *next;
+
+				prev = &info->nodes[attrib->participants_cnt -
+					1];
+				next = &info->nodes[1];
+				if (info->p1_lost)
+					prev->p2_lost = 1;
+				if (info->p2_lost)
+					next->p1_lost = 1;
+			}
+			node->p1_down = info->p1_down;
+			node->p2_down = info->p2_down;
+			node->p1_lost = info->p1_lost;
+			node->p2_lost = info->p2_lost;
+		}
+		break;
+	case PROC_SET_DLR_RING_SUPERVISOR_CONFIG:
+	{
+		struct ksz_dlr_super_cfg super;
+
+		/* Value can only be set through supervisor. */
+		if (!(info->attrib.cap & DLR_CAP_SUPERVISOR_CAPABLE))
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		if (num) {
+			super.enable = true;
+			if (!super.beacon_interval)
+				super.beacon_interval = 400;
+			if (!super.beacon_timeout)
+				super.beacon_timeout = 1960;
+		} else
+			super.enable = false;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	}
+	case PROC_SET_DLR_RING_FAULT_COUNT:
+		rc = set_dlr_ring_fault_cnt(info, num, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CLEAR_FAULT:
+		rc = set_dlr_clear_rapid_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CLEAR_GATEWAY_FAULT:
+		rc = set_dlr_clear_gateway_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_PORT_1:
+		if (netif_running(info->dev))
+			printk(KERN_ALERT "stop %s first", info->dev->name);
+		if (0 <= num && num < sw->phy_port_cnt &&
+		    num != sw->HOST_PORT && num != info->ports[1])
+			info->ports[0] = (u8) num;
+		info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+		break;
+	case PROC_SET_DLR_PORT_2:
+		if (netif_running(info->dev))
+			printk(KERN_ALERT "stop %s first", info->dev->name);
+		if (0 <= num && num < sw->phy_port_cnt &&
+		    num != sw->HOST_PORT && num != info->ports[0])
+			info->ports[1] = (u8) num;
+		info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+		break;
+	}
+}  /* sysfs_dlr_write */
+
+static struct dlr_ops dlr_ops = {
+	.change_addr		= dlr_change_addr,
+	.link_change		= dlr_link_change,
+	.timeout		= dlr_timeout,
+
+	.dev_req		= dlr_dev_req,
+
+	.sysfs_read		= sysfs_dlr_read,
+	.sysfs_write		= sysfs_dlr_write,
+};
+
+static void ksz_dlr_exit(struct ksz_dlr_info *dlr)
+{
+	ksz_stop_timer(&dlr->announce_timeout_timer_info);
+	ksz_stop_timer(&dlr->neigh_chk_timer_info);
+	ksz_stop_timer(&dlr->signon_timer_info);
+	cancel_delayed_work_sync(&dlr->announce_tx);
+	cancel_delayed_work_sync(&dlr->beacon_tx);
+}  /* ksz_dlr_exit */
+
+static void ksz_dlr_init(struct ksz_dlr_info *dlr, struct ksz_sw *sw)
+{
+#if 0
+	dlr->overrides = DLR_BEACON_LEAK_HACK;
+#endif
+	dlr->ports[0] = 0;
+	dlr->ports[1] = 1;
+	dlr->member = (1 << dlr->ports[0]) | (1 << dlr->ports[1]);
+	dlr->sw_dev = sw;
+	dlr->node = DLR_BEACON_NODE;
+	setup_dlr(dlr);
+	INIT_DELAYED_WORK(&dlr->announce_tx, announce_monitor);
+	INIT_DELAYED_WORK(&dlr->beacon_tx, beacon_monitor);
+	INIT_WORK(&dlr->beacon_rx_timeout, proc_beacon_timeout);
+	init_dlr_work(dlr);
+#ifdef CONFIG_HAVE_DLR_HW_
+sw->overrides |= ACL_INTR_MONITOR;
+#endif
+	dlr->ops = &dlr_ops;
+#ifdef CONFIG_HAVE_DLR_HW
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		schedule_delayed_work(&dlr->beacon_tx, BEACON_TICK);
+#endif
+}  /* ksz_dlr_init */
+
diff --git a/drivers/net/ethernet/micrel/ksz_dlr.h b/drivers/net/ethernet/micrel/ksz_dlr.h
new file mode 100644
index 0000000..aaa9fb8
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_dlr.h
@@ -0,0 +1,347 @@
+/**
+ * Micrel DLR driver header
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_DLR_H
+#define KSZ_DLR_H
+
+#include "ksz_dlr_api.h"
+
+
+#define DLR_TAG_TYPE			0x80E1
+
+#define DLR_RING_SUBTYPE		2
+
+#define DLR_BEACON			0x1
+#define DLR_NEIGH_CHK_REQ		0x2
+#define DLR_NEIGH_CHK_RESP		0x3
+#define DLR_LINK_STATUS			0x4
+#define DLR_LOCATE_FAULT		0x5
+#define DLR_ANNOUNCE			0x6
+#define DLR_SIGN_ON			0x7
+#define DLR_ADVERTISE			0x8
+#define DLR_FLUSH_TABLES		0x9
+#define DLR_LEARNING_UPDATE		0xA
+
+#define DLR_PORT_NONE			0
+#define DLR_PORT_1			1
+#define DLR_PORT_2			2
+
+#define RING_NORMAL_STATE		1
+#define RING_FAULT_STATE		2
+
+#define DLR_GW_ACTIVE_LISTEN_STATE	1
+#define DLR_GW_ACTIVE_NORMAL_STATE	2
+#define DLR_GW_FAULT_STATE		3
+
+
+struct ksz_dlr_hdr {
+	u8 ring_subtype;
+	u8 ring_protocol_version;
+	u8 frame_type;
+	u8 src_port;
+	u32 ip_addr;
+	u32 seqid;
+} __packed;
+
+struct ksz_dlr_advertise {
+	u8 state;
+	u8 precedence;
+	u32 interval;
+	u32 timeout;
+	u8 learning_update_enable;
+} __packed;
+
+struct ksz_dlr_announce {
+	u8 ring_state;
+} __packed;
+
+struct ksz_dlr_beacon {
+	u8 ring_state;
+	u8 precedence;
+	u32 interval;
+	u32 timeout;
+	u8 reserved[20];
+} __packed;
+
+struct ksz_dlr_flush_tables {
+	u8 learning_update_enable;
+} __packed;
+
+struct ksz_dlr_status {
+	u8 port1_active:1;
+	u8 port2_active:1;
+	u8 reserved:5;
+	u8 neighbor:1;
+} __packed;
+
+struct ksz_dlr_node {
+	u8 addr[ETH_ALEN];
+	u32 ip_addr;
+} __packed;
+
+struct ksz_dlr_signon {
+	u16 num;
+	struct ksz_dlr_node node[1];
+} __packed;
+
+struct ksz_dlr_neigh_chk_resp {
+	u8 src_port;
+} __packed;
+
+struct ksz_dlr_frame {
+	struct ksz_dlr_hdr hdr;
+	union {
+		struct ksz_dlr_advertise advertise;
+		struct ksz_dlr_announce announce;
+		struct ksz_dlr_beacon beacon;
+		struct ksz_dlr_flush_tables flush;
+		struct ksz_dlr_status status;
+		struct ksz_dlr_neigh_chk_resp neigh_chk_resp;
+		struct ksz_dlr_signon signon;
+		u8 reserved[30];
+	} data;
+} __packed;
+
+struct ksz_dlr_tx_frame {
+	struct vlan_ethhdr vlan;
+	struct ksz_dlr_frame body;
+} __packed;
+
+struct ksz_dlr_update_frame {
+	struct ethhdr eth;
+	struct ksz_dlr_hdr hdr;
+	u8 reserved[34];
+} __packed;
+
+struct ksz_dlr_rx_frame {
+	struct vlan_ethhdr *vlan;
+	struct ksz_dlr_frame *body;
+};
+
+#define DLR_SUPERVISOR_NUM	10
+
+struct ksz_dlr_super_info {
+	u8 prec_addr[ETH_ALEN + 1];
+	u32 crc;
+	int port;
+	u32 timeout[2];
+	u32 cnt;
+	u32 last_cnt;
+	u32 sent:1;
+};
+
+struct ksz_dlr_info;
+
+struct dlr_work {
+	struct sk_buff *skb;
+	int cmd;
+	int subcmd;
+	int option;
+	int used;
+	int index;
+	struct dlr_work *prev;
+};
+
+#define DLR_WORK_NUM			(1 << 4)
+#define DLR_WORK_LAST			(DLR_WORK_NUM - 1)
+
+struct dlr_work_info {
+	struct work_struct work;
+	int head;
+	int tail;
+int ready;
+	struct dlr_work works[DLR_WORK_NUM];
+};
+
+struct dlr_ops {
+	void (*change_addr)(struct ksz_dlr_info *dlr, u8 *addr);
+	void (*link_change)(struct ksz_dlr_info *dlr, int link1, int link2);
+	void (*timeout)(struct ksz_dlr_info *dir, int port);
+
+	int (*dev_req)(struct ksz_dlr_info *dlr, char *arg);
+
+	ssize_t (*sysfs_read)(struct ksz_dlr_info *dlr, int proc_num,
+		ssize_t len, char *buf);
+	void (*sysfs_write)(struct ksz_dlr_info *dlr, int proc_num, int num,
+		const char *buf);
+
+};
+
+struct ksz_dlr_node_info {
+	struct ksz_dlr_node signon;
+	u32 p1_down:1;
+	u32 p2_down:1;
+	u32 p1_lost:1;
+	u32 p2_lost:1;
+};
+
+struct ksz_dlr_beacon_info {
+	u32 timer:1;
+	u32 rcv_once:1;
+	u32 timeout_start:1;
+	u32 timeout_stop:1;
+	u32 interval;
+	u32 timeout;
+	struct ksz_dlr_beacon last;
+};
+
+#define DLR_BEACON_LEAK_HACK		(1 << 0)
+#define DLR_TEST_SEQ			(1 << 30)
+#define DLR_TEST			(1 << 31)
+
+struct ksz_dlr_info {
+	u32 p1_down:1;
+	u32 p2_down:1;
+	u32 p1_lost:1;
+	u32 p2_lost:1;
+	u32 p1_rcvd:1;
+	u32 p2_rcvd:1;
+	u32 p1_set:1;
+	u32 p2_set:1;
+	u32 p1_timeout:1;
+	u32 p2_timeout:1;
+	u32 one_down:1;
+	u32 both_down:1;
+	u32 one_rcvd:1;
+	u32 both_rcvd:1;
+	u32 one_timeout:1;
+	u32 both_timeout:1;
+	u32 new_supervisor:1;
+	u32 ann_rcvd:1;
+	u32 ann_timeout:1;
+	u32 ann_delay:1;
+	u32 signon_delay:1;
+	u32 signon_start:1;
+	u32 new_val:1;
+	u32 neigh_chk:1;
+	u32 state_change:1;
+	u32 wait_done:1;
+	u32 reset:1;
+	u32 reset_fault:1;
+	u32 start:1;
+	u32 chk_hw:1;
+
+	struct ksz_dlr_gateway_capable attrib;
+
+	u32 beacon_interval;
+	u32 beacon_timeout;
+	u32 ip_addr;
+	u16 vid;
+	u8 src_addr[ETH_ALEN];
+	u8 next_node;
+	u8 node;
+	u8 port;
+	u8 precedence;
+	u8 ring_state;
+	u8 drop_beacon;
+	u8 skip_beacon;
+	u8 LastBcnRcvPort;
+	struct ksz_dlr_beacon_info beacon_info[2];
+	u32 interval;
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct ksz_timer_info announce_timer_info;
+	struct ksz_timer_info announce_timeout_timer_info;
+	struct ksz_timer_info neigh_chk_timer_info;
+	struct ksz_timer_info signon_timer_info;
+	struct delayed_work announce_tx;
+	struct delayed_work beacon_tx;
+	u32 beacon_timeout_ports;
+	struct work_struct beacon_rx_timeout;
+	struct dlr_work_info work_info;
+
+	struct ksz_dlr_tx_frame frame;
+	struct ksz_dlr_update_frame update_frame;
+	u8 signon_frame[2000];
+	u8 signon_addr[ETH_ALEN];
+	struct ksz_dlr_node_info nodes[500];
+	u8 *tx_frame;
+	int signon_port;
+	int len;
+	int cur_state;
+	int state;
+	int rx_port;
+	int tx_port;
+	int active_port;
+	u32 seqid;
+	u32 seqid_announce;
+	u32 seqid_beacon;
+	u32 seqid_signon;
+	u32 seqid_chk[2];
+	u32 seqid_first[2];
+	u32 seqid_last[2];
+	u32 seqid_rcv[2];
+	u8 port_chk[2];
+	u8 port_rcv[2];
+	u32 seqid_accept[2];
+	u8 ports[2];
+	u16 member;
+
+	struct ksz_dlr_super_info supers[DLR_SUPERVISOR_NUM];
+	struct ksz_dlr_super_info *rogue_super;
+
+	int seqid_cnt;
+	int signon_space;
+	u8 ignore_req;
+	u8 req_cnt[2];
+	u8 link_break;
+
+	uint overrides;
+
+	const struct dlr_ops *ops;
+};
+
+struct dlr_attributes {
+	int info;
+	int node;
+	int prec;
+	int interval;
+	int timeout;
+	int vid;
+	int state;
+	int port;
+
+	int test;
+	int req;
+	int resp;
+	int link;
+	int learn;
+	int fault;
+	int signon;
+	int clear_rapid;
+	int clear_partial;
+
+	int all;
+	int rev;
+	int topology;
+	int network;
+	int super;
+	int super_cfg;
+	int fault_cnt;
+	int last_active_1;
+	int last_active_2;
+	int part_cnt;
+	int part_list;
+	int active_super_addr;
+	int active_super_prec;
+	int cap;
+	int port_1;
+	int port_2;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_dlr_api.h b/drivers/net/ethernet/micrel/ksz_dlr_api.h
new file mode 100644
index 0000000..df53988
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_dlr_api.h
@@ -0,0 +1,192 @@
+/**
+ * Micrel DLR driver API header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_DLR_API_H
+#define KSZ_DLR_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN				6
+#endif
+
+
+#define STATUS_INVALID_ATTRIB_VALUE		0x09
+#define STATUS_OBJECT_STATE_CONFLICT		0x0C
+#define STATUS_REPLY_DATA_TOO_LARGE		0x11
+
+#define SVC_GET_ATTRIBUTES_ALL			0x01
+#define SVC_GET_ATTRIBUTE_SINGLE		0x0E
+#define SVC_SET_ATTRIBUTE_SINGLE		0x10
+#define SVC_GET_MEMBER				0x18
+
+
+#define CIP_CLASS_ATTRIBUTES			0
+#define CIP_INSTANCE_ATTRIBUTES			1
+
+
+#define CLASS_DLR_OBJECT			0x47
+
+
+#define CIP_SVC_S				24
+#define CIP_CLASS_S				16
+#define CIP_ATTR_S				8
+
+
+#define DLR_GET_REVISION			1
+
+#define DLR_REVISION				3
+
+
+#define DLR_GET_NETWORK_TOPOLOGY		1
+#define DLR_GET_NETWORK_STATUS			2
+#define DLR_GET_RING_SUPERVISOR_STATUS		3
+#define DLR_SET_RING_SUPERVISOR_CONFIG		4
+#define DLR_SET_RING_FAULT_COUNT		5
+#define DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1	6
+#define DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2	7
+#define DLR_GET_RING_PARTICIPANTS_COUNT		8
+#define DLR_GET_RING_PARTICIPANTS_LIST		9
+#define DLR_GET_ACTIVE_SUPERVISOR_ADDRESS	10
+#define DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE	11
+#define DLR_GET_CAPABILITY_FLAGS		12
+#define DLR_SET_REDUNDANT_GATEWAY_CONFIG	13
+#define DLR_GET_REDUNDANT_GATEWAY_STATUS	14
+#define DLR_GET_ACTIVE_GATEWAY_ADDRESS		15
+#define DLR_GET_ACTIVE_GATEWAY_PRECEDENCE	16
+
+#define SVC_DLR_VERIFY_FAULT_LOCATION		0x4B
+#define SVC_DLR_CLEAR_RAPID_FAULTS		0x4C
+#define SVC_DLR_RESTART_SIGN_ON			0x4D
+#define SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT	0x4E
+
+
+#define DLR_TOPOLOGY_LINEAR			0
+#define DLR_TOPOLOGY_RING			1
+
+#define DLR_NET_NORMAL				0
+#define DLR_NET_RING_FAULT			1
+#define DLR_NET_UNEXPECTED_LOOP_DETECTED	2
+#define DLR_NET_PARTIAL_FAULT			3
+#define DLR_NET_RAPID_FAULT			4
+
+#define DLR_STAT_BACKUP_SUPERVISOR		0
+#define DLR_STAT_ACTIVE_SUPERVISOR		1
+#define DLR_STAT_RING_NODE			2
+#define DLR_STAT_NO_SUPERVISOR			3
+#define DLR_STAT_NODE_NOT_SUPPORTED		4
+
+#define DLR_CAP_ANNOUNCE_BASED			(1 << 0)
+#define DLR_CAP_BEACON_BASED			(1 << 1)
+#define DLR_CAP_SUPERVISOR_CAPABLE		(1 << 5)
+#define DLR_CAP_GATEWAY_CAPABLE			(1 << 6)
+#define DLR_CAP_FLUSH_TABLE_CAPABLE		(1 << 7)
+
+#define DLR_STAT_NON_GATEWAY			0
+#define DLR_STAT_BACKUP_GATEWAY			1
+#define DLR_STAT_ACTIVE_GATEWAY			2
+#define DLR_STAT_GATEWAY_FAULT_TO_UPLINK	3
+#define DLR_STAT_GATEWAY_NOT_SUPPORTED		4
+#define DLR_STAT_GATEWAY_FAULT_TO_NETWORK	5
+
+
+struct ksz_dlr_active_node {
+	u32 ip_addr;
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct ksz_dlr_super_cfg {
+	u8 enable;
+	u8 prec;
+	u32 beacon_interval;
+	u32 beacon_timeout;
+	u16 vid;
+}  __packed;
+
+struct ksz_dlr_gateway_cfg {
+	u8 enable;
+	u8 prec;
+	u32 advertise_interval;
+	u32 advertise_timeout;
+	u8 learning_enable;
+}  __packed;
+
+struct ksz_dlr_non_super_capable_1 {
+	u8 net_topology;
+	u8 net_status;
+} __packed;
+
+struct ksz_dlr_super_capable_1 {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+} __packed;
+
+struct ksz_dlr_non_super_capable_2 {
+	u8 net_topology;
+	u8 net_status;
+	struct ksz_dlr_active_node active_super_addr;
+	u32 cap;
+} __packed;
+
+struct ksz_dlr_super_capable_2 {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+	u32 cap;
+} __packed;
+
+struct ksz_dlr_gateway_capable {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+	u32 cap;
+	struct ksz_dlr_gateway_cfg gateway_cfg;
+	u8 gateway_status;
+	struct ksz_dlr_active_node active_gateway_addr;
+	u8 active_gateway_prec;
+} __packed;
+
+union dlr_data {
+	struct ksz_dlr_gateway_capable gateway;
+	struct ksz_dlr_super_capable_2 super;
+	struct ksz_dlr_non_super_capable_2 non_super;
+	struct ksz_dlr_super_cfg super_cfg;
+	struct ksz_dlr_gateway_cfg gateway_cfg;
+	struct ksz_dlr_active_node active;
+	u32 dword;
+	u16 word;
+	u8 byte;
+} __packed;
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c b/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c
new file mode 100644
index 0000000..78bd296
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c
@@ -0,0 +1,184 @@
+/**
+ * Micrel DLR common sysfs code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static ssize_t dlr_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ksz_dlr_info *dlr;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	dlr = &sw->info->dlr;
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = dlr->ops->sysfs_read(dlr, proc_num, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t dlr_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ksz_dlr_info *dlr;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	dlr = &sw->info->dlr;
+	proc_num = offset / sizeof(int);
+	ret = count;
+	dlr->ops->sysfs_write(dlr, proc_num, num, buf);
+	up(proc_sem);
+	return ret;
+}
+
+#define DLR_ATTR(_name, _mode, _show, _store) \
+struct device_attribute dlr_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define DLR_RD_ENTRY(name)						\
+static ssize_t show_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return dlr_show(d, attr, buf,					\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static DLR_ATTR(name, S_IRUGO, show_dlr_##name, NULL)
+
+/* generate a write-able attribute */
+#define DLR_WR_ENTRY(name)						\
+static ssize_t show_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return dlr_show(d, attr, buf,					\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static ssize_t store_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return dlr_store(d, attr, buf, count,				\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static DLR_ATTR(name, S_IRUGO | S_IWUSR, show_dlr_##name, store_dlr_##name)
+
+DLR_RD_ENTRY(info);
+DLR_WR_ENTRY(node);
+DLR_WR_ENTRY(prec);
+DLR_WR_ENTRY(interval);
+DLR_WR_ENTRY(timeout);
+DLR_WR_ENTRY(vid);
+DLR_WR_ENTRY(state);
+DLR_WR_ENTRY(port);
+DLR_WR_ENTRY(test);
+DLR_WR_ENTRY(req);
+DLR_WR_ENTRY(resp);
+DLR_WR_ENTRY(link);
+DLR_WR_ENTRY(learn);
+DLR_WR_ENTRY(fault);
+DLR_WR_ENTRY(signon);
+DLR_WR_ENTRY(clear_rapid);
+DLR_WR_ENTRY(clear_partial);
+
+DLR_RD_ENTRY(all);
+DLR_RD_ENTRY(rev);
+DLR_RD_ENTRY(topology);
+DLR_RD_ENTRY(network);
+DLR_RD_ENTRY(super);
+DLR_WR_ENTRY(super_cfg);
+DLR_WR_ENTRY(fault_cnt);
+DLR_RD_ENTRY(last_active_1);
+DLR_RD_ENTRY(last_active_2);
+DLR_RD_ENTRY(part_cnt);
+DLR_RD_ENTRY(part_list);
+DLR_RD_ENTRY(active_super_addr);
+DLR_RD_ENTRY(active_super_prec);
+DLR_RD_ENTRY(cap);
+DLR_WR_ENTRY(port_1);
+DLR_WR_ENTRY(port_2);
+
+static struct attribute *dlr_attrs[] = {
+	&dlr_attr_info.attr,
+	&dlr_attr_node.attr,
+	&dlr_attr_prec.attr,
+	&dlr_attr_interval.attr,
+	&dlr_attr_timeout.attr,
+	&dlr_attr_vid.attr,
+	&dlr_attr_state.attr,
+	&dlr_attr_port.attr,
+	&dlr_attr_test.attr,
+	&dlr_attr_req.attr,
+	&dlr_attr_resp.attr,
+	&dlr_attr_link.attr,
+	&dlr_attr_learn.attr,
+	&dlr_attr_fault.attr,
+	&dlr_attr_signon.attr,
+	&dlr_attr_clear_rapid.attr,
+	&dlr_attr_clear_partial.attr,
+
+	&dlr_attr_all.attr,
+	&dlr_attr_rev.attr,
+	&dlr_attr_topology.attr,
+	&dlr_attr_network.attr,
+	&dlr_attr_super.attr,
+	&dlr_attr_super_cfg.attr,
+	&dlr_attr_fault_cnt.attr,
+	&dlr_attr_last_active_1.attr,
+	&dlr_attr_last_active_2.attr,
+	&dlr_attr_part_cnt.attr,
+	&dlr_attr_part_list.attr,
+	&dlr_attr_active_super_addr.attr,
+	&dlr_attr_active_super_prec.attr,
+	&dlr_attr_cap.attr,
+	&dlr_attr_port_1.attr,
+	&dlr_attr_port_2.attr,
+	NULL
+};
+
+static struct attribute_group dlr_group = {
+	.name  = "dlrfs",
+	.attrs  = dlr_attrs,
+};
+
+static void exit_dlr_sysfs(struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, &dlr_group);
+}
+
+static int init_dlr_sysfs(struct device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->kobj, &dlr_group);
+	if (err)
+		return err;
+	return err;
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz_dsa.c b/drivers/net/ethernet/micrel/ksz_dsa.c
new file mode 100644
index 0000000..13ad519
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_dsa.c
@@ -0,0 +1,322 @@
+/**
+ * Micrel tail tagging switch DSA driver
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ * Copyright (c) 2008-2009 Marvell Semiconductor
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/phy.h>
+#include <net/dsa.h>
+
+static int ksz_dsa_reg_read(struct dsa_switch *ds, int addr, int reg)
+{
+	++addr;
+	return mdiobus_read(ds->master_mii_bus, ds->pd->sw_addr + addr, reg);
+}
+
+static int ksz_dsa_reg_write(struct dsa_switch *ds, int addr, int reg, u16 val)
+{
+	++addr;
+	return mdiobus_write(ds->master_mii_bus, ds->pd->sw_addr + addr,
+			     reg, val);
+}
+
+#define FAMILY_ID_87			0x87
+#define CHIP_ID_8794			0x60
+#define CHIP_ID_8795			0x90
+
+#define FAMILY_ID_88			0x88
+#define CHIP_ID_8863_MLI		0x30
+
+#define FAMILY_ID_84			0x84
+#define CHIP_ID_8463_MLI		0x40
+#define CHIP_ID_8463_RLI		0x50
+
+#define FAMILY_ID_85			0x85
+#define FAMILY_ID_95			0x95
+#define CHIP_ID_9567_RNX		0x67
+#define CHIP_ID_9566_RNX		0x66
+
+#define FAMILY_ID_88			0x88
+#define FAMILY_ID_98			0x98
+#define CHIP_ID_9893_RNX		0x93
+
+static char *ksz_dsa_probe(struct mii_bus *bus, int sw_addr)
+{
+	struct phy_device *phydev = bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+	u8 id1;
+	u8 id2;
+	int id;
+	static char switch_name[80];
+
+	sw->ops->acquire(sw);
+	id = sw->ops->get_id(sw, &id1, &id2);
+	sw->ops->release(sw);
+	strncpy(switch_name, "Micrel KSZ", sizeof(switch_name));
+	switch (id1) {
+	case FAMILY_ID_87:
+		strcat(switch_name, "87");
+		switch (id2) {
+		case CHIP_ID_8794:
+			strcat(switch_name, "94CNX");
+			break;
+		case CHIP_ID_8795:
+			strcat(switch_name, "95CLX");
+			break;
+		}
+		break;
+	case FAMILY_ID_88:
+		strcat(switch_name, "88");
+		switch (id2) {
+		case CHIP_ID_8863_MLI:
+			strcat(switch_name, "63MLI");
+			break;
+#if 0
+		case CHIP_ID_8873_MLI:
+			strcat(switch_name, "73MLI");
+			break;
+#endif
+		}
+		break;
+	case FAMILY_ID_84:
+		strcat(switch_name, "84");
+		switch (id2) {
+		case CHIP_ID_8463_MLI:
+			strcat(switch_name, "63MLI");
+			break;
+		case CHIP_ID_8463_RLI:
+			strcat(switch_name, "63RLI");
+			break;
+		}
+		break;
+	case FAMILY_ID_95:
+		strcat(switch_name, "95");
+		switch (id2) {
+		case CHIP_ID_9567_RNX:
+			strcat(switch_name, "67RNX");
+			break;
+		case CHIP_ID_9566_RNX:
+			strcat(switch_name, "66RNX");
+			break;
+		case CHIP_ID_9893_RNX:
+			strcat(switch_name, "63RNX");
+			break;
+		}
+		break;
+	case FAMILY_ID_98:
+	case 0x64:
+		strcat(switch_name, "98");
+		switch (id2) {
+		case CHIP_ID_9567_RNX:
+			strcat(switch_name, "97RNX");
+			break;
+		case CHIP_ID_9566_RNX:
+			strcat(switch_name, "96RNX");
+			break;
+		case CHIP_ID_9893_RNX:
+			strcat(switch_name, "93RNX");
+			break;
+		}
+		break;
+	}
+	if (!switch_name[10])
+		return NULL;
+	return switch_name;
+}
+
+static int ksz_dsa_switch_reset(struct dsa_switch *ds)
+{
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+
+	sw->ops->acquire(sw);
+	sw_reset(sw);
+	sw_init(sw);
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_setup_global(struct dsa_switch *ds)
+{
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+
+	sw->ops->acquire(sw);
+	sw->features |= DSA_SUPPORT;
+	if (!(sw->overrides & TAIL_TAGGING)) {
+		sw->ops->cfg_tail_tag(sw, 1);
+		sw->overrides |= TAIL_TAGGING;
+	}
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_setup_port(struct dsa_switch *ds, int p)
+{
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+
+	if (p == sw->port_cnt)
+		return 0;
+	sw->ops->acquire(sw);
+	sw->ops->cfg_each_port(sw, p, dsa_is_cpu_port(ds, p));
+	port_set_stp_state(sw, p, STP_STATE_SIMPLE);
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_setup(struct dsa_switch *ds)
+{
+	int i;
+	int ret;
+
+	ret = ksz_dsa_switch_reset(ds);
+	if (ret < 0)
+		return ret;
+
+	ret = ksz_dsa_setup_global(ds);
+	if (ret < 0)
+		return ret;
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++) {
+		ret = ksz_dsa_setup_port(ds, i);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int ksz_dsa_set_addr(struct dsa_switch *ds, u8 *addr)
+{
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+	int port;
+
+	sw->ops->acquire(sw);
+	sw_set_addr(sw, addr);
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		if (port == sw->port_cnt)
+			continue;
+		sw->ops->set_port_addr(sw, port, addr);
+	}
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_phy_read(struct dsa_switch *ds, int port, int regnum)
+{
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+	int addr;
+
+	addr = sw->ops->port_to_phy_addr(sw, port);
+	if (addr == -1)
+		return 0xffff;
+
+	return ksz_dsa_reg_read(ds, addr, regnum);
+}
+
+static int
+ksz_dsa_phy_write(struct dsa_switch *ds, int port, int regnum, u16 val)
+{
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+	int addr;
+
+	addr = sw->ops->port_to_phy_addr(sw, port);
+	if (addr == -1)
+		return 0xffff;
+
+	return ksz_dsa_reg_write(ds, addr, regnum, val);
+}
+
+static void ksz_dsa_poll_link(struct dsa_switch *ds)
+{
+	int i;
+	struct phy_device *phydev = ds->master_mii_bus->phy_map[0];
+	struct phy_priv *phydata = phydev->priv;
+	struct ksz_sw *sw = phydata->port.sw;
+	struct ksz_port_info *info;
+
+	for (i = 0; i < SWITCH_PORT_NUM; i++) {
+		struct net_device *dev;
+		int link;
+		int speed;
+		int duplex;
+		int fc;
+
+		dev = ds->ports[i];
+		if (dev == NULL)
+			continue;
+
+		info = &sw->port_info[i];
+		link = 0;
+		if (dev->flags & IFF_UP)
+			link = (info->state == media_connected);
+
+		if (!link) {
+			if (netif_carrier_ok(dev)) {
+				printk(KERN_INFO "%s: link down\n", dev->name);
+				netif_carrier_off(dev);
+			}
+			continue;
+		}
+
+		speed = info->tx_rate / TX_RATE_UNIT;
+		duplex = (info->duplex == 2);
+		fc = (info->flow_ctrl & 3) == 3;
+
+		if (!netif_carrier_ok(dev)) {
+			printk(KERN_INFO "%s: link up, %d Mb/s, %s duplex, "
+					 "flow control %sabled\n", dev->name,
+					 speed, duplex ? "full" : "half",
+					 fc ? "en" : "dis");
+			netif_carrier_on(dev);
+		}
+	}
+}
+
+static struct dsa_switch_driver micrel_switch_driver = {
+	.tag_protocol	= htons(ETH_P_TRAILER),
+	.probe		= ksz_dsa_probe,
+	.setup		= ksz_dsa_setup,
+	.set_addr	= ksz_dsa_set_addr,
+	.phy_read	= ksz_dsa_phy_read,
+	.phy_write	= ksz_dsa_phy_write,
+	.poll_link	= ksz_dsa_poll_link,
+};
+
+static int ksz_dsa_init(void)
+{
+	register_switch_driver(&micrel_switch_driver);
+	return 0;
+}
+
+static void ksz_dsa_cleanup(void)
+{
+	unregister_switch_driver(&micrel_switch_driver);
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz_iba.c b/drivers/net/ethernet/micrel/ksz_iba.c
new file mode 100644
index 0000000..d147283
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_iba.c
@@ -0,0 +1,2761 @@
+/**
+ * Micrel IBA code
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#define ETH_P_IBA			IBA_TAG_TYPE
+
+static void prepare_iba(struct ksz_iba_info *iba, u8 *dst, u8 *src)
+{
+#ifndef CAPTURE_IBA
+	if (iba->dst != dst)
+		memcpy(iba->dst, dst, ETH_ALEN);
+	if (iba->src != src)
+		memcpy(iba->src, src, ETH_ALEN);
+#endif
+	memcpy(iba->packet, iba->dst, ETH_ALEN);
+	memcpy(&iba->packet[ETH_ALEN], iba->src, ETH_ALEN);
+
+	iba->frame->tag.type = htons(iba->tag_type);
+	iba->frame->tag.prio = 0;
+	iba->frame->tag.cfi = 0;
+	iba->frame->tag.mode = 1;
+	iba->frame->format.format = htons(IBA_FORMAT_KSZ98XX);
+	iba->frame->format.reserved = 0;
+
+	iba->cmds[0].cmd = 0;
+}  /* prepare_iba */
+
+static void *iba_command(void *frame, int *size, u32 cmd, int cnt, u32 *data)
+{
+	struct iba_cmd *iba = frame;
+	int i;
+	int len = 4;
+	int final_len = *size + sizeof(u32) * cnt + 8;
+
+	if (final_len > IBA_LEN_MAX && cmd) {
+		cnt = (IBA_LEN_MAX - *size - 8) / (int) sizeof(u32);
+		if (cnt > 0)
+			data[0] = cnt - 1;
+		else
+			cmd = 0;
+dbg_msg(" command: %d\n", cnt);
+	}
+
+	iba->cmd = htonl(cmd);
+	for (i = 0; i < cnt; i++) {
+		iba->data[i] = htonl(data[i]);
+		len += 4;
+	}
+	frame = &iba->data[i];
+	*size += len;
+	return frame;
+}  /* iba_command */
+
+static unsigned long last_iba_jiffies;
+static int dbg_iba;
+static int last_ok_iba;
+static int last_ok_reg;
+
+static void *iba_pre_cmd(struct ksz_iba_info *info, u16 code)
+{
+	struct iba_frame *iba = info->frame;
+
+if (info->respid != info->seqid) {
+dbg_msg(" pre %x %x; %x; %x %x\n", info->respid, info->seqid, code,
+last_iba_jiffies, jiffies);
+dbg_iba = 1;
+}
+last_iba_jiffies = jiffies;
+	info->index = 0;
+	info->len = sizeof(struct iba_frame) - sizeof(struct iba_cmd) +
+		ETH_ALEN * 2;
+	iba->code = htons(code);
+	return &iba->cmd;
+}  /* iba_pre_cmd */
+
+static u32 iba_get_val(u32 size, u32 val)
+{
+	int shift;
+
+	switch (size) {
+	case IBA_CMD_32:
+		break;
+	case IBA_CMD_16:
+		val >>= 16;
+		break;
+	case IBA_CMD_16_M:
+		val >>= 8;
+		val &= 0xffff;
+		break;
+	case IBA_CMD_16_H:
+		val &= 0xffff;
+		break;
+	case IBA_CMD_24:
+		val >>= 8;
+		break;
+	case IBA_CMD_24_H:
+		val &= 0xffffff;
+		break;
+	default:
+		switch (size) {
+		case IBA_CMD_BYTE_0:
+			shift = 3;
+			break;
+		case IBA_CMD_BYTE_1:
+			shift = 2;
+			break;
+		case IBA_CMD_BYTE_2:
+			shift = 1;
+			break;
+		default:
+			shift = 0;
+			break;
+		}
+		val >>= shift * 8;
+		val &= 0xff;
+	}
+	return val;
+}  /* iba_get_val */
+
+static u32 iba_set_size(u32 addr, u32 size)
+{
+	switch (size) {
+	case IBA_CMD_8:
+		size = IBA_CMD_8 >> (addr & 3);
+		break;
+	case IBA_CMD_16:
+		if (addr & 2)
+			size = IBA_CMD_16 >> 2;
+		else if (addr & 1)
+			size = IBA_CMD_16 >> 1;
+		break;
+	case IBA_CMD_24:
+		size = IBA_CMD_24 >> (addr & 1);
+		break;
+	}
+	return size;
+}  /* iba_set_size */
+
+static u32 iba_set_val(u32 size, u32 addr, u32 val)
+{
+	switch (size) {
+	case IBA_CMD_8:
+		val &= 0xff;
+		val <<= (3 - (addr & 3)) * 8;
+		break;
+	case IBA_CMD_16:
+		val &= 0xffff;
+		if (!(addr & 2)) {
+			if (addr & 1)
+				val <<= 1 * 8;
+			else
+				val <<= 2 * 8;
+		}
+		break;
+	case IBA_CMD_24:
+		val &= 0xffffff;
+		val <<= (1 - (addr & 1)) * 8;
+		break;
+	}
+	return val;
+}  /* iba_set_val */
+
+/**
+ * iba_chk_regs - IBA register check
+ * @sw:		The switch instance.
+ * @cmds:	The IBA command.
+ * @data:	The IBA data.
+ *
+ * This routine checks the value written to specific registers to determine
+ * the correct tail tag length to use.
+ */
+static void iba_chk_regs(struct ksz_sw *sw, u32 cmds, u32 data)
+{
+	u32 port_reg;
+	u32 reg = cmds & IBA_CMD_ADDR_M;
+	u32 size = cmds & IBA_CMD_32;
+	u32 val = iba_get_val(size, data);
+	int need_ptp_tag = 0;
+	int need_tail_tag = 0;
+
+	if (IBA_CMD_BYTE_1 == size && (REG_PTP_MSG_CONF1 + 1) == reg) {
+		need_ptp_tag++;
+		if (val & PTP_ENABLE)
+			need_ptp_tag++;
+	} else if (IBA_CMD_16 == size && REG_PTP_MSG_CONF1 == reg) {
+		need_ptp_tag++;
+		if (val & PTP_ENABLE)
+			need_ptp_tag++;
+	} else if (IBA_CMD_32 == size && REG_PTP_MSG_CONF1 == reg) {
+		need_ptp_tag++;
+		if (val & (PTP_ENABLE << 16))
+			need_ptp_tag++;
+	}
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+	if (!(sw->features & NEW_CAP) &&
+	    sw->TAIL_TAG_LOOKUP >= 0x100 && 1 == need_ptp_tag)
+		need_ptp_tag = 2;
+#endif
+	if (2 == need_ptp_tag)
+		sw->overrides |= PTP_TAG;
+	else if (1 == need_ptp_tag)
+		sw->overrides &= ~PTP_TAG;
+
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if (IBA_CMD_BYTE_0 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & PORT_TAIL_TAG_ENABLE)
+			need_tail_tag++;
+	} else if (IBA_CMD_16 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & (PORT_TAIL_TAG_ENABLE << 8))
+			need_tail_tag++;
+	} else if (IBA_CMD_32 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & (PORT_TAIL_TAG_ENABLE << 24))
+			need_tail_tag++;
+	}
+	if (2 == need_tail_tag)
+		sw->overrides |= TAIL_TAGGING;
+	else if (1 == need_tail_tag)
+		sw->overrides &= ~TAIL_TAGGING;
+}  /* iba_chk_regs */
+
+static void *iba_cmd_data(struct ksz_iba_info *info, u32 cmd, u32 size,
+	u32 addr)
+{
+	int cnt = 1;
+	int shift = IBA_CMD_S;
+	struct iba_frame *iba = info->frame;
+
+	if (iba->code == htons(IBA_CODE_BURST)) {
+		cnt = info->data[0] + 1;
+		shift = IBA_BURST_S;
+	} else {
+		if (IBA_CMD_16 == size && 3 == (addr & 3))
+			pr_info("16-bit used with register ended with 3\n");
+
+		/* write can be 8-bit, 16-bit, or 32-bit. */
+		if (IBA_CMD_WRITE == cmd) {
+			if (REG_SW_IBA__4 <= addr && addr < REG_SW_IBA__4 + 4) {
+				info->data[0] = info->cfg;
+				addr = REG_SW_IBA__4;
+				size = IBA_CMD_32;
+			}
+			info->data[0] = iba_set_val(size, addr, info->data[0]);
+		}
+		size = iba_set_size(addr, size);
+	}
+	cmd <<= shift;
+	cmd |= size;
+	cmd |= addr & IBA_CMD_ADDR_M;
+	info->cmds[info->index].data[0] = info->data[0];
+	info->cmds[info->index++].cmd = cmd;
+	info->fptr = iba_command(info->fptr, &info->len, cmd, cnt, info->data);
+	if (info->len + 4 >= IBA_LEN_MAX && iba->code == htons(IBA_CODE_BURST))
+		info->cmds[info->index - 1].data[0] = info->data[0];
+	return info->fptr;
+}  /* iba_cmd_data */
+
+static void *iba_post_cmd(struct ksz_iba_info *info)
+{
+	struct iba_frame *iba = info->frame;
+
+if (info->respid != info->seqid) {
+dbg_msg(" post %x %x\n", info->respid, info->seqid);
+dbg_iba = 1;
+}
+	info->cmds[info->index++].cmd = 0;
+	info->fptr = iba_command(info->fptr, &info->len, 0, 0, NULL);
+	iba->tag.seqid = ++info->seqid;
+	iba->length = htons(info->len);
+	return info->fptr;
+}  /* iba_post_cmd */
+
+/**
+ * iba_xmit - Transmit IBA request.
+ * @info:	The IBA instance.
+ *
+ * This function prepares IBA request for transmit.
+ */
+static int iba_xmit(struct ksz_iba_info *info)
+{
+	int rc;
+	struct sk_buff *skb;
+	int len = ntohs(info->frame->length);
+	const struct net_device_ops *ops = info->dev->netdev_ops;
+
+	if (len < 60) {
+		memset(&info->packet[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, info->packet, len);
+	skb_put(skb, len);
+	skb->protocol = htons(ETH_P_IBA);
+	skb->dev = info->dev;
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(info->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* iba_xmit */
+
+#ifdef VERIFY_IBA
+static void prepare_cmd(struct ksz_iba_info *info, int message)
+{
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	switch (message) {
+	case 0:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, 0);
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, 4);
+		break;
+	case 1:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_UNIT_INDEX__4);
+		info->data[0] = TS_RESET;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_RESET;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_ENABLE;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_ENABLE;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 2:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0xB;
+		info->data[0] <<= MIB_COUNTER_INDEX_S;
+		info->data[0] |= MIB_COUNTER_READ;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_CTRL_STAT__4));
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_DATA));
+		break;
+	case 3:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_BURST);
+		info->data[0] = 2;
+		info->data[1] = 0x12340000;
+		info->data[2] = 0x56780000;
+		info->fptr = iba_cmd_data(info, IBA_BURST_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 4;
+		info->data[1] = 0x12340000;
+		info->data[2] = 0x56780000;
+		info->data[3] = 0x00001234;
+		info->data[4] = 0x00005678;
+		info->fptr = iba_cmd_data(info, IBA_BURST_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 60;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		if (info->len + 4 >= IBA_LEN_MAX)
+			break;
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 4:
+#if 1
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#else
+		info->fptr = iba_pre_cmd(info, 4);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#if 0
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#if 0
+		info->fptr = iba_cmd_data(info, 3,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#endif
+		break;
+	case 5:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_BURST);
+		info->data[0] = 63;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 6:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 1
+		info->data[0] = 0x1;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0x00800000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			0x00);
+		info->data[0] = 0x10000000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0x10000000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x8;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			0x304);
+		break;
+	case 7:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 0
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		break;
+	case 8:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 0
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		break;
+	default:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+	}
+	info->fptr = iba_post_cmd(info);
+}
+
+static int dbg_iba_test;
+static int iba_test(struct ksz_iba_info *info, int n)
+{
+	int k;
+	int rc;
+	unsigned long wait;
+
+	prepare_cmd(info, n);
+	info->regs[0].cmd = (u32) -1;
+	INIT_COMPLETION(info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		dbg_msg("send err: %d\n", rc);
+		return rc;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	dbg_msg("wait: %lx\n", wait);
+
+	k = 0;
+	while (info->regs[k].cmd != (u32) -1) {
+		dbg_msg("%08x=%08x\n", info->regs[k].cmd,
+			info->regs[k].data[0]);
+		k++;
+	}
+#ifndef TEST_IBA
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+		u32 status;
+
+if (mutex_is_locked(sw->reglock))
+printk(" reg locked\n");
+		mutex_lock(sw->reglock);
+		info->use_iba = false;
+		status = sw_r32(sw, REG_SW_IBA_STATUS__4);
+		dbg_msg("status %08x q:%d p:%d d:%d f:%d o:%d m:%d\n", status,
+			!!(status & SW_IBA_REQ),
+			!!(status & SW_IBA_RESP),
+			!!(status & SW_IBA_DA_MISMATCH),
+			!!(status & SW_IBA_FMT_MISMATCH),
+			!!(status & SW_IBA_CODE_ERROR),
+			!!(status & SW_IBA_CMD_ERROR));
+		status = sw_r32(sw, REG_SW_IBA_STATES__4);
+		dbg_msg("states %08x %u\n", status,
+			((status >> SW_IBA_PACKET_SIZE_S) &
+			SW_IBA_PACKET_SIZE_M) * 4);
+		status = sw_r32(sw, REG_SW_IBA_RESULT__4);
+		dbg_msg("result %08x %u\n", status, status >> SW_IBA_SIZE_S);
+		info->use_iba = true;
+		mutex_unlock(sw->reglock);
+	} while (0);
+#endif
+
+	return 0;
+}
+#endif
+
+static struct ksz_iba_info *iba_info;
+
+static void iba_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->hwlock);
+	++sw->info->iba.cnt;
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed\n", __func__);
+dbg_msg("  %s changed\n", __func__);
+		mutex_unlock(sw->hwlock);
+		sw->reg->lock(sw);
+	}
+}  /* iba_lock */
+
+static void iba_unlock(struct ksz_sw *sw)
+{
+	if (sw->info->iba.cnt)
+		--sw->info->iba.cnt;
+	else
+printk("wrong release\n");
+	mutex_unlock(sw->hwlock);
+}  /* iba_unlock */
+
+/**
+ * This helper function is used to prepare the read registers for use with
+ * the iba_r_post function.
+ */
+static u32 *iba_prepare_data(u32 reg, u32 *data)
+{
+	data[0] = reg;
+	if (-1 == reg)
+		return data + 1;
+	data[1] = 0xdeadfeed;
+	return data + 2;
+}  /* iba_prepare_data */
+
+/**
+ * This helper routine is used to check the allocated buffers for use with
+ * the iba_reqs function.
+ */
+static void assert_buf(const char *name, int i, size_t func_size, u32 *buf,
+	u32 *data, size_t buf_size)
+{
+	int assert = false;
+
+	if ((i + 1) > func_size / sizeof(void *)) {
+		printk(KERN_INFO "  [%s func] %u %u\n",
+			name, i, func_size / sizeof(void *));
+		assert = true;
+	}
+	if (data > buf + buf_size / sizeof(u32)) {
+		printk(KERN_INFO "  [%s data] %u\n",
+			name, (data - buf));
+		assert = true;
+	}
+	if (assert)
+		BUG();
+}  /* assert_buf */
+
+/**
+ * iba_r_pre - IBA register read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for register read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *iba_r_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, data[0], data[1]);
+	return info->fptr;
+}  /* iba_r_pre */
+
+/**
+ * iba_r_post - IBA register read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA register read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_r_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+if (info->seqid != info->respid)
+dbg_msg(" id %x %x\n", info->seqid, info->respid);
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+			int j = 0;
+
+			while (data[j] != -1) {
+				if (reg == data[j] &&
+				    (data[j + 1] & 0xffff0000) == 0xdead0000) {
+					data[j + 1] = iba_get_val(size,
+						info->regs[i].data[0]);
+					break;
+				}
+				j += 2;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* iba_r_post */
+
+/**
+ * iba_w_pre - IBA register write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for register write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *iba_w_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = data[2];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, data[0], data[1]);
+	return info->fptr;
+}  /* iba_w_pre */
+
+/**
+ * sw_setup_iba - Switch IBA setup
+ * @sw:		The switch instance.
+ *
+ * This routines setups IBA function of the switch.
+ */
+static void sw_setup_iba(struct ksz_sw *sw)
+{
+	u32 data;
+
+	data = sw_r32(sw, REG_SW_IBA__4);
+	sw->info->iba.tag_type = (data & SW_IBA_FRAME_TPID_M);
+	data &= ~(SW_IBA_PORT_M << SW_IBA_PORT_S);
+	data |= sw->HOST_PORT << SW_IBA_PORT_S;
+	data |= SW_IBA_ENABLE;
+	data |= SW_IBA_INIT;
+#if 0
+	data |= SW_IBA_DA_MATCH;
+#endif
+	sw_w32(sw, REG_SW_IBA__4, data);
+	sw->info->iba.cfg = data;
+	dbg_msg("status %08x\n", sw_r32(sw, REG_SW_IBA_STATUS__4));
+	dbg_msg("states %08x\n", sw_r32(sw, REG_SW_IBA_STATES__4));
+	dbg_msg("result %08x\n", sw_r32(sw, REG_SW_IBA_RESULT__4));
+}  /* sw_setup_iba */
+
+/**
+ * iba_to_spi - Switch IBA to SPI
+ * @sw:		The switch instance.
+ * @info:	The IBA instance.
+ *
+ * This routine switches from using IBA to using SPI.
+ */
+static void iba_to_spi(struct ksz_sw *sw, struct ksz_iba_info *info)
+{
+	/* Not calling from interrupt handling. */
+	if (sw->intr_using != 2)
+		mutex_lock(sw->reglock);
+	sw_set_spi(sw, info);
+	if (sw->intr_using != 2)
+		mutex_unlock(sw->hwlock);
+	printk(KERN_ALERT "revert to SPI\n");
+
+#if 1
+	sw_setup_iba(sw);
+	schedule_delayed_work(&sw->set_ops, 100);
+#endif
+}  /* iba_to_spi */
+
+static void iba_dbg_states(struct ksz_iba_info *info)
+{
+	int i;
+	u32 status;
+	struct ksz_sw *sw = info->sw_dev;
+	int iba_test_override = (sw->overrides & IBA_TEST);
+	int use_iba = info->use_iba;
+
+iba_test_override = 1;
+	if (!iba_test_override)
+		return;
+
+dbg_msg(" w seq: %x\n", info->seqid);
+if (sw->intr_using < 2 && mutex_is_locked(sw->reglock))
+printk(" reg locked: %d\n", sw->intr_using);
+	if (sw->intr_using < 2)
+		mutex_lock(sw->reglock);
+	info->use_iba = false;
+	status = sw_r32(sw, REG_SW_IBA_STATUS__4);
+	dbg_msg("status %08x q:%d p:%d d:%d f:%d o:%d m:%d\n", status,
+		!!(status & SW_IBA_REQ),
+		!!(status & SW_IBA_RESP),
+		!!(status & SW_IBA_DA_MISMATCH),
+		!!(status & SW_IBA_FMT_MISMATCH),
+		!!(status & SW_IBA_CODE_ERROR),
+		!!(status & SW_IBA_CMD_ERROR));
+	status = sw_r32(sw, REG_SW_IBA_STATES__4);
+	dbg_msg("states %08x %u\n", status, ((status >> SW_IBA_PACKET_SIZE_S) &
+		SW_IBA_PACKET_SIZE_M) * 4);
+	status = sw_r32(sw, REG_SW_IBA_RESULT__4);
+	dbg_msg("result %08x %u\n", status, status >> SW_IBA_SIZE_S);
+	info->use_iba = use_iba;
+	if (sw->intr_using < 2)
+		mutex_unlock(sw->reglock);
+
+	for (i = 0; i < ntohs(info->frame->length); i++) {
+		dbg_msg("%02x ", info->packet[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (i % 16)
+		dbg_msg("\n");
+}  /* iba_dbg_states */
+
+/**
+ * iba_reqs - IBA register request
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @func:	The pre-processing routines.
+ * @post:	The post-processing function.
+ *
+ * This function sends a request with many pre-processing routines to IBA and
+ * waits for a response.
+ *
+ */
+static int iba_reqs(struct ksz_iba_info *info, void **in, void *out, void *obj,
+	void **func,
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int rc;
+	unsigned long wait;
+	u16 code = IBA_CODE_NORMAL;
+	void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj);
+
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		if (!mutex_is_locked(sw->hwlock))
+			pr_alert("IBA not locked\n");
+	} while (0);
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	info->fptr = iba_pre_cmd(info, code);
+
+	do {
+		prepare = *func;
+		info->fptr = prepare(info, *in, obj);
+		++func;
+		++in;
+	} while (*func);
+
+	info->fptr = iba_post_cmd(info);
+	info->regs[0].cmd = (u32) -1;
+	INIT_COMPLETION(info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+printk("  !! %x\n", rc);
+		iba_dbg_states(info);
+
+		/* Not testing if IBA is okay. */
+		if (!(info->use_iba & 0x80))
+			iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	if (!wait) {
+if (dbg_iba)
+dbg_msg("  w timeout\n");
+		iba_dbg_states(info);
+
+		/* Not testing if IBA is okay. */
+		if (!(info->use_iba & 0x80))
+			iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+
+	rc = 1;
+	if (post)
+		rc = post(info, out, obj);
+	return rc * 4;
+}  /* iba_reqs */
+
+/**
+ * iba_req - IBA basic register request
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @prepare:	The pre-processing routine.
+ * @post:	The post-processing function.
+ *
+ * This function sends a request to IBA and waits for a response.
+ *
+ * Return number of bytes read.
+ */
+static int iba_req(struct ksz_iba_info *info, void *in, void *out, void *obj,
+	void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj),
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int rc;
+	void *func[2];
+	void *data_in[1];
+	int i = 0;
+
+	data_in[i] = in;
+	func[i++] = prepare;
+
+	func[i] = NULL;
+#if 0
+	rc = iba_reqs(info, data_in, NULL, obj, func, NULL);
+	if (!rc)
+		return 0;
+
+	rc = 1;
+	if (post)
+		rc = post(info, out, obj);
+	return rc * 4;
+#else
+	rc = iba_reqs(info, data_in, out, obj, func, post);
+	return rc;
+#endif
+}  /* iba_req */
+
+/**
+ * iba_r - IBA basic register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ * @size:	The data size.
+ *
+ * This function reads a register through IBA.
+ */
+static u32 iba_r(struct ksz_iba_info *info, unsigned reg, u32 size)
+{
+	u32 data[4];
+	int rc;
+	static int iba_r_enter;
+
+#if 1
+if (info->respid != info->seqid || iba_r_enter) {
+dbg_msg(" iba_r %x %x %d; %x %x; %d\n", info->respid, info->seqid, info->cnt, reg,
+last_ok_reg, iba_r_enter);
+}
+#endif
+	++iba_r_enter;
+	data[0] = size;
+	data[1] = reg;
+	data[2] = 0xdeadbeaf;
+	data[3] = -1;
+	rc = iba_req(info, data, data + 1, NULL, iba_r_pre, iba_r_post);
+	if (!rc)
+dbg_msg("r %x %x %08x\n", reg, size, data[2]);
+else if (dbg_iba)
+dbg_msg(" ?? %d %08x\n", rc, data[2]);
+	last_ok_reg = reg;
+	--iba_r_enter;
+	return data[2];
+}  /* iba_r */
+
+/**
+ * iba_r8 - IBA 8-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 8-bit register through IBA.
+ */
+static u8 iba_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return (u8) iba_r(&sw->info->iba, reg, IBA_CMD_8);
+}  /* iba_r8 */
+
+/**
+ * iba_r16 - IBA 16-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 16-bit register through IBA.
+ */
+static u16 iba_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return (u16) iba_r(&sw->info->iba, reg, IBA_CMD_16);
+}  /* iba_r16 */
+
+/**
+ * iba_r24 - IBA 24-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 24-bit register through IBA.
+ */
+static u32 iba_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return iba_r(&sw->info->iba, reg, IBA_CMD_24);
+}  /* iba_r24 */
+
+/**
+ * iba_r32 - IBA 32-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 32-bit register through IBA.
+ */
+static u32 iba_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return iba_r(&sw->info->iba, reg, IBA_CMD_32);
+}  /* iba_r32 */
+
+/**
+ * iba_w - IBA basic register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ * @size:	The data size.
+ *
+ * This function writes a register through IBA.
+ */
+static void iba_w(struct ksz_iba_info *info, unsigned reg, unsigned val,
+	u32 size)
+{
+	u32 data[3];
+	int rc;
+
+	data[0] = size;
+	data[1] = reg;
+	data[2] = val;
+	rc = iba_req(info, data, NULL, NULL, iba_w_pre, NULL);
+	if (!rc)
+dbg_msg("w %x %x\n", reg, size);
+}  /* iba_w */
+
+/**
+ * iba_w8 - IBA 8-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 8-bit register through IBA.
+ */
+static void iba_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_8);
+}  /* iba_w8 */
+
+/**
+ * iba_w16 - IBA 16-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 16-bit register through IBA.
+ */
+static void iba_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_16);
+}  /* iba_w16 */
+
+/**
+ * iba_w24 - IBA 24-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 24-bit register through IBA.
+ */
+static void iba_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_24);
+}  /* iba_w24 */
+
+/**
+ * iba_w32 - IBA 32-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 32-bit register through IBA.
+ */
+static void iba_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_32);
+}  /* iba_w32 */
+
+/**
+ * iba_get_pre - IBA burst read pre-processing
+ * @info:	The IBA instance.
+ * @cnt:	The buffer count.
+ * @buf:	The buffer.
+ *
+ * This routine prepares IBA for burst read operation.
+ */
+static void iba_get_pre(u32 *data, int cnt, char *buf)
+{}
+
+/**
+ * iba_get_post - IBA burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @b:		Endian indication.
+ *
+ * This helper function retrieves the result of IBA burst read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post(struct ksz_iba_info *info, void *out, void *obj, int b)
+{
+	u32 *ptr = (u32 *) out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		*ptr = iba_get_val((info->regs[i].cmd & IBA_CMD_32),
+			info->regs[i].data[0]);
+		if (b)
+			*ptr = cpu_to_be32(*ptr);
+		ptr++;
+		i++;
+	}
+	return i;
+}  /* iba_get_post */
+
+/**
+ * iba_get_post_be - IBA big-endian burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA big-endian burst read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post_be(struct ksz_iba_info *info, void *out, void *obj)
+{
+	return iba_get_post(info, out, obj, 1);
+}  /* iba_get_post_be */
+
+/**
+ * iba_get_post_le - IBA little-endian burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA little-endian burst read
+ * operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post_le(struct ksz_iba_info *info, void *out, void *obj)
+{
+	return iba_get_post(info, out, obj, 0);
+}  /* iba_get_post_le */
+
+/**
+ * iba_set_pre - IBA burst write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This routine prepares IBA for burst write operation.
+ */
+static void iba_set_pre(u32 *data, int cnt, char *buf)
+{
+	u32 *ptr = (u32 *) buf;
+	int i;
+
+	i = 0;
+	if (cnt > 1)
+		i = 1;
+	while (cnt > 0) {
+		data[i++] = *ptr++;
+		cnt--;
+	}
+}  /* iba_set_pre */
+
+/**
+ * iba_set_post - IBA burst write post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA burst write operation.
+ *
+ * Return number of registers written.
+ */
+static int iba_set_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		dbg_msg("%08x=%08x\n", info->regs[i].cmd,
+			info->regs[i].data[0]);
+		i++;
+	}
+	return i;
+}  /* iba_set_post */
+
+/**
+ * iba_burst - IBA burst request
+ * @info:	The IBA instance.
+ * @addr:	The starting address.
+ * @cnt:	The number of addresses.
+ * @buf:	Buffer holding the data.
+ * @write:	Write indication.
+ * @prepare:	The pre-processing routine.
+ * @post:	The post-processing function.
+ *
+ * This function sends a burst request to IBA and waits for a response.
+ *
+ * Return number of bytes read.
+ */
+static int iba_burst(struct ksz_iba_info *info, u32 addr, size_t cnt,
+	char *buf, int write, void (*prepare)(u32 *data, int cnt, char *buf),
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int mult;
+	int rc;
+	unsigned long wait;
+	u32 val;
+	u16 code = IBA_CODE_NORMAL;
+	u32 cmd = IBA_CMD_READ;
+	u32 size = IBA_CMD_32;
+	void *data = buf;
+
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	if (cnt > 4) {
+		mult = cnt / 4;
+		info->data[0] = mult;
+		code = IBA_CODE_BURST;
+		cmd = IBA_BURST_READ;
+		if contain_reg(addr, cnt, REG_SW_IBA__4) {
+			u32 *ptr = (u32 *) buf;
+			u32 loc = (REG_SW_IBA__4 - addr) / 4;
+
+			if (write)
+				ptr[loc] = info->cfg;
+		}
+	} else {
+		mult = 1;
+		if (1 == cnt) {
+#ifdef VERIFY_IBA
+if (!dbg_iba_test) {
+iba_test(info, 7);
+dbg_iba_test = 1;
+}
+#endif
+			if (write) {
+				u8 *ptr = data;
+
+				val = *ptr;
+				data = &val;
+			}
+			size = IBA_CMD_8;
+		} else if (2 == cnt) {
+			if (write) {
+				u16 *ptr = data;
+
+				val = *ptr;
+				data = &val;
+			}
+			size = IBA_CMD_16;
+		} else if (addr & 1)
+			size = IBA_CMD_8;
+		else if (addr & 2)
+			size = IBA_CMD_16;
+	}
+	cmd += write;
+	info->fptr = iba_pre_cmd(info, code);
+
+	prepare(info->data, mult, data);
+	info->fptr = iba_cmd_data(info, cmd, size, addr);
+
+	info->fptr = iba_post_cmd(info);
+	info->regs[0].cmd = (u32) -1;
+	INIT_COMPLETION(info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		iba_dbg_states(info);
+		iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	if (!wait) {
+dbg_msg("burst to\n");
+		iba_dbg_states(info);
+		iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+
+	rc = post(info, data, NULL);
+	rc *= 4;
+	return rc;
+}  /* iba_burst */
+
+static void iba_r_buf(struct ksz_sw *sw, unsigned reg, void *buf, size_t count)
+{
+	u8 *orig_buf = buf;
+	size_t orig_cnt = count;
+	int start = 0;
+
+	/* Not in multiple of 4. */
+	if ((count & 3) || (reg & 3)) {
+		orig_buf = buf;
+		orig_cnt = count;
+		start = reg & 3;
+		reg &= ~3;
+		buf = sw->info->iba.buf;
+		count += start;
+		count += 3;
+		count &= ~3;
+	}
+	iba_burst(&sw->info->iba, reg, count, buf, 0,
+		iba_get_pre, iba_get_post_be);
+	if (orig_buf != buf)
+		memcpy(orig_buf, &sw->info->iba.buf[start], orig_cnt);
+}  /* iba_r_buf */
+
+static u32 buf_to_val(u8 *buf, int i, int cnt)
+{
+	int j;
+	u32 val = buf[i];
+
+	for (j = 1; j < cnt; j++) {
+		val <<= 8;
+		val |= buf[i + j];
+	}
+	return val;
+}  /* buf_to_val */
+
+/**
+ * w_buf_pre - IBA buffer write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for buffer write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_buf_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 *buf = obj;
+	u16 reg = data[0];
+	size_t cnt = data[1];
+	int i;
+	u32 size;
+	u32 val;
+
+	/* Register may not be in 4-byte boundary. */
+	switch (reg & 3) {
+	case 1:
+		size = IBA_CMD_24;
+		i = 3;
+		break;
+	case 2:
+		size = IBA_CMD_16;
+		i = 2;
+		break;
+	case 3:
+		size = IBA_CMD_8;
+		i = 1;
+		break;
+	default:
+		size = IBA_CMD_32;
+		i = 4;
+		break;
+	}
+
+	/* Count may be too small. */
+	if (i > cnt) {
+		i = cnt;
+		switch (i) {
+		case 1:
+			size = IBA_CMD_8;
+			break;
+		case 2:
+			size = IBA_CMD_16;
+			break;
+		default:
+			size = IBA_CMD_24;
+			break;
+		}
+	}
+
+	/* Prepare the initial value. */
+	val = buf_to_val(buf, 0, i);
+
+	cnt -= i;
+	info->data[0] = val;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, size, reg);
+	reg &= ~3;
+	size = IBA_CMD_32;
+	while (cnt >= 4) {
+		val = buf_to_val(buf, i, 4);
+		reg += 4;
+		info->data[0] = val;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, size, reg);
+		i += 4;
+		cnt -= 4;
+	}
+	if (cnt) {
+		switch (cnt) {
+		case 1:
+			size = IBA_CMD_8;
+			break;
+		case 2:
+			size = IBA_CMD_16;
+			break;
+		default:
+			size = IBA_CMD_24;
+			break;
+		}
+		val = buf_to_val(buf, i, cnt);
+		reg += 4;
+		info->data[0] = val;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, size, reg);
+	}
+	return info->fptr;
+}  /* w_buf_pre */
+
+static void iba_w_buf(struct ksz_sw *sw, unsigned reg, void *buf, size_t count)
+{
+	/* Not in multiple of 4. */
+	if ((count & 3) || (reg & 3)) {
+		u32 data[3];
+
+		data[0] = reg;
+		data[1] = count;
+		iba_req(&sw->info->iba, data, NULL, buf, w_buf_pre,
+			iba_set_post);
+	} else {
+		int i;
+		u32 *src = buf;
+		u32 *dst = (u32 *) sw->info->iba.buf;
+
+		for (i = 0; i < count; i += 4)
+			*dst++ = be32_to_cpu(*src++);
+		buf = sw->info->iba.buf;
+		iba_burst(&sw->info->iba, reg, count, buf, 1,
+			iba_set_pre, iba_set_post);
+	}
+}  /* iba_w_buf */
+
+static int iba_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	int rc;
+
+	rc = iba_burst(&sw->info->iba, reg, count, buf, 0,
+		iba_get_pre, iba_get_post_le);
+
+	/*
+	 * Return zero to let the calling program know the boundary must be
+	 * 32-bit.
+	 */
+	if (4 == count && (reg & 3))
+		rc = 0;
+	return rc;
+}  /* iba_get */
+
+static int iba_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	return iba_burst(&sw->info->iba, reg, count, buf, 1,
+		iba_set_pre, iba_set_post);
+}  /* iba_set */
+
+/**
+ * r_mac_table_pre - IBA MAC table read pre-processing
+ * @info:	The IBA instance.
+ *
+ * This routine prepares IBA for MAC table read operation.
+ */
+static void r_mac_table_pre(struct ksz_iba_info *info)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_A);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_B);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_C);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_D);
+}  /* r_mac_table_pre */
+
+/**
+ * r_mac_table_post - IBA MAC table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA MAC table read operation.
+ *
+ * Return number of registers read.
+ */
+static int r_mac_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u16 *entry = out;
+	struct ksz_mac_table *mac = obj;
+	int i = 0;
+	u32 data[5];
+
+	memset(data, 0, sizeof(data));
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_SW_ALU_VAL_A:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_B:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_C:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_D:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				get_mac_table_info(mac, data);
+				++mac;
+				memset(data, 0, sizeof(data));
+				break;
+			case REG_SW_LUE_INDEX_0__2:
+				data[4] = iba_get_val(size,
+					info->regs[i].data[0]);
+				if (entry) {
+					*entry = (data[4] & ENTRY_INDEX_M) +
+						1;
+					++entry;
+				}
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_mac_table_post */
+
+/**
+ * w_mac_table_pre - IBA MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @mac:	The MAC table entries.
+ *
+ * This routine prepares IBA for MAC table write operation.
+ */
+static void w_mac_table_pre(struct ksz_iba_info *info,
+	struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	set_mac_table_info(mac, data);
+	info->data[0] = data[0];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_A);
+	info->data[0] = data[1];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_B);
+	info->data[0] = data[2];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_C);
+	info->data[0] = data[3];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_D);
+}  /* w_mac_table_pre */
+
+/**
+ * s_dyn_mac_pre - IBA dynamic MAC table set pre-processing
+ * @info:	The IBA instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ *
+ * This helper routine prepares IBA for dynamic MAC table set operation.
+ */
+static u32 s_dyn_mac_pre(struct ksz_iba_info *info, u16 addr, u8 *src_addr,
+	u16 src_fid)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & ALU_DIRECT_INDEX_M;
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_1);
+		ctrl |= ALU_DIRECT;
+	} else {
+		data = (u32) src_fid << ALU_FID_INDEX_S;
+		data |= ((u32) src_addr[0] << 8) | src_addr[1];
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_0);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_1);
+	}
+	return ctrl;
+}  /* s_dyn_mac_pre */
+
+/**
+ * r_dyn_mac_pre - IBA dynamic MAC table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[0];
+	u8 *src_addr = (u8 *) data[1];
+	u16 src_fid = data[2];
+	u32 ctrl;
+
+	ctrl = s_dyn_mac_pre(info, addr, src_addr, src_fid);
+	ctrl |= ALU_READ;
+	ctrl |= ALU_START;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	info->data[0] = ALU_START;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	r_mac_table_pre(info);
+
+	/* Hash read. */
+	if (!addr) {
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+			REG_SW_LUE_INDEX_0__2);
+	}
+	return info->fptr;
+}  /* r_dyn_mac_pre */
+
+/**
+ * iba_r_dyn_mac_hw - read from dynamic MAC table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This routine reads an entry of the dynamic MAC table using IBA.
+ */
+static void iba_r_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	u32 data[3];
+
+#if 0
+	mac->ignore_use_fid = 1;
+#endif
+if (!mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+	data[0] = addr;
+	data[1] = (u32) src_addr;
+	data[2] = src_fid;
+	iba_req(&sw->info->iba, data, entry, mac, r_dyn_mac_pre,
+		r_mac_table_post);
+}  /* iba_r_dyn_mac_hw */
+
+/**
+ * w_dyn_mac_pre - IBA dynamic MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ksz_mac_table *mac = obj;
+	u16 addr = data[0];
+	u8 *src_addr = (u8 *) data[1];
+	u16 src_fid = data[2];
+	u32 ctrl;
+
+#if 0
+	mac->ignore_use_fid = 1;
+#endif
+if (!mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+	ctrl = s_dyn_mac_pre(info, addr, src_addr, src_fid);
+	ctrl |= ALU_WRITE;
+	ctrl |= ALU_START;
+	w_mac_table_pre(info, mac);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	return info->fptr;
+}  /* w_dyn_mac_pre */
+
+/**
+ * iba_w_dyn_mac_hw - write to dynamic MAC table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the dynamic MAC table using IBA.
+ */
+static void iba_w_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	u32 data[3];
+
+	data[0] = addr;
+	data[1] = (u32) src_addr;
+	data[2] = src_fid;
+	iba_req(&sw->info->iba, data, NULL, mac, w_dyn_mac_pre, NULL);
+}  /* iba_w_dyn_mac_hw */
+
+/**
+ * start_dyn_mac_pre - IBA dynamic MAC table start pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table start operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *start_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = ALU_SEARCH;
+	ctrl |= ALU_START;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	return info->fptr;
+}  /* start_dyn_mac_pre */
+
+/**
+ * iba_start_dyn_mac_hw - start dynamic MAC table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This routine starts dynamic MAC table search using IBA.
+ */
+static void iba_start_dyn_mac_hw(struct ksz_sw *sw)
+{
+	iba_req(&sw->info->iba, NULL, NULL, NULL, start_dyn_mac_pre, NULL);
+}  /* iba_start_dyn_mac_hw */
+
+/**
+ * g_dyn_mac_pre - IBA dynamic MAC table retrieve pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table retrieve operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *g_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	r_mac_table_pre(info);
+	return info->fptr;
+}  /* g_dyn_mac_pre */
+
+/**
+ * iba_g_dyn_mac_hw - retrieve dynamic MAC table result using IBA
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This routine retrieves dynamic MAC table result using IBA.
+ */
+static void iba_g_dyn_mac_hw(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+#if 0
+	mac->ignore_use_fid = 1;
+#endif
+if (!mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+	iba_req(&sw->info->iba, NULL, NULL, mac, g_dyn_mac_pre,
+		r_mac_table_post);
+}  /* iba_g_dyn_mac_hw */
+
+/**
+ * stop_dyn_mac_pre - IBA dynamic MAC table stop pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table stop operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *stop_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	return info->fptr;
+}  /* stop_dyn_mac_pre */
+
+/**
+ * stop_dyn_mac_post - IBA dynamic MAC table stop post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA dynamic MAC table stop operation.
+ *
+ * Return number of registers read.
+ */
+static int stop_dyn_mac_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			if (reg == REG_SW_ALU_CTRL__4)
+				*data = iba_get_val(size,
+					info->regs[i].data[0]);
+		}
+		i++;
+	}
+	return i;
+}  /* stop_dyn_mac_post */
+
+/**
+ * iba_stop_dyn_mac_hw - stop dynamic MAC table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function stops dynamic MAC table search using IBA.
+ *
+ * Return the last MAC table control.
+ */
+static u32 iba_stop_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	iba_req(&sw->info->iba, NULL, &ctrl, NULL, stop_dyn_mac_pre,
+		stop_dyn_mac_post);
+	return ctrl;
+}  /* iba_stop_dyn_mac_hw */
+
+/**
+ * r_sta_mac_pre - IBA static MAC table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for static MAC table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_sta_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int cnt = 0;
+	int num = data[0];
+	u32 *ctrl = &data[1];
+
+	do {
+		info->data[0] = *ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		info->data[0] = ALU_STAT_START;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		r_mac_table_pre(info);
+		++cnt;
+		++ctrl;
+	} while (cnt < num);
+	return info->fptr;
+}  /* r_sta_mac_pre */
+
+/**
+ * iba_r_sta_mac_hw - read from static MAC table using IBA
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for read operation.
+ * @num:	Number of entries to read.
+ * @mac:	Buffer to hold the MAC table entries.
+ *
+ * This routine reads from static MAC table using IBA.
+ */
+static void iba_r_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 data[MAX_IBA_MAC_ENTRIES + 1];
+	int i;
+
+	if (num > MAX_IBA_MAC_ENTRIES)
+		num = MAX_IBA_MAC_ENTRIES;
+	for (i = 0; i < num; i++)
+#if 0
+		mac[i].ignore_use_fid = 0;
+#endif
+if (mac[i].ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+	data[0] = num;
+	memcpy(&data[1], ctrl, sizeof(u32) * num);
+	iba_req(&sw->info->iba, data, NULL, mac, r_sta_mac_pre,
+		r_mac_table_post);
+}  /* iba_r_sta_mac_hw */
+
+/**
+ * w_sta_mac_pre - IBA static MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for static MAC table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_sta_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int cnt;
+	int num = data[0];
+	struct ksz_mac_table *mac = obj;
+
+	for (cnt = 0; cnt < num; cnt++, data++) {
+#if 0
+		mac->ignore_use_fid = 0;
+#endif
+if (mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+		w_mac_table_pre(info, mac);
+		info->data[0] = data[1];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		info->data[0] = ALU_STAT_START;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		++mac;
+	}
+	return info->fptr;
+}  /* w_sta_mac_pre */
+
+/**
+ * iba_w_sta_mac_hw - write to static MAC table using IBA
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for write operation.
+ * @num:	Number of entries to write.
+ * @mac:	MAC table entries.
+ *
+ * This routine writes to static MAC table using IBA.
+ */
+static void iba_w_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 data[MAX_IBA_MAC_ENTRIES + 1];
+
+	if (num > MAX_IBA_MAC_ENTRIES)
+		num = MAX_IBA_MAC_ENTRIES;
+	data[0] = num;
+	memcpy(&data[1], ctrl, sizeof(u32) * num);
+	iba_req(&sw->info->iba, data, NULL, mac, w_sta_mac_pre, NULL);
+}  /* iba_w_sta_mac_hw */
+
+/**
+ * r_vlan_table_pre - IBA VLAN table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for VLAN table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_vlan_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[3];
+	u32 ctrl;
+	int cnt = 0;
+	int num = *((int *) obj);
+
+	if (num > MAX_IBA_VLAN_ENTRIES)
+		num = MAX_IBA_VLAN_ENTRIES;
+	do {
+		info->data[0] = addr;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_SW_VLAN_ENTRY_INDEX__2);
+		ctrl = VLAN_READ;
+		ctrl |= VLAN_START;
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+			REG_SW_VLAN_CTRL);
+		info->data[0] = VLAN_START << 24;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_VLAN_CTRL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_UNTAG__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_PORTS__4);
+		++cnt;
+		++addr;
+	} while (cnt < num);
+	return info->fptr;
+}  /* r_vlan_table_pre */
+
+/**
+ * r_vlan_table_post - IBA VLAN table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA VLAN table read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_vlan_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+	u32 *data = out;
+	int num = *((int *) obj);
+
+	memset(data, 0, sizeof(u32) * num * READ_VLAN_ENTRY_SIZE);
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_SW_VLAN_ENTRY__4:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_VLAN_ENTRY_UNTAG__4:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_VLAN_ENTRY_PORTS__4:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data += READ_VLAN_ENTRY_SIZE;
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_vlan_table_post */
+
+/**
+ * iba_r_vlan_hw - read from VLAN table using IBA
+ * @sw:		The switch instance.
+ * @data:	Buffer to hold the VLAN data.
+ * @num:	Number of entries to read.
+ *
+ * This routine reads from VLAN table using IBA.
+ */
+static void iba_r_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	iba_req(&sw->info->iba, data, data, &num, r_vlan_table_pre,
+		r_vlan_table_post);
+}  /* iba_r_vlan_hw */
+
+/**
+ * w_vlan_table_pre - IBA VLAN table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for VLAN table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_vlan_table_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[3];
+	u32 ctrl;
+	int cnt;
+	int num = *((int *) obj);
+
+	if (num > MAX_IBA_VLAN_ENTRIES)
+		num = MAX_IBA_VLAN_ENTRIES;
+	for (cnt = 0; cnt < num; cnt++) {
+		info->data[0] = data[0];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY__4);
+		info->data[0] = data[1];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_UNTAG__4);
+		info->data[0] = data[2];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_PORTS__4);
+		addr = data[3];
+		info->data[0] = addr;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_SW_VLAN_ENTRY_INDEX__2);
+		ctrl = VLAN_WRITE;
+		ctrl |= VLAN_START;
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+			REG_SW_VLAN_CTRL);
+		info->data[0] = VLAN_START << 24;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_VLAN_CTRL);
+		data += WRITE_VLAN_ENTRY_SIZE;
+	}
+	return info->fptr;
+}  /* w_vlan_table_pre */
+
+/**
+ * iba_w_vlan_hw - write to VLAN table using IBA
+ * @sw:		The switch instance.
+ * @data:	VLAN data to write.
+ * @num:	Number of entries to write.
+ *
+ * This routine writes to VLAN table using IBA.
+ */
+static void iba_w_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	iba_req(&sw->info->iba, data, NULL, &num, w_vlan_table_pre, NULL);
+}  /* iba_w_vlan_hw */
+
+/**
+ * r_mib_cnt_pre - IBA MIB counter read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for MIB counter read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_mib_cnt_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+	u32 *data = in;
+	int *port = obj;
+	int cnt;
+	int num = data[0];
+
+	for (cnt = 0; cnt < num; cnt++, data++) {
+		ctrl = data[1] & MIB_COUNTER_INDEX_M;
+		ctrl <<= MIB_COUNTER_INDEX_S;
+		ctrl |= MIB_COUNTER_READ;
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = MIB_COUNTER_VALID;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_MIB_DATA));
+	}
+	return info->fptr;
+}  /* r_mib_cnt_pre */
+
+/**
+ * r_mib_cnt_post - IBA MIB counter read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA MIB counter read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_mib_cnt_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 cmd;
+	int i = 0;
+	u32 *data = out;
+	int *port = obj;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		cmd = (info->regs[i].cmd >> IBA_CMD_S);
+		if (IBA_CMD_READ == cmd || IBA_CMD_WAIT_ON_1 == cmd) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+#if 1
+if (((reg >> 12) & 0xf) != *port + 1)
+dbg_msg(" ?? %s %x %x\n", __func__, reg, *port);
+#endif
+			reg &= ((1 << 12) - 1);
+			switch (reg) {
+			case REG_PORT_MIB_CTRL_STAT__4:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PORT_MIB_DATA:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data += READ_MIB_ENTRY_SIZE;
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_mib_cnt_post */
+
+/**
+ * iba_r_mib_cnt_hw - read MIB counters using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The addresses of the counters.
+ * @num:	Number of entries to read.
+ * @data:	Buffer to store the counters.
+ *
+ * This routine reads MIB counters of the port using IBA.
+ */
+static void iba_r_mib_cnt_hw(struct ksz_sw *sw, int port, u32 addr[], int num,
+	u32 data[])
+{
+	u32 data_in[MAX_IBA_MIB_ENTRIES + 1];
+
+	if (num > MAX_IBA_MIB_ENTRIES)
+		num = MAX_IBA_MIB_ENTRIES;
+	data_in[0] = num;
+	memcpy(&data_in[1], addr, sizeof(u32) * num);
+	memset(data, 0, sizeof(u32) * num * READ_MIB_ENTRY_SIZE);
+	iba_req(&sw->info->iba, data_in, data, &port, r_mib_cnt_pre,
+		r_mib_cnt_post);
+}  /* iba_r_mib_cnt_hw */
+
+#ifndef NO_ACL
+/**
+ * r_acl_table_pre - IBA ACL table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for ACL table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_acl_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int *port = obj;
+	u16 addr = data[4];
+	u32 ctrl = (addr & PORT_ACL_INDEX_M);
+	int i;
+
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0));
+	info->data[0] = PORT_ACL_READ_DONE << 8;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0 & ~3));
+	for (i = 0; i < 4; i++) {
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_ACL_0 + 4 * i));
+	}
+	return info->fptr;
+}  /* r_acl_table_pre */
+
+/**
+ * r_acl_table_post - IBA ACL table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA ACL table read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_acl_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+	u32 *data = out;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			reg &= ((1 << 12) - 1);
+			switch (reg) {
+			case REG_PORT_ACL_0:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[0] = cpu_to_be32(data[0]);
+				break;
+			case REG_PORT_ACL_4:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[1] = cpu_to_be32(data[1]);
+				break;
+			case REG_PORT_ACL_8:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[2] = cpu_to_be32(data[2]);
+				break;
+			case REG_PORT_ACL_C:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[3] = cpu_to_be32(data[3]);
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_acl_table_post */
+#endif
+
+/**
+ * iba_r_acl_hw - read from ACL table using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This routine reads from ACL table of the port using IBA.
+ */
+static void iba_r_acl_hw(struct ksz_sw *sw, int port, u16 addr, u8 data[])
+{
+#ifndef NO_ACL
+	u32 *ptr_32 = (u32 *) data;
+
+	memset(data, 0, sizeof(u32) * 4);
+	ptr_32[4] = addr;
+	iba_req(&sw->info->iba, data, data, &port, r_acl_table_pre,
+		r_acl_table_post);
+#else
+	memset(data, 0, 16);
+#endif
+}  /* iba_r_acl_hw */
+
+#ifndef NO_ACL
+/**
+ * w_acl_table_pre - IBA ACL table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for ACL table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_acl_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int *port = obj;
+	u16 addr = data[4];
+	u32 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		info->data[0] = be32_to_cpu(data[i]);
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_ACL_0 + 4 * i));
+	}
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0));
+	info->data[0] = PORT_ACL_WRITE_DONE << 8;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0 & ~3));
+	return info->fptr;
+}  /* w_acl_table_pre */
+#endif
+
+/**
+ * iba_w_acl_hw - write to ACL table using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This routine writes to ACL table of the port using IBA.
+ */
+static void iba_w_acl_hw(struct ksz_sw *sw, int port, u16 addr, u8 data[])
+{
+#ifndef NO_ACL
+	u32 *ptr_32 = (u32 *) data;
+
+	ptr_32[4] = addr;
+	iba_req(&sw->info->iba, data, NULL, &port, w_acl_table_pre, NULL);
+#endif
+}  /* iba_w_acl_hw */
+
+static struct ksz_sw_reg_ops sw_iba_ops = {
+	.lock			= iba_lock,
+	.unlock			= iba_unlock,
+
+	.r8			= iba_r8,
+	.r16			= iba_r16,
+	.r24			= iba_r24,
+	.r32			= iba_r32,
+	.w8			= iba_w8,
+	.w16			= iba_w16,
+	.w24			= iba_w24,
+	.w32			= iba_w32,
+
+	.r			= iba_r_buf,
+	.w			= iba_w_buf,
+
+	.get			= iba_get,
+	.set			= iba_set,
+
+	.r_dyn_mac_hw		= iba_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= iba_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= iba_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= iba_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= iba_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= iba_r_sta_mac_hw,
+	.w_sta_mac_hw		= iba_w_sta_mac_hw,
+	.r_vlan_hw		= iba_r_vlan_hw,
+	.w_vlan_hw		= iba_w_vlan_hw,
+	.r_mib_cnt_hw		= iba_r_mib_cnt_hw,
+	.r_acl_hw		= iba_r_acl_hw,
+	.w_acl_hw		= iba_w_acl_hw,
+};
+
+/**
+ * iba_rcv - Receive IBA response.
+ * @info:	The IBA instance.
+ * @skb:	The received socket buffer.
+ *
+ * This function processes IBA response.
+ */
+static int iba_rcv(struct ksz_iba_info *info, struct sk_buff *skb)
+{
+	int i;
+	int j;
+	int k;
+	int cnt;
+	int len;
+	int cmd_shift;
+	u32 cmd;
+	u32 cmds;
+	u32 addr;
+	u32 data;
+	struct iba_cmd *frame;
+	struct iba_frame *iba;
+	u8 *ptr;
+	int ret = 1;
+
+if (dbg_iba)
+dbg_msg(" iba rx: %x\n", info->seqid);
+
+	ptr = skb->data;
+	ptr += ETH_ALEN * 2;
+	iba = (struct iba_frame *) ptr;
+
+	ptr -= ETH_ALEN * 2;
+	if (!info->cmds[0].cmd)
+		goto out_drop;
+
+	if (iba->tag.type != htons(info->tag_type) ||
+	    iba->format.format != htons(IBA_FORMAT_KSZ98XX))
+		goto out_drop;
+
+#if 0
+	dbg_msg("seq: %x\n", iba->tag.seqid);
+#endif
+	if (iba->tag.seqid != info->seqid)
+		goto out_debug;
+
+	len = ntohs(iba->length);
+	cnt = skb->len;
+	if (len != cnt) {
+		if (skb->len > 60 && len + 4 != cnt)
+			dbg_msg("len: %d != %d\n", len, cnt);
+		if (len > cnt)
+			len = cnt;
+	}
+	len -= ETH_ALEN * 2 + sizeof(struct iba_frame) -
+		sizeof(struct iba_cmd);
+	if (ntohs(iba->code) == IBA_CODE_NORMAL) {
+#if 0
+		dbg_msg("normal\n");
+#endif
+		cmd_shift = IBA_CMD_S;
+	} else {
+#if 0
+		dbg_msg("burst\n");
+#endif
+		cmd_shift = IBA_BURST_S;
+	}
+	frame = &iba->cmd;
+	j = 0;
+	k = 0;
+	while (len >= 4 && frame->cmd) {
+		cmd = ntohl(frame->cmd);
+		if (0xdeadbeef == cmd) {
+dbg_msg("apb\n");
+			break;
+		}
+		cmds = cmd;
+		addr = cmd & IBA_CMD_ADDR_M;
+		i = 0;
+		data = ntohl(frame->data[i++]);
+		if (cmd != info->cmds[j].cmd || (IBA_BURST_S == cmd_shift
+				&& data != info->cmds[j].data[0]))
+			dbg_msg("?cmd %x=%x %x=%x\n", info->cmds[j].cmd, cmd,
+				info->cmds[j].data[0], data);
+		cmd >>= cmd_shift;
+		if (IBA_BURST_S == cmd_shift) {
+			cnt = data;
+			if (IBA_BURST_WRITE == cmd) {
+#if 0
+				dbg_msg("w: %08x=%d\n", addr, cnt);
+#endif
+#if 0
+				i += cnt;
+#else
+				for (; i <= cnt; i++) {
+					data = ntohl(frame->data[i]);
+					iba_chk_regs(info->sw_dev, cmds, data);
+					cmds += 4;
+#if 0
+					dbg_msg("%08x ", data);
+#endif
+				}
+#if 0
+				if (cnt)
+					dbg_msg("\n");
+#endif
+#endif
+			} else if (IBA_BURST_READ == cmd) {
+#if 0
+				dbg_msg("r: %08x=%d\n", addr, cnt);
+#endif
+				info->regs[k].cmd = cmds;
+				for (; i <= cnt; i++) {
+					data = ntohl(frame->data[i]);
+					info->regs[k++].data[0] = data;
+					info->regs[k].cmd =
+						info->regs[k - 1].cmd + 4;
+#if 0
+					dbg_msg("%08x ", data);
+#endif
+				}
+#if 0
+				if (cnt)
+					dbg_msg("\n");
+#endif
+			} else
+				break;
+			len -= sizeof(u32) * cnt;
+		} else {
+			switch (cmd) {
+			case IBA_CMD_READ:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("r: ");
+#endif
+				break;
+			case IBA_CMD_WRITE:
+				iba_chk_regs(info->sw_dev, cmds, data);
+#if 0
+				dbg_msg("w: ");
+#endif
+				break;
+			case IBA_CMD_WAIT_ON_0:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("z: ");
+#endif
+				break;
+			case IBA_CMD_WAIT_ON_1:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("s: ");
+#endif
+				break;
+			case IBA_CMD_WRITE_0:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("0: ");
+#endif
+				break;
+			case IBA_CMD_WRITE_1:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("1: ");
+#endif
+				break;
+			}
+#if 0
+			dbg_msg("%08x=%08x\n", addr, data);
+#endif
+		}
+		j++;
+		len -= sizeof(struct iba_cmd);
+		frame = (struct iba_cmd *) &frame->data[i];
+	}
+#if 0
+	dbg_msg("\n");
+#endif
+	if (len != 4)
+		dbg_msg("?len: %d\n", len);
+	if (info->cmds[j].cmd != 0)
+		dbg_msg("? %x\n", info->cmds[j].cmd);
+	if (len != 4 && info->cmds[j].cmd != 0) {
+		for (i = 0; i < skb->len + ETH_ALEN * 2 + 2; i++) {
+			dbg_msg("%02x ", ptr[i]);
+			if (15 == (i % 16))
+				dbg_msg("\n");
+		}
+		if (15 != (i % 16))
+			dbg_msg("\n");
+	}
+	info->cmds[0].cmd = 0;
+	info->regs[k].cmd = (u32) -1;
+	info->respid = iba->tag.seqid;
+if (dbg_iba) {
+dbg_iba = 0;
+dbg_msg("ok %x %x\n", info->seqid, last_ok_iba);
+}
+last_ok_iba = info->respid;
+
+	dev_kfree_skb_irq(skb);
+	complete(&info->done);
+	return 0;
+
+out_debug:
+dbg_iba = 1;
+dbg_msg("last ok: %x\n", last_ok_iba);
+	dbg_msg("seq: %x\n", info->seqid);
+#if 0
+	for (i = 0; i < sizeof(struct iba_frame) + 14; i++) {
+		dbg_msg("%02x ", ptr[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (15 != (i % 16))
+		dbg_msg("\n");
+#endif
+	for (i = 0; i < ntohs(info->frame->length); i++) {
+		dbg_msg("%02x ", info->packet[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (15 != (i % 16))
+		dbg_msg("\n");
+	for (i = 0; i < skb->len; i++) {
+		dbg_msg("%02x ", skb->data[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (15 != (i % 16))
+		dbg_msg("\n");
+
+out_drop:
+	return ret;
+}  /* iba_rcv */
+
+static void ksz_iba_init(struct ksz_iba_info *iba, struct ksz_sw *sw)
+{
+	u32 data;
+	u16 tag_type;
+
+	/* Running nuttcp UDP TX can affect IBA communication if too short. */
+	data = 200;
+	iba->delay_ticks = data * HZ / 1000;
+
+#ifndef TEST_IBA
+	sw->ops->acquire(sw);
+	data = sw_r32(sw, REG_SW_IBA__4);
+	tag_type = (data & SW_IBA_FRAME_TPID_M);
+	sw->ops->release(sw);
+#else
+	tag_type = ETH_P_IBA;
+#endif
+
+	iba->sw_dev = sw;
+	iba->packet = kzalloc(IBA_LEN_MAX, GFP_KERNEL);
+	iba->buf = kzalloc(IBA_LEN_MAX, GFP_KERNEL);
+	iba->data = kzalloc(IBA_BURST_CNT_MAX * sizeof(u32), GFP_KERNEL);
+	iba->regs = kmalloc(IBA_BURST_CNT_MAX * sizeof(struct iba_cmd),
+		GFP_KERNEL);
+	iba->cmds = kmalloc(IBA_BURST_CNT_MAX * sizeof(struct iba_cmd) / 4,
+		GFP_KERNEL);
+	iba->frame = (struct iba_frame *) &iba->packet[ETH_ALEN * 2];
+	iba->tag_type = tag_type;
+	iba->dst[0] = 0x01;
+	iba->dst[1] = 0x00;
+	iba->dst[2] = 0x5E;
+	iba->dst[3] = 0x00;
+	iba->dst[4] = 0x01;
+	iba->dst[5] = 0x81;
+#ifndef CAPTURE_IBA
+	memcpy(iba->dst, sw->info->mac_addr, ETH_ALEN);
+#endif
+	iba->src[0] = 0x00;
+#ifdef CAPTURE_IBA
+	iba->src[0] = 0x01;
+#endif
+	iba->src[1] = 0x10;
+	iba->src[2] = 0xA1;
+	iba->src[3] = 0x98;
+	iba->src[4] = 0x97;
+	iba->src[5] = 0x81;
+	init_completion(&iba->done);
+	init_waitqueue_head(&iba->queue);
+	mutex_init(&iba->lock);
+
+	iba_info = iba;
+}  /* ksz_iba_init */
+
+static void ksz_iba_exit(struct ksz_iba_info *iba)
+{
+	kfree(iba->cmds);
+	kfree(iba->regs);
+	kfree(iba->data);
+	kfree(iba->buf);
+	kfree(iba->packet);
+}  /* ksz_iba_exit */
+
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_iba.c"
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz_iba.h b/drivers/net/ethernet/micrel/ksz_iba.h
new file mode 100644
index 0000000..6a32a34
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_iba.h
@@ -0,0 +1,125 @@
+/**
+ * Micrel IBA header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_IBA_H
+#define KSZ_IBA_H
+
+
+#define IBA_TAG_TYPE		0x40FE
+
+struct iba_tag {
+	u16 type;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 prio:3;
+	u8 cfi:1;
+	u8 mode:4;
+#else
+	u8 mode:4;
+	u8 cfi:1;
+	u8 prio:3;
+#endif
+	u8 seqid;
+} __packed;
+
+#define IBA_FORMAT_KSZ98XX	0x9800
+
+struct iba_format {
+	u16 format;
+	u16 reserved;
+} __packed;
+
+#define IBA_CODE_NORMAL		0x0001
+#define IBA_CODE_BURST		0x0002
+
+#define IBA_CMD_READ		1
+#define IBA_CMD_WRITE		2
+#define IBA_CMD_WAIT_ON_0	4
+#define IBA_CMD_WAIT_ON_1	5
+#define IBA_CMD_WRITE_0		6
+#define IBA_CMD_WRITE_1		7
+#define IBA_CMD_S		29
+
+#define IBA_CMD_BYTE_0		(1 << 27)
+#define IBA_CMD_BYTE_1		(1 << 26)
+#define IBA_CMD_BYTE_2		(1 << 25)
+#define IBA_CMD_BYTE_3		(1 << 24)
+
+#define IBA_CMD_32		\
+	(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_24		\
+	(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2)
+#define IBA_CMD_24_H		\
+	(IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_16		(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1)
+#define IBA_CMD_16_M		(IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2)
+#define IBA_CMD_16_H		(IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_8		(IBA_CMD_BYTE_0)
+
+#define IBA_CMD_ADDR_M		((1 << 24) - 1)
+
+#define IBA_BURST_READ		1
+#define IBA_BURST_WRITE		2
+#define IBA_BURST_S		30
+#define IBA_BURST_CNT_MAX	(1 << 7)
+#define IBA_BURST_CNT_M		((1 << 7) - 1)
+
+struct iba_cmd {
+	u32 cmd;
+	u32 data[1];
+} __packed;
+
+struct iba_frame {
+	struct iba_tag tag;
+	u16 length;
+	struct iba_format format;
+	u16 code;
+	struct iba_cmd cmd;
+};
+
+#define IBA_LEN_MAX		288
+
+struct ksz_iba_info {
+	int use_iba;
+	void *sw_dev;
+	u16 tag_type;
+	u8 dst[ETH_ALEN];
+	u8 src[ETH_ALEN];
+	u8 *buf;
+	u8 *packet;
+	struct iba_frame *frame;
+	struct iba_cmd *cmds;
+	struct iba_cmd *regs;
+	u8 seqid;
+	u8 respid;
+	struct completion done;
+	wait_queue_head_t queue;
+	unsigned long delay_ticks;
+	struct net_device *dev;
+	struct mutex lock;
+	int cnt;
+	u32 cfg;
+
+	/* Used for putting in commands. */
+	u32 *data;
+	void *fptr;
+	int index;
+	int len;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_mrp.c b/drivers/net/ethernet/micrel/ksz_mrp.c
new file mode 100644
index 0000000..1f136fe
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_mrp.c
@@ -0,0 +1,2810 @@
+/**
+ * Micrel MRP driver code
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#define SW_CREDIT_SHAPING_SCALE	0x10000
+#define SW_CREDIT_SHAPING_S	16
+
+#define CREDIT_PERCENTAGE_S	(16 + 9)
+
+#define NETWORK_SPEED_IN_MBIT	1000000
+
+static u16 get_credit_increment(u32 speed, u32 bandwidth)
+{
+	u64 val;
+	u32 rem;
+
+	speed *= NETWORK_SPEED_IN_MBIT;
+
+	/* Cannot get higher than the running speed. */
+	if (bandwidth > speed)
+		return 0;
+	val = bandwidth;
+	val <<= SW_CREDIT_SHAPING_S;
+	val = div_u64_rem(val, speed, &rem);
+
+	/* Cannot become zero. */
+	if (!val)
+		val = 1;
+	return (u16) val;
+}  /* get_credit_increment */
+
+static u16 get_hi_credit(u32 size, u32 slope)
+{
+	u64 val;
+	u32 rem;
+
+	val = size;
+	val *= slope;
+	size = 100;
+	size <<= CREDIT_PERCENTAGE_S;
+	val = div_u64_rem(val, size, &rem);
+	val >>= 3;
+	return (u16) val;
+}  /* get_hi_credit */
+
+static u16 get_lo_credit(u32 size, u32 slope)
+{
+	u64 val;
+	u32 rem;
+
+	val = size;
+	val *= slope;
+	size = 100;
+	size <<= CREDIT_PERCENTAGE_S;
+	val = div_u64_rem(val, size, &rem);
+	val >>= 3;
+	return (u16) val;
+}  /* get_lo_credit */
+
+static u32 get_idle_slope(u32 speed, u32 bandwidth)
+{
+	u64 val;
+	u32 rem;
+
+	speed *= NETWORK_SPEED_IN_MBIT;
+
+	/* Cannot get higher than the running speed. */
+	if (bandwidth > speed)
+		return 0;
+	val = bandwidth;
+	val *= 100;
+	val <<= CREDIT_PERCENTAGE_S;
+	val = div_u64_rem(val, speed, &rem);
+	return (u32) val;
+}  /* get_idle_slope */
+
+static u32 get_send_slope(u32 idle_slope)
+{
+	u32 slope;
+
+	slope = 100;
+	slope <<= CREDIT_PERCENTAGE_S;
+	slope -= idle_slope;
+	return slope;
+}  /* get_send_slope */
+
+static u8 get_queue_priority(struct mrp_info *mrp, u8 tc)
+{
+	return mrp->prio[tc];
+}  /* get_queue_priority */
+
+static void srp_cfg_credit_shaper(struct mrp_info *mrp, u8 port, u8 tc)
+{
+	u16 credit;
+	u16 credit_lo;
+	u16 credit_hi;
+	u32 idle;
+	u32 send;
+	u8 queue;
+	struct mrp_port_info *info = &mrp->port_info[port];
+
+	credit = get_credit_increment(info->speed,
+		info->traffic[tc].bandwidth_used);
+	idle = get_idle_slope(info->speed, info->traffic[tc].bandwidth_used);
+	send = get_send_slope(idle);
+	credit_hi = get_hi_credit(mrp->max_interference_size, idle);
+	credit_lo = get_lo_credit(info->traffic[tc].max_frame_size, send);
+	queue = get_queue_priority(mrp, tc);
+dbg_msg("%s %d:%d=%x %x %x; %u %u %u\n", __func__, port,
+	queue, credit, credit_hi, credit_lo, info->traffic[tc].bandwidth_used,
+idle, send);
+#ifdef CONFIG_HAVE_KSZ9897
+	do {
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		sw->reg->lock(sw);
+		if (credit > 1) {
+			port_set_hi_water_mark(sw, port, queue, credit_hi);
+			port_set_lo_water_mark(sw, port, queue, credit_lo);
+			port_set_increment(sw, port, queue, credit);
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_STRICT_PRIO);
+			port_set_shaping(sw, port, queue, MTI_SHAPING_SRP);
+		} else {
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_STRICT_PRIO);
+			port_set_shaping(sw, port, queue, MTI_SHAPING_OFF);
+		}
+		sw->reg->unlock(sw);
+	} while (0);
+#endif
+}  /* srp_cfg_credit_shaper */
+
+static void mrp_cfg_dest_addr(struct mrp_info *mrp, u8 index, u8 *dest,
+	u32 ports)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+dbg_msg("%s %d=%02x:%02x:%02x:%02x:%02x:%02x %04x\n", __func__, index,
+dest[0], dest[1], dest[2], dest[3], dest[4],dest[5],
+ports);
+	ports &= ~(1 << 15);
+	sw->ops->cfg_mac(sw, index, dest, ports, false, true, 2);
+}  /* mrp_cfg_dest_addr */
+
+static void mrp_cfg_vlan(struct mrp_info *mrp, u8 index, u16 vid, u32 ports)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+dbg_msg("%s %d=%x %04x\n", __func__, index, vid, ports);
+	ports &= ~(1 << 15);
+	sw->ops->cfg_vlan(sw, index, vid, 2, ports);
+}  /* mrp_cfg_vlan */
+
+static int get_traffic_class(struct mrp_info *mrp, u8 prio)
+{
+	return mrp->tc[prio];
+}  /* get_traffic_class */
+
+static int get_traffic_priority(struct mrp_info *mrp, u8 tc)
+{
+	int i;
+
+	for (i = 0; i < 4; i++)
+		if (mrp->tc[i] == tc)
+			return i;
+	return 0;
+}  /* get_traffic_priority */
+
+static int frames_per_sec(int traffic_class)
+{
+	int frames;
+
+	switch (traffic_class) {
+	case SR_CLASS_A:
+		frames = 8000;
+		break;
+	case SR_CLASS_B:
+	default:
+		frames = 4000;
+	}
+	return frames;
+}  /* frames_per_sec */
+
+static u64 calculate_bandwidth(u32 size, u32 interval, u32 frames)
+{
+	u64 bandwidth;
+
+	bandwidth = size;
+	bandwidth += 42;
+	bandwidth *= interval;
+	bandwidth *= frames;
+	bandwidth *= 8;
+	return bandwidth;
+}  /* calculate_bandwidth */
+
+static int cmp_mac(void *first, void *second)
+{
+	int cmp;
+	u8 *a = first;
+	u8 *b = second;
+
+	cmp = memcmp(a, b, ETH_ALEN);
+	return cmp;
+}  /* cmp_mac */
+
+static void show_mac_info(void *this)
+{
+	struct mrp_mac_info *info = this;
+
+	dbg_msg(
+		"%02x:%02x:%02x:%02x:%02x:%02x %d=%04x %04x %04x\n",
+		info->addr[0], info->addr[1], info->addr[2],
+		info->addr[3], info->addr[4], info->addr[5],
+		info->index,
+		info->mrp_ports, info->srp_ports, info->tx_ports);
+}  /* show_mac_info */
+
+static int cmp_vlan(void *first, void *second)
+{
+	int cmp;
+	struct mrp_vlan_info *a = first;
+	struct mrp_vlan_info *b = second;
+
+	cmp = a->vid - b->vid;
+	if (!cmp)
+		cmp = memcmp(a->addr, b->addr, ETH_ALEN);
+	return cmp;
+}  /* cmp_vlan */
+
+static void show_vlan_info(void *this)
+{
+	struct mrp_vlan_info *info = this;
+
+	if (info->addr[0] != 0xff)
+		dbg_msg(
+			"[%02x:%02x:%02x:%02x:%02x:%02x] ",
+			info->addr[0], info->addr[1], info->addr[2],
+			info->addr[3], info->addr[4], info->addr[5]);
+	dbg_msg(
+		"%d=%03x %04x %04x\n", info->index,
+		info->vid, info->ports, info->tx_ports);
+}  /* show_vlan_info */
+
+static void prepare_stream_info(struct SRP_reservation *reserv, u8 tc,
+	struct srp_stream_info *x)
+{
+	x->reserv = reserv;
+	x->id = reserv->stream->id;
+	x->age = reserv->streamAge;
+	x->rank = reserv->stream->rank;
+	x->tc = tc;
+}  /* prepare_stream_info */
+
+static int cmp_stream(struct srp_stream_info *a, struct srp_stream_info *b)
+{
+	int cmp;
+
+	cmp = b->tc - a->tc;
+	if (cmp)
+		return cmp;
+	cmp = a->rank - b->rank;
+	if (cmp)
+		return cmp;
+	cmp = a->age - b->age;
+	if (cmp)
+		return cmp;
+	cmp = memcmp(a->id, b->id, 8);
+	return cmp;
+}  /* cmp_stream */
+
+static int cmp_lower_stream(void *first, void *second)
+{
+	return cmp_stream(second, first);
+}  /* cmp_lower_stream */
+
+static int cmp_higher_stream(void *first, void *second)
+{
+	return cmp_stream(first, second);
+}  /* cmp_higher_stream */
+
+static void show_stream_info(void *this)
+{
+	struct srp_stream_info *info = this;
+
+	dbg_msg(
+		"p:%u r:%u t:%08x %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x\n",
+		info->tc, info->rank, info->age,
+		info->id[0], info->id[1], info->id[2], info->id[3],
+		info->id[4], info->id[5], info->id[6], info->id[7]);
+}  /* show_stream_info */
+
+void *get_show(int (*cmp)(void *a, void *b))
+{
+	if (cmp == cmp_mac)
+		return show_mac_info;
+	else if (cmp == cmp_vlan)
+		return show_vlan_info;
+	else
+		return show_stream_info;
+}  /* get_show */
+
+static void mrp_init_list(struct mrp_node_anchor *list)
+{
+	list->last = &list->anchor;
+}  /* mrp_init_list */
+
+static void *mrp_find_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), void *data)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_find_node_done;
+
+		/* Will not be found as list is sorted. */
+		if (c > 0) {
+#ifdef DEBUG
+			void (*show)(void *this) = get_show(cmp);
+
+dbg_msg(" %s ", __func__);
+			show(next->data);
+#endif
+			next = NULL;
+			break;
+		}
+		prev = next;
+		next = prev->next;
+	}
+
+mrp_find_node_done:
+	list->last = prev;
+	return next;
+}  /* mrp_find_node */
+
+static void mrp_insert_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	if (list->last != &list->anchor) {
+		prev = list->last;
+		next = prev->next;
+#ifdef DEBUG
+dbg_msg(" %s ", __func__);
+		if (next) {
+			void (*show)(void *this) = get_show(cmp);
+
+			show(next->data);
+		} else
+dbg_msg("last one\n");
+#endif
+		c = 1;
+		if (next) {
+			c = cmp(next->data, this->data);
+		}
+		if (c > 0) {
+			this->next = prev->next;
+			prev->next = this;
+			list->last = &list->anchor;
+			return;
+		}
+	}
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, this->data);
+
+		/* Stop if next one is higher in the list. */
+		if (c > 0)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	this->next = prev->next;
+	prev->next = this;
+	list->last = &list->anchor;
+}  /* mrp_insert_node */
+
+static struct mrp_node *mrp_delete_this_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this, int delete)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	if (list->last != &list->anchor) {
+		prev = list->last;
+		next = prev->next;
+#ifdef DEBUG
+dbg_msg(" %s ", __func__);
+		if (next) {
+			void (*show)(void *this) = get_show(cmp);
+
+			show(next->data);
+		} else
+dbg_msg("last one\n");
+#endif
+		c = 1;
+		if (next == this)
+			c = 0;
+		else if (next)
+			c = cmp(next->data, this->data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_delete_this_node_done;
+	}
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, this->data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_delete_this_node_done;
+
+		/* Stop if next one is higher in the list. */
+		if (c > 0)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+
+	/* Nothing is removed. */
+	return NULL;
+
+mrp_delete_this_node_done:
+	prev->next = this->next;
+	list->last = &list->anchor;
+
+	/* Just remove the node. */
+	if (!delete)
+		return next;
+
+	/* Free the node data. */
+	if (delete > 1)
+		kfree(next->data);
+
+	/* Free the node. */
+	kfree(next);
+	return NULL;
+}  /* mrp_delete_this_node */
+
+static void mrp_delete_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this, int delete)
+{
+	mrp_delete_this_node(list, cmp, this, ++delete);
+}  /* mrp_delete_node */
+
+static struct mrp_node *mrp_remove_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	return mrp_delete_this_node(list, cmp, this, 0);
+}  /* mrp_remove_node */
+
+static void mrp_show_node(struct mrp_node_anchor *list,
+	void (*show)(void *a))
+{
+	struct mrp_node *next;
+
+	next = list->anchor.next;
+	while (next) {
+		show(next->data);
+		next = next->next;
+	}
+}  /* mrp_show_node */
+
+static struct SRP_stream *srp_create_stream(u8 *id, u8 *dest, u16 vlan_id,
+	u16 size, u16 interval, u8 prio, u8 rank)
+{
+	struct SRP_stream *stream;
+
+	stream = kzalloc(sizeof(struct SRP_stream), GFP_KERNEL);
+	if (stream) {
+		memcpy(stream->id, id, 8);
+		memcpy(stream->dest, dest, ETH_ALEN);
+		stream->vlan_id = vlan_id;
+		stream->MaxFrameSize = size;
+		stream->MaxIntervalFrames = interval;
+		stream->priority = prio;
+		stream->rank = rank;
+	}
+	return stream;
+}  /* srp_create_stream */
+
+static struct SRP_stream *srp_find_stream_id(struct mrp_info *mrp, u8 *id)
+{
+	int cmp;
+	struct SRP_stream *stream;
+
+	stream = mrp->stream_by_id.id_next;
+	while (stream) {
+		cmp = memcmp(stream->id, id, 8);
+		if (!cmp)
+			break;
+		stream = stream->id_next;
+	}
+	return stream;
+}  /* srp_find_stream_id */
+
+static struct SRP_stream *srp_find_dest_addr(struct mrp_info *mrp, u8 *dest)
+{
+	int cmp;
+	struct SRP_stream *stream;
+
+	stream = mrp->stream_by_dest.dest_next;
+	while (stream) {
+		cmp = memcmp(stream->dest, dest, ETH_ALEN);
+		if (!cmp)
+			break;
+		stream = stream->dest_next;
+	}
+	return stream;
+}  /* srp_find_dest_addr */
+
+static void srp_insert_stream_by_id(struct mrp_info *mrp,
+	struct SRP_stream *stream)
+{
+	struct SRP_stream *prev;
+	struct SRP_stream *next;
+
+	prev = &mrp->stream_by_id;
+	next = prev->id_next;
+	while (next) {
+		if (memcmp(next->id, stream->id, 8) > 0)
+			break;
+		prev = next;
+		next = prev->id_next;
+	}
+	if (next) {
+		stream->id_next = next;
+		next->id_prev = stream;
+	}
+	stream->id_prev = prev;
+	prev->id_next = stream;
+}  /* srp_insert_stream_by_id */
+
+static void srp_insert_stream_by_dest(struct mrp_info *mrp,
+	struct SRP_stream *stream)
+{
+	struct SRP_stream *prev;
+	struct SRP_stream *next;
+
+	prev = &mrp->stream_by_dest;
+	next = prev->dest_next;
+	while (next) {
+		if (memcmp(next->dest, stream->dest, ETH_ALEN) > 0)
+			break;
+		prev = next;
+		next = prev->dest_next;
+	}
+	if (next) {
+		stream->dest_next = next;
+		next->dest_prev = stream;
+	}
+	stream->dest_prev = prev;
+	prev->dest_next = stream;
+}  /* srp_insert_stream_by_dest */
+
+static void srp_remove_stream(struct mrp_info *mrp, struct SRP_stream *stream)
+{
+	if (stream->id_next)
+		stream->id_next->id_prev = stream->id_prev;
+	stream->id_prev->id_next = stream->id_next;
+	if (stream->dest_next)
+		stream->dest_next->dest_prev = stream->dest_prev;
+	stream->dest_prev->dest_next = stream->dest_next;
+	kfree(stream);
+}  /* srp_remove_stream */
+
+static struct SRP_reservation *srp_create_reservation(u8 *id, u8 dir, u8 dec,
+	u32 latency, u8 *bridge_id, u8 code)
+{
+	struct SRP_reservation *reserv;
+
+	reserv = kzalloc(sizeof(struct SRP_reservation), GFP_KERNEL);
+	if (reserv) {
+		memcpy(reserv->id, id, 8);
+		reserv->direction = dir;
+		reserv->declaration = dec;
+		reserv->latency = latency;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+		reserv->code = code;
+	}
+	return reserv;
+}  /* srp_create_reservation */
+
+static struct SRP_reservation *srp_find_reservation(
+	struct SRP_reservation *head, u8 *id, u8 dir)
+{
+	int cmp;
+	struct SRP_reservation *reserv;
+
+	reserv = head->next;
+	while (reserv) {
+		cmp = memcmp(reserv->id, id, 8);
+		if (!cmp && dir == reserv->direction)
+			break;
+		if (cmp > 0)
+			return NULL;
+		reserv = reserv->next;
+	}
+	return reserv;
+}  /* srp_find_reservation */
+
+static void srp_insert_reservation(struct SRP_reservation *head,
+	struct SRP_reservation *reserv)
+{
+	int cmp;
+	struct SRP_reservation *prev;
+	struct SRP_reservation *next;
+
+	prev = head;
+	next = prev->next;
+	while (next) {
+		cmp = memcmp(next->id, reserv->id, 8);
+		if (cmp > 0)
+			break;
+		if (0 == cmp && next->declaration > reserv->declaration)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	if (next) {
+		reserv->next = next;
+		next->prev = reserv;
+	}
+	reserv->prev = prev;
+	prev->next = reserv;
+}  /* srp_insert_reservation */
+
+static void srp_remove_reservation(struct SRP_reservation *reserv)
+{
+	if (reserv->next)
+		reserv->next->prev = reserv->prev;
+	reserv->prev->next = reserv->next;
+	kfree(reserv);
+}  /* srp_remove_reservation */
+
+static void chk_reservation(struct mrp_port_info *info, int port)
+{
+	struct SRP_reservation *reserv;
+
+dbg_msg("%d:\n", port);
+dbg_msg("  registered: %p\n", &info->registered);
+	reserv = info->registered.next;
+	while (reserv) {
+dbg_msg("%02x:%02x %d %d\n", reserv->id[6], reserv->id[7], reserv->direction,
+	reserv->declaration);
+		reserv = reserv->next;
+	}
+dbg_msg("  declared: %p\n", &info->declared);
+	reserv = info->declared.next;
+	while (reserv) {
+dbg_msg("%02x:%02x %d %d\n", reserv->id[6], reserv->id[7], reserv->direction,
+	reserv->declaration);
+		reserv = reserv->next;
+	}
+dbg_msg("  active:\n");
+	mrp_show_node(&info->active, show_stream_info);
+dbg_msg("  passive:\n");
+	mrp_show_node(&info->passive, show_stream_info);
+dbg_msg("A:%u B:%u T:%u\n",
+	info->traffic[SR_CLASS_A].bandwidth_left,
+	info->traffic[SR_CLASS_B].bandwidth_left,
+	info->bandwidth_left);
+dbg_msg("\n");
+}  /* chk_reservation */
+
+static struct mrp_node *mrp_get_mac_info(struct mrp_node_anchor *list,
+	u8 *addr)
+{
+	struct mrp_node *node;
+
+	node = mrp_find_node(list, cmp_mac, addr);
+	if (!node) {
+		node = kzalloc(sizeof(struct mrp_node), GFP_KERNEL);
+		if (!node)
+			return NULL;
+		node->data = kzalloc(sizeof(struct mrp_mac_info), GFP_KERNEL);
+		if (!node->data) {
+			kfree(node);
+			return NULL;
+		}
+		memcpy(node->data, addr, ETH_ALEN);
+		mrp_insert_node(list, cmp_mac, node);
+	}
+	return node;
+}  /* mrp_get_mac_info */
+
+static struct mrp_node *mrp_get_vlan_info(struct mrp_node_anchor *list,
+	u16 vid, u8 *addr)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info data;
+
+	data.vid = vid;
+	if (addr)
+		memcpy(data.addr, addr, ETH_ALEN);
+	else
+		memset(data.addr, 0xff, ETH_ALEN);
+	node = mrp_find_node(list, cmp_vlan, &data);
+	if (!node) {
+		struct mrp_vlan_info *info;
+
+		node = kzalloc(sizeof(struct mrp_node), GFP_KERNEL);
+		if (!node)
+			return NULL;
+		node->data = kzalloc(sizeof(struct mrp_vlan_info), GFP_KERNEL);
+		if (!node->data) {
+			kfree(node);
+			return NULL;
+		}
+		info = node->data;
+		info->vid = vid;
+		memcpy(info->addr, data.addr, ETH_ALEN);
+		mrp_insert_node(list, cmp_vlan, node);
+	}
+	return node;
+}  /* mrp_get_vlan_info */
+
+static u16 mrp_find_vlan_ports(struct mrp_node_anchor *list, u16 vid,
+	u8 *index)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *info;
+	u16 ports = 0;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		info = next->data;
+		if (vid == info->vid) {
+			ports |= info->ports;
+			if (info->ports && index && *index != info->index)
+				*index = info->index;
+		} else if (vid > info->vid)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	return ports;
+}
+
+static struct mrp_report *mrp_create_report(struct SRP_reservation *reserv,
+	u8 port)
+{
+	struct mrp_report *attrib;
+
+	attrib = kzalloc(sizeof(struct mrp_report), GFP_KERNEL);
+	if (attrib) {
+		attrib->attrib = reserv;
+		attrib->port = port;
+	}
+	return attrib;
+}  /* mrp_create_report */
+
+static void add_attrib_report(struct mrp_info *mrp, void *ptr, u8 action,
+	u8 type, u8 port)
+{
+	struct mrp_report *attrib;
+
+	attrib = mrp_create_report(ptr, port);
+	if (!attrib)
+		return;
+
+	attrib->action = action;
+	attrib->type = type;
+
+	if (mrp->report_tail)
+		mrp->report_tail->next = attrib;
+	mrp->report_tail = attrib;
+	if (!mrp->report_head)
+		mrp->report_head = attrib;
+}  /* add_attrib_report */
+
+static u8 mrp_alloc_mac(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	return sw->ops->alloc_mac(sw);
+}  /* mrp_alloc_mac */
+
+static void mrp_free_mac(struct mrp_info *mrp, u8 index)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	sw->ops->free_mac(sw, index);
+}  /* mrp_free_mac */
+
+static u8 mrp_alloc_vlan(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	return sw->ops->alloc_vlan(sw);
+}  /* mrp_alloc_vlan */
+
+static void mrp_free_vlan(struct mrp_info *mrp, u8 index)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	sw->ops->free_vlan(sw, index);
+}  /* mrp_free_vlan */
+
+static int proc_mrp_lv(struct mrp_info *mrp, struct mrp_node *node, u16 ports,
+	u16 *tx_ports, u8 type)
+{
+	int p;
+	int result = DEV_IOC_OK;
+
+	if (!ports) {
+
+		/* Ask all ports to withdraw the declaration. */
+		for (p = 0; p < mrp->ports; p++)
+			if (*tx_ports & (1 << p))
+				add_attrib_report(mrp, node,
+					MRP_ACTION_LV, type, p);
+		*tx_ports = 0;
+
+		/* Notify the host of the Leave indication. */
+		/* Used to free the attribute. */
+		add_attrib_report(mrp, node, MRP_ACTION_LV, type, mrp->ports);
+		result = DEV_IOC_MRP_REPORT;
+	} else {
+		int uninitialized_var(q);
+		int cnt = 0;
+
+		for (p = 0; p < mrp->ports; p++)
+			if (ports & (1 << p)) {
+				q = p;
+				cnt++;
+			}
+		if (1 == cnt) {
+			*tx_ports &= ~(1 << q);
+			add_attrib_report(mrp, node, MRP_ACTION_LV, type, q);
+
+			result = DEV_IOC_MRP_REPORT;
+		}
+	}
+	return result;
+}  /* proc_mrp_lv */
+
+static int proc_mrp_lv_mac(struct mrp_info *mrp, u8 port,
+	struct MRP_mac *mac)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info *info;
+	u16 mrp_ports;
+	u16 ports;
+	int result = DEV_IOC_OK;
+dbg_msg("%s %d %02x:%02x:%02x:%02x:%02x:%02x\n", __func__, port,
+	mac->addr[0],
+	mac->addr[1],
+	mac->addr[2],
+	mac->addr[3],
+	mac->addr[4],
+	mac->addr[5]);
+
+	node = mrp_find_node(&mrp->mac_list, cmp_mac, mac->addr);
+	if (!node)
+		return DEV_IOC_INVALID_CMD;
+
+	info = node->data;
+	mrp_ports = info->mrp_ports;
+	ports = info->ports;
+	info->mrp_ports &= ~(1 << port);
+	info->ports = info->mrp_ports | info->srp_ports;
+
+	/* There is no change in port membership. */
+	if (mrp_ports == info->mrp_ports)
+		return result;
+	if (ports != info->ports) {
+		mrp_cfg_dest_addr(mrp, info->index, info->addr, info->ports);
+		if (!info->ports) {
+			mrp_free_mac(mrp, info->index);
+			mrp_remove_node(&mrp->mac_list, cmp_mac, node);
+		}
+	}
+	if (!mrp->no_report)
+		result = proc_mrp_lv(mrp, node, info->mrp_ports,
+			&info->tx_ports, MRP_TYPE_MAC);
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	return result;
+}  /* proc_mrp_lv_mac */
+
+static int proc_mrp_rx_mac(struct mrp_info *mrp, u8 port,
+	struct MRP_mac *mac)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info *info;
+	u16 mrp_ports;
+	u16 ports;
+	int p;
+	int result = DEV_IOC_OK;
+dbg_msg("%s %d %02x:%02x:%02x:%02x:%02x:%02x\n", __func__, port,
+	mac->addr[0],
+	mac->addr[1],
+	mac->addr[2],
+	mac->addr[3],
+	mac->addr[4],
+	mac->addr[5]);
+
+	node = mrp_get_mac_info(&mrp->mac_list, mac->addr);
+	if (!node)
+		return -ENOMEM;
+	info = node->data;
+	mrp_ports = info->mrp_ports;
+	ports = info->ports;
+	info->mrp_ports |= (1 << port);
+	info->ports = info->mrp_ports | info->srp_ports;
+
+	/* There is change in port membership. */
+	if (mrp_ports != info->mrp_ports) {
+
+		/* First time setting up MAC table. */
+		if (!ports) {
+			info->index = mrp_alloc_mac(mrp);
+			if (!info->index) {
+				mrp_delete_node(&mrp->mac_list, cmp_mac, node,
+					true);
+				return -ENOMEM;
+			}
+		}
+		mrp_cfg_dest_addr(mrp, info->index, info->addr, info->ports);
+	}
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	if (mrp->no_report)
+		return result;
+
+	/* Ask all other ports to declare the attribute. */
+	for (p = 0; p <= mrp->ports; p++)
+		if (p != port && !(info->tx_ports & (1 << p))) {
+			if (p < mrp->ports)
+				info->tx_ports |= (1 << p);
+			add_attrib_report(mrp, node, MRP_ACTION_TX,
+				MRP_TYPE_MAC, p);
+			result = DEV_IOC_MRP_REPORT;
+		}
+	return result;
+}  /* proc_mrp_rx_mac */
+
+static int proc_mrp_lv_vlan(struct mrp_info *mrp, u8 port,
+	struct MRP_vlan *vlan)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct mrp_vlan_info data;
+	u16 ports;
+	int result = DEV_IOC_OK;
+
+dbg_msg("%s %d %d\n", __func__, port, vlan->id);
+	data.vid = vlan->id;
+	memset(data.addr, 0xff, ETH_ALEN);
+	node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (!node)
+		return DEV_IOC_INVALID_CMD;
+
+	info = node->data;
+	ports = info->ports;
+	info->ports &= ~(1 << port);
+
+	/* There is no change in port membership. */
+	if (ports == info->ports)
+		return result;
+	ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL);
+	mrp_cfg_vlan(mrp, info->index, info->vid, ports);
+	if (!ports)
+		mrp_free_vlan(mrp, info->index);
+	if (!info->ports)
+		mrp_remove_node(&mrp->vlan_list, cmp_vlan, node);
+
+	if (!mrp->no_report)
+		result = proc_mrp_lv(mrp, node, info->ports,
+			&info->tx_ports, MRP_TYPE_VLAN);
+#ifdef DEBUG
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	return result;
+}  /* proc_mrp_lv_vlan */
+
+static int proc_mrp_rx_vlan(struct mrp_info *mrp, u8 port,
+	struct MRP_vlan *vlan)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	u16 ports;
+	int p;
+	int result = DEV_IOC_OK;
+
+dbg_msg("%s %d %d\n", __func__, port, vlan->id);
+	node = mrp_get_vlan_info(&mrp->vlan_list, vlan->id, NULL);
+	if (!node)
+		return -ENOMEM;
+	info = node->data;
+	ports = info->ports;
+	info->ports |= (1 << port);
+
+	/* There is change in port membership. */
+	if (ports != info->ports) {
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vlan->id,
+			&info->index);
+
+		/* First time setting up VLAN table. */
+		if (!ports) {
+			info->index = mrp_alloc_vlan(mrp);
+			if (!info->index) {
+				mrp_delete_node(&mrp->vlan_list, cmp_vlan,
+					node, true);
+				return -ENOMEM;
+			}
+		}
+		mrp_cfg_vlan(mrp, info->index, info->vid, ports);
+	}
+#ifdef DEBUG
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	if (mrp->no_report)
+		return result;
+
+	/* Ask all other ports to declare the attribute. */
+	for (p = 0; p <= mrp->ports; p++)
+		if (p != port && !(info->tx_ports & (1 << p))) {
+			if (p < mrp->ports)
+				info->tx_ports |= (1 << p);
+			add_attrib_report(mrp, node, MRP_ACTION_TX,
+				MRP_TYPE_VLAN, p);
+			result = DEV_IOC_MRP_REPORT;
+		}
+	return result;
+}  /* proc_mrp_rx_vlan */
+
+static int proc_mrp_lv_domain(struct mrp_info *mrp, u8 port,
+	struct SRP_domain_class *domain)
+{
+dbg_msg("%s %d %d %d %d\n", __func__, port,
+	domain->id, domain->priority, domain->vlan_id);
+	if (mrp->no_report)
+		return DEV_IOC_OK;
+
+	/* Coming from the host. */
+	if (port == mrp->ports) {
+		for (port = 0; port < mrp->ports; port++)
+			add_attrib_report(mrp, NULL, MRP_ACTION_LV,
+				MRP_TYPE_DOMAIN, port);
+	}
+	return DEV_IOC_OK;
+}  /* proc_mrp_lv_domain */
+
+static int proc_mrp_rx_domain(struct mrp_info *mrp, u8 port,
+	struct SRP_domain_class *domain)
+{
+dbg_msg("%s %d %d %d %d\n", __func__, port,
+	domain->id, domain->priority, domain->vlan_id);
+	mrp->domain = *domain;
+	if (mrp->no_report)
+		return DEV_IOC_OK;
+
+	/* Coming from the host. */
+	if (port == mrp->ports) {
+		for (port = 0; port < mrp->ports; port++)
+			add_attrib_report(mrp, NULL, MRP_ACTION_TX,
+				MRP_TYPE_DOMAIN, port);
+	}
+	return DEV_IOC_OK;
+}  /* proc_mrp_rx_domain */
+
+static int srp_update_mac(struct mrp_info *mrp, u8 *addr, u16 ports, int up)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_node *node;
+	struct mrp_mac_info *mac;
+	struct mrp_mac_info *update;
+
+	node = mrp_get_mac_info(&mrp->mac_list, addr);
+	if (!node)
+		return -ENOMEM;
+	mac = node->data;
+	if (up) {
+		/* Forward in port. */
+		mac->srp_ports |= ports;
+		ports = mac->srp_ports | mac->mrp_ports;
+		list = &mrp->mac_up;
+	} else {
+		/* Filter in port. */
+		mac->srp_ports &= ~ports;
+		ports = mac->ports & ~ports;
+		list = &mrp->mac_down;
+	}
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	node = mrp_get_mac_info(list, addr);
+	if (!node)
+		return -ENOMEM;
+	update = node->data;
+	update->ports = ports;
+	update->index = mac->index;
+	return 0;
+}  /* srp_update_mac */
+
+static int srp_update_vlan(struct mrp_info *mrp, u16 vid, u8 *addr, u16 ports,
+	int up)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_node *node;
+	struct mrp_vlan_info *vlan;
+	struct mrp_vlan_info *update;
+
+	node = mrp_get_vlan_info(&mrp->vlan_list, vid, addr);
+	if (!node)
+		return -ENOMEM;
+	vlan = node->data;
+	if (up) {
+		/* Forward in port. */
+		vlan->ports |= ports;
+		list = &mrp->vlan_up;
+	} else {
+		/* Filter in port. */
+		vlan->ports &= ~ports;
+		list = &mrp->vlan_down;
+	}
+#ifdef DEBUG
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	node = mrp_get_vlan_info(list, vid, addr);
+	if (!node)
+		return -ENOMEM;
+	update = node->data;
+	update->ports = vlan->ports;
+	return 0;
+}  /* srp_update_vlan */
+
+static int drop_reservation(struct mrp_info *mrp, u8 port,
+	struct SRP_reservation *t_reserv, int active)
+{
+	u64 bandwidth;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct SRP_reservation *reserv;
+	struct SRP_stream *stream;
+	u8 tc;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	int result = 0;
+	struct mrp_node_anchor *list;
+	struct srp_stream_info *data;
+
+	if (active)
+		list = &info->active;
+	else
+		list = &info->passive;
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		data = next->data;
+		reserv = data->reserv;
+
+		if (reserv == t_reserv) {
+			if (active) {
+				stream = reserv->stream;
+				tc = data->tc;
+				tc = get_traffic_class(mrp, stream->priority);
+				bandwidth = calculate_bandwidth(stream->MaxFrameSize,
+					stream->MaxIntervalFrames, frames_per_sec(tc));
+
+				info->traffic[tc].bandwidth_left += bandwidth;
+				info->traffic[tc].bandwidth_used -= bandwidth;
+				info->bandwidth_left += bandwidth;
+
+				result = srp_update_mac(mrp, stream->dest,
+					1 << port, false);
+				if (!result)
+					result = srp_update_vlan(mrp,
+						stream->vlan_id, stream->dest,
+						1 << port, false);
+			}
+
+			/* Remove node from list. */
+			prev->next = next->next;
+			kfree(next);
+			result = 1;
+			break;
+		}
+		prev = next;
+		next = prev->next;
+	}
+	return result;
+}  /* drop_reservation */
+
+static void srp_cfg_mac(struct mrp_info *mrp, struct mrp_node_anchor *list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		prev->next = next->next;
+
+		mac = next->data;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports);
+
+		kfree(next->data);
+		kfree(next);
+
+		next = prev->next;
+	}
+	list->last = &list->anchor;
+}  /* srp_cfg_mac */
+
+static void srp_cfg_vlan(struct mrp_info *mrp, struct mrp_node_anchor *list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *vlan;
+	u16 ports;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		prev->next = next->next;
+
+		vlan = next->data;
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vlan->vid,
+			&vlan->index);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, ports);
+
+		kfree(next->data);
+		kfree(next);
+
+		next = prev->next;
+	}
+	list->last = &list->anchor;
+}  /* srp_cfg_vlan */
+
+static void srp_cfg_reservation(struct mrp_info *mrp)
+{
+	u8 port;
+	u8 tc;
+	struct mrp_port_info *info;
+
+	srp_cfg_mac(mrp, &mrp->mac_down);
+	srp_cfg_vlan(mrp, &mrp->vlan_down);
+	for (port = 0; port <= mrp->ports; port++) {
+		info = &mrp->port_info[port];
+		for (tc = SR_CLASS_A; tc <= SR_CLASS_B; tc++) {
+			if (info->traffic[tc].bandwidth_used !=
+			    info->traffic[tc].bandwidth_set) {
+				srp_cfg_credit_shaper(mrp, port, tc);
+				info->traffic[tc].bandwidth_set =
+					info->traffic[tc].bandwidth_used;
+			}
+		}
+	}
+	srp_cfg_mac(mrp, &mrp->mac_up);
+	srp_cfg_vlan(mrp, &mrp->vlan_up);
+}  /* srp_cfg_reservation */
+
+static struct mrp_node *create_stream_info(struct mrp_info *mrp,
+	struct SRP_reservation *t_reserv)
+{
+	struct mrp_node *node;
+	struct srp_stream_info *info;
+
+	node = kzalloc(sizeof(struct mrp_node), GFP_KERNEL);
+	if (!node)
+		return NULL;
+	info = kzalloc(sizeof(struct srp_stream_info), GFP_KERNEL);
+	if (!info) {
+		kfree(node);
+		return NULL;
+	}
+	prepare_stream_info(t_reserv,
+		get_traffic_class(mrp, t_reserv->stream->priority), info);
+	node->data = info;
+	return node;
+}  /* create_stream_info */
+
+static void add_reservation(struct mrp_port_info *info, int active,
+	struct mrp_node *node)
+{
+	struct mrp_node_anchor *list;
+	int (*cmp)(void *a, void *b);
+
+	if (active) {
+		list = &info->active;
+		cmp = cmp_higher_stream;
+	} else {
+		list = &info->passive;
+		cmp = cmp_lower_stream;
+	}
+	mrp_insert_node(list, cmp, node);
+}  /* add_reservation */
+
+static int mrp_update_listener(struct mrp_info *mrp,
+	struct SRP_reservation *l_reserv, struct SRP_stream *stream)
+{
+	int declaration;
+	int i;
+	struct SRP_reservation *reserv;
+	struct SRP_reservation *t_reserv;
+	struct mrp_port_info *info = &mrp->port_info[stream->in_port];
+	int result = DEV_IOC_OK;
+
+	if (!l_reserv)
+		l_reserv = srp_find_reservation(&info->declared, stream->id,
+			SRP_LISTENER);
+	if (!l_reserv)
+		return result;
+
+	declaration = 0;
+	for (i = 0; i <= mrp->ports; i++) {
+		if (i == stream->in_port)
+			continue;
+		info = &mrp->port_info[i];
+		reserv = srp_find_reservation(&info->registered,
+			stream->id, SRP_LISTENER);
+		t_reserv = srp_find_reservation(&info->declared,
+			stream->id, SRP_TALKER);
+		if (reserv && t_reserv) {
+			if (SRP_FAILED == t_reserv->declaration)
+				declaration |= SRP_ASKING_FAILED_SCALE;
+			else
+				declaration |= (1 << reserv->declaration);
+		}
+	}
+
+	/* No more listeners. */
+	if (!declaration) {
+dbg_msg("listener leaving\n");
+		if (!mrp->no_report) {
+			add_attrib_report(mrp, l_reserv, MRP_ACTION_LV,
+				MRP_TYPE_LISTENER, stream->in_port);
+			result = DEV_IOC_MRP_REPORT;
+		} else
+			srp_remove_reservation(l_reserv);
+		return result;
+	}
+
+	if (declaration > SRP_READY_SCALE)
+		declaration = SRP_READY_FAILED;
+	else if (declaration == SRP_READY_SCALE)
+		declaration = SRP_READY;
+	else
+		declaration = SRP_ASKING_FAILED;
+
+	/* Declaration is different. */
+	if (declaration != l_reserv->declaration) {
+		l_reserv->declaration = declaration;
+		if (!mrp->no_report) {
+			add_attrib_report(mrp, l_reserv, MRP_ACTION_TX,
+				MRP_TYPE_LISTENER, stream->in_port);
+
+			/* Ask application to retrieve attributes. */
+			result = DEV_IOC_MRP_REPORT;
+		}
+	}
+	return result;
+}  /* mrp_update_listener */
+
+static int drop_other_reservations(struct mrp_info *mrp, u8 port,
+	struct SRP_reservation *t_reserv, u32 required_bandwidth)
+{
+	u64 bandwidth;
+	u64 avail_bandwidth = 0;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_node *last = NULL;
+	struct SRP_reservation *reserv;
+	int cmp;
+	struct SRP_stream *stream;
+	u8 tc;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	struct srp_stream_info b;
+	struct srp_stream_info *data;
+	int result;
+
+	prepare_stream_info(t_reserv,
+		get_traffic_class(mrp, t_reserv->stream->priority), &b);
+
+	/* Go through the loop first to check if there are enough bandwidth. */
+	prev = &info->active.anchor;
+	next = prev->next;
+	while (next) {
+		data = next->data;
+		reserv = data->reserv;
+		tc = data->tc;
+
+		cmp = cmp_lower_stream(&b, next->data);
+
+		/* stream has higher priority. */
+		if (cmp < 0)
+			break;
+		stream = reserv->stream;
+		tc = get_traffic_class(mrp, stream->priority);
+		bandwidth = calculate_bandwidth(stream->MaxFrameSize,
+			stream->MaxIntervalFrames, frames_per_sec(tc));
+		avail_bandwidth += bandwidth;
+		if (avail_bandwidth >= required_bandwidth) {
+			last = next;
+			break;
+		}
+		prev = next;
+		next = prev->next;
+	}
+
+	/* No bandwidth at all for the new stream. */
+	if (avail_bandwidth < required_bandwidth)
+		return 0;
+
+	prev = &info->active.anchor;
+	next = prev->next;
+	while (next) {
+		data = next->data;
+		reserv = data->reserv;
+		tc = data->tc;
+
+		stream = reserv->stream;
+		bandwidth = calculate_bandwidth(stream->MaxFrameSize,
+			stream->MaxIntervalFrames, frames_per_sec(tc));
+
+		info->traffic[tc].bandwidth_left += bandwidth;
+		info->traffic[tc].bandwidth_used -= bandwidth;
+
+		/* Remove node from list. */
+		prev->next = next->next;
+
+		reserv->declaration = SRP_FAILED;
+		reserv->code = RFC_NO_BANDWIDTH;
+
+		reserv->pair->code = RFC_PREEMPTED_BY_RANK;
+
+		result = srp_update_mac(mrp, stream->dest, 1 << port, false);
+		if (!result)
+			result = srp_update_vlan(mrp,
+				stream->vlan_id, stream->dest, 1 << port,
+					false);
+
+		if (!mrp->no_report)
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX,
+				MRP_TYPE_TALKER, port);
+
+		mrp_update_listener(mrp, NULL, stream);
+
+		add_reservation(info, false, next);
+
+		/* Enough bandwidth freed. */
+		if (next == last)
+			break;
+		next = prev->next;
+	}
+
+	/* Increase bandwidth */
+	info->bandwidth_left += avail_bandwidth;
+	return 1;
+}  /* drop_other_reservations */
+
+static int start_passive_reservation(struct mrp_info *mrp, u8 port)
+{
+	u64 bandwidth;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct SRP_reservation *reserv;
+	struct SRP_stream *stream;
+	u8 tc;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	int started = 0;
+	int result;
+	struct srp_stream_info *data;
+
+	prev = &info->passive.anchor;
+	next = prev->next;
+	while (next) {
+		data = next->data;
+		reserv = data->reserv;
+
+		stream = reserv->stream;
+		tc = data->tc;
+		bandwidth = calculate_bandwidth(stream->MaxFrameSize,
+			stream->MaxIntervalFrames, frames_per_sec(tc));
+		if (bandwidth <= info->traffic[tc].bandwidth_left) {
+			info->traffic[tc].bandwidth_left -= bandwidth;
+			info->traffic[tc].bandwidth_used += bandwidth;
+			info->bandwidth_left -= bandwidth;
+
+			reserv->declaration = SRP_ADVERTISE;
+			reserv->code = RFC_NO_ERROR;
+
+			reserv->pair->code = RFC_NO_ERROR;
+
+			result = srp_update_mac(mrp, stream->dest, 1 << port,
+				true);
+			if (!result)
+				result = srp_update_vlan(mrp, stream->vlan_id,
+					stream->dest, 1 << port, true);
+
+			mrp_update_listener(mrp, NULL, stream);
+
+			if (!mrp->no_report) {
+				add_attrib_report(mrp, reserv, MRP_ACTION_TX,
+					MRP_TYPE_TALKER, port);
+				started = 1;
+			}
+
+			/* Remove node from list. */
+			prev->next = next->next;
+
+			add_reservation(info, true, next);
+
+			next = prev;
+		}
+		prev = next;
+		next = prev->next;
+	}
+	return started;
+}  /* start_passive_reservation */
+
+static int check_avail_bandwidth(struct mrp_info *mrp, u8 port,
+	struct SRP_stream *stream, struct SRP_reservation *t_reserv)
+{
+	u64 bandwidth;
+	u8 tc;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	int result;
+
+	tc = get_traffic_class(mrp, stream->priority);
+	bandwidth = calculate_bandwidth(stream->MaxFrameSize,
+		stream->MaxIntervalFrames, frames_per_sec(tc));
+dbg_msg("bw: %llu %u %u\n", bandwidth, info->bandwidth_left,
+info->traffic[tc].bandwidth_left);
+
+	if (bandwidth > info->traffic[tc].bandwidth_left) {
+		t_reserv->code = RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS;
+dbg_msg("tc %llu %u\n", bandwidth, info->traffic[tc].bandwidth_max);
+
+		/* See if this stream has higher rank. */
+		if (bandwidth <= info->traffic[tc].bandwidth_max) {
+dbg_msg("higher rank\n");
+			if (drop_other_reservations(mrp, port, t_reserv,
+					bandwidth))
+				t_reserv->code = RFC_NO_ERROR;
+		}
+	} else if (bandwidth > info->bandwidth_left) {
+		t_reserv->code = RFC_NO_BANDWIDTH;
+dbg_msg("port %llu %u\n", bandwidth, info->bandwidth_max);
+
+		/* See if the lower SR class has used all of the bandwidth. */
+		if (bandwidth <= info->bandwidth_max) {
+dbg_msg("higher SR class\n");
+			if (drop_other_reservations(mrp, port, t_reserv,
+					bandwidth))
+				t_reserv->code = RFC_NO_ERROR;
+		}
+	}
+
+	if (RFC_NO_ERROR == t_reserv->code) {
+		struct mrp_node *active;
+		u16 ports = 1 << port;
+
+		/* Reduce bandwidth */
+		info->bandwidth_left -= bandwidth;
+		info->traffic[tc].bandwidth_left -= bandwidth;
+		info->traffic[tc].bandwidth_used += bandwidth;
+		if (stream->MaxFrameSize > info->traffic[tc].max_frame_size)
+			info->traffic[tc].max_frame_size =
+				stream->MaxFrameSize;
+
+		active = create_stream_info(mrp, t_reserv);
+		if (!active)
+			return SRP_ASKING_FAILED;
+
+		add_reservation(info, true, active);
+
+		result = srp_update_mac(mrp, stream->dest, ports, true);
+		if (result)
+			return SRP_ASKING_FAILED;
+		result = srp_update_vlan(mrp,
+			stream->vlan_id, stream->dest, ports, true);
+		return SRP_READY;
+	} else {
+		struct mrp_node *passive;
+
+		t_reserv->declaration = SRP_FAILED;
+dbg_msg("no bw: %d\n", t_reserv->code);
+		if (!mrp->no_report)
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_TX,
+				MRP_TYPE_TALKER, port);
+		passive = create_stream_info(mrp, t_reserv);
+		if (passive)
+			add_reservation(info, false, passive);
+	}
+	return SRP_ASKING_FAILED;
+}  /* check_avail_bandwidth */
+
+static int proc_mrp_tx_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_reservation *l_reserv, struct SRP_reservation *t_reserv,
+	struct SRP_stream *stream)
+{
+	struct SRP_reservation *reserv;
+	int result;
+	int started = 0;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	u8 declaration = l_reserv->declaration;
+
+	/* Talker is not ready. */
+	if (SRP_FAILED == stream->t_reserv->declaration) {
+		declaration = SRP_ASKING_FAILED;
+	} else {
+
+		/* Find talker reservation if already created. */
+		if (!t_reserv)
+			t_reserv = srp_find_reservation(&info->declared,
+				l_reserv->id, SRP_TALKER);
+		if (!t_reserv)
+			declaration = SRP_ASKING_FAILED;
+		else {
+			if (SRP_FAILED == t_reserv->declaration)
+				declaration = SRP_ASKING_FAILED;
+			if (!t_reserv->pair) {
+				t_reserv->pair = l_reserv;
+				l_reserv->pair = t_reserv;
+			}
+		}
+	}
+
+	/* Check available bandwidth. */
+	if (declaration != SRP_ASKING_FAILED) {
+
+		/* Actual reservation is not made yet. */
+		if (RFC_NO_BANDWIDTH == l_reserv->code) {
+			declaration = check_avail_bandwidth(mrp, port, stream,
+				t_reserv);
+			if (declaration != SRP_ASKING_FAILED) {
+				l_reserv->code = RFC_NO_ERROR;
+				srp_cfg_reservation(mrp);
+			}
+		}
+	} else if (RFC_NO_BANDWIDTH != l_reserv->code) {
+
+		/* Listener is no longer ready to receive stream. */
+		if (!drop_reservation(mrp, port, t_reserv, true))
+			drop_reservation(mrp, port, t_reserv, false);
+		l_reserv->code = RFC_NO_BANDWIDTH;
+		started = start_passive_reservation(mrp, port);
+		srp_cfg_reservation(mrp);
+	}
+
+	info = &mrp->port_info[stream->in_port];
+	reserv = srp_find_reservation(&info->declared, l_reserv->id,
+		SRP_LISTENER);
+	if (!reserv) {
+
+		/* Listener declaration is not known yet. */
+		declaration = 0;
+		reserv = srp_create_reservation(l_reserv->id, SRP_LISTENER,
+			declaration, 0, mrp->id, 0);
+		if (!reserv)
+			return -ENOMEM;
+
+		reserv->stream = stream;
+		srp_insert_reservation(&info->declared, reserv);
+	}
+
+	/* Listener declaration will be updated in following call. */
+	result = mrp_update_listener(mrp, reserv, stream);
+	if (DEV_IOC_OK == result && started)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_tx_listener */
+
+static int proc_mrp_lv_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_listener *listener)
+{
+	struct SRP_reservation *l_reserv;
+	struct SRP_reservation *t_reserv;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	int declaration;
+	int result = DEV_IOC_OK;
+	int started = 0;
+
+dbg_msg("%s %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x %u\n", __func__, port,
+listener->id[0],
+listener->id[1],
+listener->id[2],
+listener->id[3],
+listener->id[4],
+listener->id[5],
+listener->id[6],
+listener->id[7],
+listener->substate);
+	l_reserv = srp_find_reservation(&info->registered, listener->id,
+		SRP_LISTENER);
+	if (!l_reserv)
+		return result;
+
+	/* Check if a talker reservation is using bandwidth. */
+	t_reserv = srp_find_reservation(&info->declared, listener->id,
+		SRP_TALKER);
+	if (!t_reserv) {
+
+		/* Delete listener reservation when leaving. */
+		srp_remove_reservation(l_reserv);
+		return result;
+	}
+
+	/* Remember the listener state. */
+	declaration = l_reserv->declaration;
+
+	t_reserv->pair = NULL;
+
+	/* Delete listener reservation when leaving. */
+	srp_remove_reservation(l_reserv);
+
+	/* Update the listener attribute reported back to talker. */
+	result = mrp_update_listener(mrp, NULL, t_reserv->stream);
+
+	/* Listener reservation is not using bandwidth if not ready. */
+	if (declaration != SRP_ASKING_FAILED) {
+		if (t_reserv->declaration != SRP_ADVERTISE) {
+
+			/* Look in passive list for reservation. */
+			if (drop_reservation(mrp, port, t_reserv, false)) {
+dbg_msg(" reset advert\n");
+				if (!mrp->no_report) {
+					t_reserv->declaration = SRP_ADVERTISE;
+					t_reserv->code = RFC_NO_ERROR;
+					add_attrib_report(mrp, t_reserv,
+						MRP_ACTION_TX,
+						MRP_TYPE_TALKER, port);
+				}
+			}
+		} else if (!drop_reservation(mrp, port, t_reserv, true))
+			drop_reservation(mrp, port, t_reserv, false);
+
+		started = start_passive_reservation(mrp, port);
+		srp_cfg_reservation(mrp);
+	}
+
+	if (DEV_IOC_OK == result && started)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv_listener */
+
+static int proc_mrp_rx_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_listener *listener)
+{
+	int result = DEV_IOC_OK;
+	u8 declaration = SRP_READY;
+	struct SRP_reservation *l_reserv;
+	struct SRP_stream *stream;
+	struct mrp_port_info *info = &mrp->port_info[port];
+
+dbg_msg("%s %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x %u\n", __func__, port,
+listener->id[0],
+listener->id[1],
+listener->id[2],
+listener->id[3],
+listener->id[4],
+listener->id[5],
+listener->id[6],
+listener->id[7],
+listener->substate);
+	if (SRP_IGNORED == listener->substate)
+		return DEV_IOC_INVALID_CMD;
+
+	declaration = listener->substate;
+	stream = srp_find_stream_id(mrp, listener->id);
+
+	/* Create reservation if new. */
+	l_reserv = srp_find_reservation(&info->registered, listener->id,
+		SRP_LISTENER);
+	if (!l_reserv) {
+		struct timespec ts;
+
+		l_reserv = srp_create_reservation(listener->id, SRP_LISTENER,
+			declaration, 0, mrp->id, RFC_NO_BANDWIDTH);
+		if (!l_reserv)
+			return -ENOMEM;
+
+		ts = ktime_to_timespec(ktime_get_real());
+		l_reserv->streamAge = ts.tv_sec;
+		srp_insert_reservation(&info->registered, l_reserv);
+	} else {
+		l_reserv->declaration = declaration;
+	}
+
+	/* Stream may not exist yet when listener is received. */
+	l_reserv->stream = stream;
+
+	/* There is a talker. */
+	if (stream && stream->t_reserv && stream->in_port != port) {
+		result = proc_mrp_tx_listener(mrp, port, l_reserv, NULL,
+			stream);
+	}
+	return result;
+}  /* proc_mrp_rx_listener */
+
+static int proc_mrp_tx_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker)
+{
+	u8 declaration;
+	u64 bandwidth;
+	u8 tc;
+	u8 code = talker->FailureCode;
+	u32 latency = talker->AccumulatedLatency;
+	struct SRP_stream *stream_id = NULL;
+	struct SRP_stream *stream_dest;
+	struct SRP_reservation *t_reserv;
+	struct SRP_reservation *l_reserv;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	int result = DEV_IOC_OK;
+
+	if (code != RFC_NO_ERROR)
+		goto get_talker_done;
+
+	tc = get_traffic_class(mrp, talker->priority);
+	if (port > mrp->ports)
+		code = RFC_PORT_IS_NOT_AVB;
+	else if (talker->MaxFrameSize > mrp->max_interference_size)
+		code = RFC_MAXFRAMESIZE_TOO_LARGE;
+	else if (!tc)
+		code = RFC_PRIORITY_IS_NOT_SR_CLASS;
+	if (code)
+		goto get_talker_done;
+
+	stream_id = srp_find_stream_id(mrp, talker->id);
+	stream_dest = srp_find_dest_addr(mrp, talker->dest);
+	if (stream_id && !stream_dest)
+		code = RFC_STREAM_ID_USED;
+	else if (stream_dest && !stream_id)
+		code = RFC_DEST_ADDR_USED;
+	if (code)
+		goto get_talker_done;
+
+	bandwidth = calculate_bandwidth(talker->MaxFrameSize,
+		talker->MaxIntervalFrames, frames_per_sec(tc));
+dbg_msg(" t bw: %llu %u %u\n", bandwidth, info->bandwidth_max,
+info->traffic[tc].bandwidth_max);
+	if (bandwidth > info->traffic[tc].bandwidth_max)
+		code = RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS;
+	else if (bandwidth > info->bandwidth_max)
+		code = RFC_NO_BANDWIDTH;
+
+get_talker_done:
+
+	/* Create reservation if new. */
+	t_reserv = srp_find_reservation(&info->declared, talker->id,
+		SRP_TALKER);
+	declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+	if (!t_reserv) {
+		struct timespec ts;
+
+		t_reserv = srp_create_reservation(talker->id, SRP_TALKER,
+			declaration, latency, mrp->id, code);
+		if (!t_reserv)
+			return -ENOMEM;
+
+		ts = ktime_to_timespec(ktime_get_real());
+		t_reserv->streamAge = ts.tv_sec;
+		t_reserv->stream = stream_id;
+		srp_insert_reservation(&info->declared, t_reserv);
+	} else {
+		t_reserv->declaration = declaration;
+		t_reserv->latency = latency;
+		memcpy(t_reserv->bridge_id, mrp->id, 8);
+		t_reserv->code = code;
+	}
+	if (!mrp->no_report) {
+		add_attrib_report(mrp, t_reserv, MRP_ACTION_TX,
+			MRP_TYPE_TALKER, port);
+		result = DEV_IOC_MRP_REPORT;
+	}
+
+	l_reserv = srp_find_reservation(&info->registered, talker->id,
+		SRP_LISTENER);
+	if (l_reserv) {
+		int rc;
+
+		rc = proc_mrp_tx_listener(mrp, port, l_reserv, t_reserv,
+			t_reserv->stream);
+	}
+
+	/* Ask application to retrieve attributes. */
+	return result;
+}  /* proc_mrp_tx_talker */
+
+static int proc_mrp_lv_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker)
+{
+	struct SRP_reservation *l_reserv;
+	struct SRP_reservation *reserv;
+	struct SRP_reservation *t_reserv;
+	struct mrp_port_info *info = &mrp->port_info[port];
+	int result = DEV_IOC_OK;
+		int p;
+	struct mrp_node *mac_node;
+	struct mrp_node *vlan_node;
+	struct mrp_vlan_info data;
+
+
+dbg_msg("%s %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x\n", __func__, port,
+talker->id[0],
+talker->id[1],
+talker->id[2],
+talker->id[3],
+talker->id[4],
+talker->id[5],
+talker->id[6],
+talker->id[7]);
+	reserv = srp_find_reservation(&info->registered, talker->id,
+		SRP_TALKER);
+	if (!reserv)
+		return result;
+
+	/* No listener propagation if no talker. */
+	l_reserv = srp_find_reservation(&info->declared, talker->id,
+		SRP_LISTENER);
+	if (l_reserv)
+		srp_remove_reservation(l_reserv);
+
+	for (p = 0; p <= mrp->ports; p++) {
+		if (p == port)
+			continue;
+
+		info = &mrp->port_info[p];
+		t_reserv = srp_find_reservation(&info->declared, talker->id,
+			SRP_TALKER);
+		if (!t_reserv)
+{
+dbg_msg("no talker reserv: %d\n", p);
+			continue;
+}
+
+		/* Unlink listener from talker. */
+		l_reserv = srp_find_reservation(&info->registered, talker->id,
+			SRP_LISTENER);
+		if (l_reserv) {
+			l_reserv->pair = NULL;
+
+			/* Drop reservation if listener is ready. */
+			if (l_reserv->declaration != SRP_ASKING_FAILED) {
+				drop_reservation(mrp, p, t_reserv, true);
+if (l_reserv->code != RFC_NO_ERROR)
+dbg_msg(" not ok? %d\n", l_reserv->code);
+				l_reserv->code = RFC_NO_BANDWIDTH;
+				start_passive_reservation(mrp, p);
+			} else
+				drop_reservation(mrp, p, t_reserv, false);
+		}
+
+		/* Reservation will be freed after reporting leaving. */
+		if (!mrp->no_report) {
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_LV,
+				MRP_TYPE_TALKER, p);
+			result = DEV_IOC_MRP_REPORT;
+		} else
+			srp_remove_reservation(t_reserv);
+	}
+
+	srp_cfg_reservation(mrp);
+
+	/* This is purely used to free the stream after reporting. */
+	if (!mrp->no_report)
+		add_attrib_report(mrp, reserv, MRP_ACTION_LV, MRP_TYPE_TALKER,
+			mrp->ports + 1);
+	else {
+		srp_remove_stream(mrp, reserv->stream);
+		srp_remove_reservation(reserv);
+	}
+
+	mac_node = mrp_find_node(&mrp->mac_list, cmp_mac, talker->dest);
+	if (mac_node) {
+		struct mrp_mac_info *mac = NULL;
+
+#ifdef DEBUG
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+		mac = mac_node->data;
+		mac->srp_ports &= ~(1 << 15);
+		mac->ports = mac->srp_ports | mac->mrp_ports;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports);
+		if (!mac->ports) {
+			mrp_free_mac(mrp, mac->index);
+			mrp_delete_node(&mrp->mac_list, cmp_mac, mac_node,
+				true);
+		}
+	}
+	data.vid = talker->vlan_id;
+	memcpy(data.addr, talker->dest, ETH_ALEN);
+	vlan_node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (vlan_node) {
+		u16 ports;
+		struct mrp_vlan_info *vlan = NULL;
+
+#ifdef DEBUG
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+		vlan = vlan_node->data;
+		vlan->ports &= ~(1 << 15);
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, ports);
+		if (!ports)
+			mrp_free_vlan(mrp, vlan->index);
+
+		/* Nobody is using the VLAN. */
+		if (!vlan->ports)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan,
+				vlan_node, true);
+	}
+	return result;
+}  /* proc_mrp_lv_talker */
+
+static int proc_mrp_rx_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker)
+{
+	int i;
+	int result = DEV_IOC_OK;
+	u8 declaration;
+	struct SRP_reservation *reserv;
+	struct SRP_stream *stream;
+	struct mrp_port_info *info = &mrp->port_info[port];
+
+dbg_msg("%s %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x\n", __func__, port,
+talker->id[0],
+talker->id[1],
+talker->id[2],
+talker->id[3],
+talker->id[4],
+talker->id[5],
+talker->id[6],
+talker->id[7]);
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x %04x %u %u:%u %u %u %u\n",
+talker->dest[0],
+talker->dest[1],
+talker->dest[2],
+talker->dest[3],
+talker->dest[4],
+talker->dest[5],
+talker->vlan_id, talker->priority, talker->rank,
+talker->MaxFrameSize, talker->MaxIntervalFrames, talker->AccumulatedLatency,
+talker->FailureCode);
+
+	/* Create stream if new. */
+	stream = srp_find_stream_id(mrp, talker->id);
+	if (!stream) {
+		stream = srp_create_stream(talker->id, talker->dest,
+			talker->vlan_id, talker->MaxFrameSize,
+			talker->MaxIntervalFrames, talker->priority,
+			talker->rank);
+		if (stream) {
+			srp_insert_stream_by_id(mrp, stream);
+			srp_insert_stream_by_dest(mrp, stream);
+		}
+	} else if (!memcmp(stream->dest, talker->dest, ETH_ALEN)) {
+		stream->vlan_id = talker->vlan_id;
+		stream->MaxFrameSize = talker->MaxFrameSize;
+		stream->MaxIntervalFrames = talker->MaxIntervalFrames;
+		stream->priority = talker->priority;
+		stream->rank = talker->rank;
+	} else
+		stream = NULL;
+	if (!stream)
+		return -ENOMEM;
+
+	/* Create reservation if new. */
+	reserv = srp_find_reservation(&info->registered, talker->id,
+		SRP_TALKER);
+	declaration = talker->FailureCode ? SRP_FAILED : SRP_ADVERTISE;
+	if (!reserv) {
+		struct mrp_node *node;
+		struct mrp_mac_info *mac;
+		struct mrp_vlan_info *vlan;
+		u16 ports = 1 << port;
+
+		reserv = srp_create_reservation(talker->id, SRP_TALKER,
+			declaration, talker->AccumulatedLatency,
+			talker->bridge_id, talker->FailureCode);
+		if (!reserv)
+			return -ENOMEM;
+
+		/* Indicate registration source. */
+		reserv->streamAge = 0;
+		srp_insert_reservation(&info->registered, reserv);
+		stream->in_port = port;
+		stream->t_reserv = reserv;
+
+		/* Setup initial MAC configuration. */
+		node = mrp_get_mac_info(&mrp->mac_list, talker->dest);
+		if (!node)
+			return -ENOMEM;
+		mac = node->data;
+		ports = mac->ports;
+		mac->srp_ports = (1 << 15);
+		mac->ports = mac->mrp_ports | mac->srp_ports;
+
+		/* First time setting up MAC table. */
+		if (!ports)
+			mac->index = mrp_alloc_mac(mrp);
+
+		/* Setup initial VLAN configuration. */
+		node = mrp_get_vlan_info(&mrp->vlan_list, talker->vlan_id,
+			talker->dest);
+		if (!node)
+			return -ENOMEM;
+		vlan = node->data;
+		ports = vlan->ports;
+		vlan->ports = (1 << 15);
+
+		/* First time setting up VLAN table. */
+		if (!ports)
+			vlan->index = mrp_alloc_vlan(mrp);
+#ifdef DEBUG
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	} else {
+		reserv->declaration = declaration;
+		reserv->latency = talker->AccumulatedLatency;
+		memcpy(reserv->bridge_id, talker->bridge_id, 8);
+		reserv->code = talker->FailureCode;
+		if (port == mrp->ports)
+			return result;
+	}
+	reserv->stream = stream;
+
+	for (i = 0; i <= mrp->ports; i++) {
+		if (i == port)
+			continue;
+		result = proc_mrp_tx_talker(mrp, i, talker);
+		if (result < 0)
+			return result;
+	}
+	return result;
+}  /* proc_mrp_rx_talker */
+
+static int proc_mrp_get_tx(struct mrp_info *mrp, struct mrp_cfg_options *cmd,
+	int *output)
+{
+	struct mrp_report *attrib;
+	struct SRP_reservation *reserv;
+	struct SRP_stream *stream;
+	int result = DEV_IOC_MRP_REPORT;
+
+	attrib = mrp->report_head;
+	if (!attrib)
+		return DEV_IOC_INVALID_CMD;
+
+	*output = 0;
+	cmd->action = attrib->action;
+	cmd->type = attrib->type;
+	cmd->port = attrib->port + 1;
+	if (MRP_TYPE_MAC == attrib->type) {
+		struct MRP_mac *mac = &cmd->data.mac;
+		struct mrp_node *node = attrib->attrib;
+		struct mrp_mac_info *info = node->data;
+
+		memcpy(mac->addr, info->addr, ETH_ALEN);
+		*output = SIZEOF_MRP_mac;
+		if (MRP_ACTION_LV == attrib->action &&
+		    attrib->port == mrp->ports &&
+		    !info->ports) {
+			kfree(info);
+			kfree(node);
+		}
+	} else if (MRP_TYPE_VLAN == attrib->type) {
+		struct MRP_vlan *vlan = &cmd->data.vlan;
+		struct mrp_node *node = attrib->attrib;
+		struct mrp_vlan_info *info = node->data;
+
+		vlan->id = info->vid;
+		*output = SIZEOF_MRP_vlan;
+		if (MRP_ACTION_LV == attrib->action &&
+		    attrib->port == mrp->ports &&
+		    !info->ports) {
+			kfree(info);
+			kfree(node);
+		}
+	} else if (MRP_TYPE_DOMAIN == attrib->type) {
+		struct SRP_domain_class *domain = &cmd->data.domain;
+
+		*domain = mrp->domain;
+		*output = SIZEOF_SRP_domain_class;
+	} else if (MRP_TYPE_LISTENER == attrib->type ||
+		   MRP_TYPE_TALKER == attrib->type) {
+		reserv = attrib->attrib;
+		if (SRP_TALKER == reserv->direction) {
+			struct SRP_talker *talker = &cmd->data.talker;
+
+			stream = reserv->stream;
+			memcpy(talker->id, stream->id, 8);
+			memcpy(talker->dest, stream->dest, ETH_ALEN);
+			talker->vlan_id = stream->vlan_id;
+			talker->MaxFrameSize = stream->MaxFrameSize;
+			talker->MaxIntervalFrames = stream->MaxIntervalFrames;
+			talker->priority = stream->priority;
+			talker->rank = stream->rank;
+			talker->reserved = 0;
+			talker->AccumulatedLatency = reserv->latency;
+			memcpy(talker->bridge_id, reserv->bridge_id, 8);
+			talker->FailureCode = reserv->code;
+			*output = SIZEOF_SRP_talker;
+		} else {
+			memcpy(cmd->data.listener.id, reserv->id, 8);
+			cmd->data.listener.substate = reserv->declaration;
+			*output = SIZEOF_SRP_listener;
+		}
+
+		/* Did receive Leave indication. */
+		if (MRP_ACTION_LV == attrib->action) {
+			struct mrp_report *next = attrib->next;
+
+			srp_remove_reservation(reserv);
+
+			/* Special one to free the stream. */
+			if (next && next->port == mrp->ports + 1) {
+				reserv = next->attrib;
+				srp_remove_stream(mrp, reserv->stream);
+				srp_remove_reservation(reserv);
+
+				kfree(attrib);
+				attrib = next;
+			}
+		}
+	}
+
+	/* Assume there are more attributes to report. */
+	mrp->report_head = attrib->next;
+	if (mrp->report_tail == attrib) {
+		mrp->report_tail = NULL;
+		result = DEV_IOC_OK;
+	}
+	kfree(attrib);
+	return result;
+}  /* proc_mrp_get_tx */
+
+static int proc_mrp_get_attribute(struct mrp_info *mrp, u8 *data, int *output)
+{
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	int result = DEV_IOC_OK;
+
+	switch (cmd->type) {
+	case MRP_TYPE_UNKNOWN:
+		result = proc_mrp_get_tx(mrp, cmd, output);
+		break;
+	case MRP_TYPE_MAC:
+		break;
+	case MRP_TYPE_VLAN:
+		break;
+	case MRP_TYPE_DOMAIN:
+		break;
+	case MRP_TYPE_LISTENER:
+		break;
+	case MRP_TYPE_TALKER:
+		break;
+	}
+	return result;
+}  /* proc_mrp_get_attribute */
+
+static int proc_mrp_set_attribute(struct mrp_info *mrp, u8 *data)
+{
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	u8 port = cmd->port;
+	int result = DEV_IOC_OK;
+
+	if (!port)
+		port = mrp->ports;
+	else
+		--port;
+	switch (cmd->type) {
+	case MRP_TYPE_MAC:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_mac(mrp, port, &cmd->data.mac);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_mac(mrp, port, &cmd->data.mac);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_mac(mrp, port, &cmd->data.mac);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_mac(mrp, port, &cmd->data.mac);
+			mrp->no_report = 0;
+		} else
+			mrp_show_node(&mrp->mac_list, show_mac_info);
+		break;
+	case MRP_TYPE_VLAN:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_vlan(mrp, port, &cmd->data.vlan);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_vlan(mrp, port, &cmd->data.vlan);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_vlan(mrp, port, &cmd->data.vlan);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_vlan(mrp, port, &cmd->data.vlan);
+			mrp->no_report = 0;
+		} else
+			mrp_show_node(&mrp->vlan_list, show_vlan_info);
+		break;
+	case MRP_TYPE_DOMAIN:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_domain(mrp, port,
+				&cmd->data.domain);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_domain(mrp, port,
+				&cmd->data.domain);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_domain(mrp, port,
+				&cmd->data.domain);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_domain(mrp, port,
+				&cmd->data.domain);
+			mrp->no_report = 0;
+		}
+		break;
+	case MRP_TYPE_LISTENER:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_listener(mrp, port,
+				&cmd->data.listener);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_listener(mrp, port,
+				&cmd->data.listener);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_listener(mrp, port,
+				&cmd->data.listener);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_listener(mrp, port,
+				&cmd->data.listener);
+			mrp->no_report = 0;
+		}
+		break;
+	case MRP_TYPE_TALKER:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_talker(mrp, port,
+				&cmd->data.talker);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_talker(mrp, port,
+				&cmd->data.talker);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_talker(mrp, port,
+				&cmd->data.talker);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_talker(mrp, port,
+				&cmd->data.talker);
+			mrp->no_report = 0;
+		} else {
+			int i;
+
+			for (i = 0; i <= mrp->ports; i++)
+				chk_reservation(&mrp->port_info[i], i);
+		}
+		break;
+	}
+	return result;
+}  /* proc_mrp_set_attribute */
+
+static void mrp_execute(struct mrp_info *mrp, struct work_struct *work)
+{
+	queue_work(mrp->access, work);
+}  /* mrp_execute */
+
+static int mrp_execute_wait(struct mrp_work *work)
+{
+	int rc = 0;
+
+	mrp_execute(work->mrp, &work->work);
+	wait_for_completion(&work->done);
+	return rc;
+}  /* mrp_execute_wait */
+
+static void proc_mrp_work(struct work_struct *work)
+{
+	struct mrp_work *parent =
+		container_of(work, struct mrp_work, work);
+	struct mrp_info *mrp = parent->mrp;
+	u8 *data = parent->param.data;
+	int result = DEV_IOC_OK;
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+			break;
+		case DEV_INFO_EXIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			result = proc_mrp_set_attribute(mrp, data);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			result = proc_mrp_get_attribute(mrp, data,
+				&parent->output);
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_mrp_work */
+
+static int proc_mrp_hw_access(struct mrp_info *mrp, int cmd, int subcmd,
+	int option, void *data, size_t len, int *output, int wait)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int ret = 0;
+
+	access = &mrp->hw_access;
+	work = &access->works[access->index];
+	if (work->used) {
+		pr_alert("work full\n");
+		return -EFAULT;
+	}
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->used = true;
+	access->index++;
+	access->index &= MRP_WORK_LAST;
+	init_completion(&work->done);
+	if (!wait) {
+		mrp_execute(mrp, &work->work);
+		goto hw_access_end;
+	}
+	ret = mrp_execute_wait(work);
+
+	/* Cannot continue if ERESTARTSYS. */
+	if (ret < 0)
+		return ret;
+
+	ret = work->result;
+	*output = work->output;
+	if (DEV_CMD_GET == work->cmd) {
+		int rc = ret;
+
+		if (DEV_IOC_MRP_REPORT == rc) {
+			rc = DEV_IOC_OK;
+			len = *output;
+		}
+		if (DEV_IOC_OK == rc)
+			memcpy(data, work->param.data, len);
+	}
+
+hw_access_end:
+	return ret;
+}  /* proc_mrp_hw_access */
+
+static void init_mrp_work(struct mrp_info *mrp)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int i;
+
+	access = &mrp->hw_access;
+	for (i = 0; i < MRP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->mrp = mrp;
+		INIT_WORK(&work->work, proc_mrp_work);
+		init_completion(&work->done);
+	}
+}  /* init_mrp_work */
+
+
+
+static int mrp_dev_req(struct mrp_info *mrp, char *arg)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	int err = 0;
+	int result = 0;
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) &req->param;
+	size_t param_size;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+
+	switch (cmd->type) {
+	case MRP_TYPE_MAC:
+		param_size = SIZEOF_MRP_mac;
+		break;
+	case MRP_TYPE_VLAN:
+		param_size = SIZEOF_MRP_vlan;
+		break;
+	case MRP_TYPE_DOMAIN:
+		param_size = SIZEOF_SRP_domain_class;
+		break;
+	case MRP_TYPE_LISTENER:
+		param_size = SIZEOF_SRP_listener;
+		break;
+	default:
+		param_size = SIZEOF_SRP_talker;
+		break;
+	}
+
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = mrp->version;
+				data[5] = mrp->ports;
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+						6) ||
+						copy_to_user(req->param.data,
+						data, 6)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+				result = proc_mrp_hw_access(mrp,
+					maincmd, subcmd, 0,
+					data, 6, &output, true);
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, 0, &output, true);
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			if (chk_ioctl_size(len, param_size,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, len, &output, true);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			if (chk_ioctl_size(len, param_size,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, len, &output, true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					param_size) ||
+					copy_to_user(req->param.data, data,
+					param_size)) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* mrp_dev_req */
+
+static u32 calculate_max_bandwidth(u32 speed, u32 percent)
+{
+	u32 bandwidth;
+
+	bandwidth = speed;
+	bandwidth *= percent;
+	bandwidth /= 100;
+	bandwidth *= NETWORK_SPEED_IN_MBIT;
+	return bandwidth;
+}  /* calculate_max_bandwidth */
+
+static void mrp_set_bandwidth(struct mrp_port_info *info)
+{
+	u32 bandwidth;
+
+	bandwidth = calculate_max_bandwidth(info->speed,
+		info->bandwidth[SR_CLASS_A].deltaBandwidth +
+		info->bandwidth[SR_CLASS_B].deltaBandwidth);
+	info->bandwidth_max = bandwidth;
+	info->bandwidth_left = bandwidth;
+
+	bandwidth = calculate_max_bandwidth(info->speed,
+		info->bandwidth[SR_CLASS_A].deltaBandwidth);
+	info->traffic[SR_CLASS_A].bandwidth_max = bandwidth;
+	info->traffic[SR_CLASS_A].bandwidth_left = bandwidth;
+
+	bandwidth = calculate_max_bandwidth(info->speed,
+		info->bandwidth[SR_CLASS_B].deltaBandwidth);
+	bandwidth += info->traffic[SR_CLASS_A].bandwidth_left;
+	info->traffic[SR_CLASS_B].bandwidth_max = bandwidth;
+	info->traffic[SR_CLASS_B].bandwidth_left = bandwidth;
+}  /* mrp_set_bandwidth */
+
+static void mrp_set_speed(struct mrp_info *mrp, int port, u32 speed)
+{
+	struct mrp_port_info *info = &mrp->port_info[port];
+
+	if (speed != info->speed) {
+		if (speed) {
+			info->speed = speed;
+			mrp_set_bandwidth(info);
+		}
+	}
+}  /* mrp_set_speed */
+
+static void mrp_init(struct mrp_info *mrp)
+{
+	u8 port;
+	u8 tc;
+	struct mrp_port_info *info;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	mrp->access = create_singlethread_workqueue("mrp_access");
+	init_mrp_work(mrp);
+	mutex_init(&mrp->lock);
+
+	mrp->ports = sw->mib_port_cnt - 1;
+
+	mrp->id[0] = 0x01;
+	mrp->id[1] = 0x80;
+	mrp->id[2] = 0xA6;
+	mrp->id[3] = 0x98;
+	mrp->id[4] = 0x97;
+	mrp->id[5] = 0x01;
+	mrp->id[6] = 0x01;
+	mrp->id[7] = 0x01;
+	mrp->max_interference_size = 9000;
+	mrp->tc[2] = SR_CLASS_B;
+	mrp->tc[3] = SR_CLASS_A;
+	mrp->prio[SR_CLASS_A] = 3;
+	mrp->prio[SR_CLASS_B] = 2;
+
+	mrp_init_list(&mrp->mac_down);
+	mrp_init_list(&mrp->mac_up);
+	mrp_init_list(&mrp->vlan_down);
+	mrp_init_list(&mrp->vlan_up);
+
+	mrp_init_list(&mrp->mac_list);
+	mrp_init_list(&mrp->vlan_list);
+
+	for (port = 0; port <= mrp->ports; port++) {
+		info = &mrp->port_info[port];
+		info->speed = 100;
+
+		info->bandwidth[SR_CLASS_A].traffic_class = 3;
+		info->bandwidth[SR_CLASS_A].deltaBandwidth = 50;
+		info->bandwidth[SR_CLASS_A].adminIdleSlope = 0;
+
+		info->bandwidth[SR_CLASS_B].traffic_class = 2;
+		info->bandwidth[SR_CLASS_B].deltaBandwidth = 25;
+		info->bandwidth[SR_CLASS_B].adminIdleSlope = 0;
+
+		for (tc = 0; tc < 4; tc++)
+			info->algorithm[tc].traffic_class = tc;
+		info->algorithm[2].algorithm = 1;
+		info->algorithm[3].algorithm = 1;
+
+		mrp_set_bandwidth(info);
+dbg_msg("bw: %d %u; %u %u; %u %u\n", port, info->bandwidth_left,
+	info->traffic[SR_CLASS_A].bandwidth_max,
+	info->traffic[SR_CLASS_A].bandwidth_left,
+	info->traffic[SR_CLASS_B].bandwidth_max,
+	info->traffic[SR_CLASS_B].bandwidth_left);
+
+		mrp_init_list(&info->active);
+		mrp_init_list(&info->passive);
+#ifdef CONFIG_HAVE_KSZ9897
+	do {
+		int queue = 3;
+		u16 credit;
+		u16 credit_hi;
+		u16 credit_lo;
+		u8 schedule;
+		u8 shaping;
+
+		sw->reg->lock(sw);
+		credit_hi = port_get_hi_water_mark(sw, port, queue);
+		credit_lo = port_get_lo_water_mark(sw, port, queue);
+		credit = port_get_increment(sw, port, queue);
+		schedule = port_get_schedule_mode(sw, port, queue);
+		shaping = port_get_shaping(sw, port, queue);
+		for (queue = 0; queue < 4; queue++)
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_STRICT_PRIO);
+		sw->reg->unlock(sw);
+dbg_msg("%d %x %x %x %u %u\n", port,
+credit, credit_hi, credit_lo, schedule, shaping);
+	} while (0);
+#endif
+	}
+}  /* mrp_init */
+
+static void mrp_exit(struct mrp_info *mrp)
+{
+	if (mrp->access) {
+		destroy_workqueue(mrp->access);
+		mrp->access = NULL;
+	}
+}  /* mrp_exit */
+
+static struct mrp_ops mrp_ops = {
+	.init			= mrp_init,
+	.exit			= mrp_exit,
+
+	.dev_req		= mrp_dev_req,
+};
+
diff --git a/drivers/net/ethernet/micrel/ksz_mrp.h b/drivers/net/ethernet/micrel/ksz_mrp.h
new file mode 100644
index 0000000..e77f3ae
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_mrp.h
@@ -0,0 +1,247 @@
+/**
+ * Micrel MRP driver header
+ *
+ * Copyright (c) 2015 Microchp Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MRP_H
+#define KSZ_MRP_H
+
+#include "ksz_mrp_api.h"
+
+
+struct SRP_bridge_base {
+	uint msrpEnabledStatus:1;
+	uint msrpTalkerPruning:1;
+	uint msrpMaxFanInPorts;
+	uint msrpLatencyMaxFrameSize;
+};
+
+struct SRP_bridge_port {
+	uint msrpPortEnabledStatus:1;
+	uint Failed_Registrations;
+	u8 Last_PDU_Origin[ETH_ALEN];
+	u16 SR_PVID;
+};
+
+struct SRP_latency_parameter {
+	u8 traffic_class;
+	u32 portTcMaxLatency;
+};
+
+struct SRP_reservation;
+
+struct SRP_stream {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 MaxFrameSize;
+	u16 MaxIntervalFrames;
+	u8 priority:3;
+	u8 rank:1;
+	u8 reserved:4;
+
+	u8 in_port;
+	struct SRP_reservation *t_reserv;
+
+	struct SRP_stream *id_prev;
+	struct SRP_stream *id_next;
+	struct SRP_stream *dest_prev;
+	struct SRP_stream *dest_next;
+};
+
+enum {
+	SRP_TALKER,
+	SRP_LISTENER,
+};
+
+#define SRP_ASKING_FAILED_SCALE		(1 << SRP_ASKING_FAILED)
+#define SRP_READY_SCALE			(1 << SRP_READY)
+#define SRP_READY_FAILED_SCALE		(1 << SRP_READY_FAILED)
+
+struct SRP_reservation {
+	u8 id[8];
+	u8 direction;
+	u8 declaration;
+	u32 latency;
+	u8 bridge_id[8];
+	u8 code;
+
+	uint dropped_frames;
+	uint streamAge;
+
+	struct SRP_stream *stream;
+	struct SRP_reservation *pair;
+
+	struct SRP_reservation *next;
+	struct SRP_reservation *prev;
+};
+
+struct SRP_bandwidth_availability_parameter {
+	u8 traffic_class;
+	int deltaBandwidth;
+	u32 adminIdleSlope;
+	u32 operIdleSlope;
+};
+
+struct SRP_transmission_selection_algorithm {
+	u8 traffic_class;
+	int algorithm;
+};
+
+struct SRP_priority_regeneration_override {
+	u8 received_priority;
+	u8 regenerated_priority;
+	int SRPdomainBoundaryPort;
+};
+
+struct mrp_traffic_info {
+	u32 bandwidth_max;
+	u32 bandwidth_left;
+	u32 bandwidth_used;
+	u32 bandwidth_set;
+	u32 max_frame_size;
+	u8 prio;
+};
+
+struct mrp_report {
+	void *attrib;
+	u8 action;
+	u8 type;
+	u8 port;
+	struct mrp_report *next;
+};
+
+struct mrp_mac_info {
+	u8 addr[ETH_ALEN];
+	u16 ports;
+	u16 mrp_ports;
+	u16 srp_ports;
+	u16 tx_ports;
+	u8 index;
+};
+
+struct mrp_vlan_info {
+	u16 vid;
+	u8 addr[ETH_ALEN];
+	u16 ports;
+	u16 tx_ports;
+	u8 index;
+};
+
+struct srp_stream_info {
+	struct SRP_reservation *reserv;
+	u8 *id;
+	u32 age;
+	u8 rank;
+	u8 tc;
+};
+
+struct mrp_node {
+	void *data;
+	struct mrp_node *next;
+};
+
+struct mrp_node_anchor {
+	struct mrp_node anchor;
+	struct mrp_node *last;
+};
+	
+struct mrp_port_info {
+	u32 bandwidth_max;
+	u32 bandwidth_left;
+	u32 speed;
+	struct mrp_traffic_info traffic[3];
+
+	struct SRP_bridge_port status;
+	struct SRP_latency_parameter latency[2];
+	struct SRP_bandwidth_availability_parameter bandwidth[3];
+	struct SRP_transmission_selection_algorithm algorithm[4];
+	struct SRP_priority_regeneration_override priority[8];
+
+	struct SRP_reservation declared;
+	struct SRP_reservation registered;
+
+	struct mrp_node_anchor active;
+	struct mrp_node_anchor passive;
+};
+
+struct mrp_info;
+
+struct mrp_work {
+	struct work_struct work;
+	struct completion done;
+	struct mrp_info *mrp;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	union {
+		struct mrp_cfg_options cfg;
+		u8 data[8];
+	} param;
+};
+
+#define MRP_WORK_NUM			(1 << 4)
+#define MRP_WORK_LAST			(MRP_WORK_NUM - 1)
+
+struct mrp_access {
+	int index;
+	struct mrp_work works[MRP_WORK_NUM];
+};
+
+struct mrp_ops {
+	void (*init)(struct mrp_info *ptp);
+	void (*exit)(struct mrp_info *ptp);
+	int (*dev_req)(struct mrp_info *ptp, char *arg);
+};
+
+struct mrp_info {
+	struct mutex lock;
+	struct mrp_access hw_access;
+	struct workqueue_struct *access;
+	u8 version;
+	u8 ports;
+	uint no_report;
+
+	const struct mrp_ops *ops;
+
+	u8 id[8];
+	u8 tc[8];
+	u8 prio[4];
+	u32 max_interference_size;
+
+	struct mrp_port_info port_info[TOTAL_PORT_NUM];
+
+	struct SRP_domain_class domain;
+
+	struct SRP_stream stream_by_id;
+	struct SRP_stream stream_by_dest;
+
+	struct mrp_node_anchor mac_down;
+	struct mrp_node_anchor mac_up;
+	struct mrp_node_anchor vlan_down;
+	struct mrp_node_anchor vlan_up;
+	struct mrp_node_anchor mac_list;
+	struct mrp_node_anchor vlan_list;
+
+	struct mrp_report *report_head;
+	struct mrp_report *report_tail;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_mrp_api.h b/drivers/net/ethernet/micrel/ksz_mrp_api.h
new file mode 100644
index 0000000..661a171
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_mrp_api.h
@@ -0,0 +1,184 @@
+/**
+ * Micrel MRP driver API header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MRP_API_H
+#define KSZ_MRP_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN			6
+#endif
+
+
+enum {
+	DEV_IOC_MRP_REPORT = DEV_IOC_LAST,
+};
+
+enum {
+	DEV_MRP_ATTRIBUTE,
+};
+
+struct MRP_mac {
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct MRP_vlan {
+	u16 id;
+} __packed;
+
+enum {
+	SRP_IGNORED,
+	SRP_ASKING_FAILED,
+	SRP_READY,
+	SRP_READY_FAILED,
+	SRP_ADVERTISE,
+	SRP_FAILED,
+};
+
+struct SRP_listener {
+	u8 id[8];
+	u8 substate;
+} __packed;
+
+struct SRP_talker {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 MaxFrameSize;
+	u16 MaxIntervalFrames;
+	u8 priority:3;
+	u8 rank:1;
+	u8 reserved:4;
+	u32 AccumulatedLatency;
+	u8 bridge_id[8];
+	u8 FailureCode;
+} __packed;
+
+enum {
+	RFC_NO_ERROR,
+	RFC_NO_BANDWIDTH,
+	RFC_NO_RESOURCES,
+	RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS,
+	RFC_STREAM_ID_USED,
+	RFC_DEST_ADDR_USED,
+	RFC_PREEMPTED_BY_RANK,
+	RFC_LATENCY_CHANGED,
+	RFC_PORT_IS_NOT_AVB,
+	RFC_USE_DIFF_DEST_ADDR,
+	RFC_OUT_OF_MSRP_RESOURCE,
+	RFC_OUT_OF_MMRP_RESOURCE,
+	RFC_CANNOT_STORE_DEST_ADDR,
+	RFC_PRIORITY_IS_NOT_SR_CLASS,
+	RFC_MAXFRAMESIZE_TOO_LARGE,
+	RFC_MAXFANINPORTS_LIMIT_REACHED,
+	RFC_FIRSTVALUE_CHANGED,
+	RFC_VLAN_BLOCKED,
+	RFC_VLAN_TAGGING_DISABLED,
+	RFC_SR_CLASS_PRIORITY_MISMATCHED,
+};
+
+enum {
+	SR_CLASS_UNDEFINED,
+	SR_CLASS_A,
+	SR_CLASS_B,
+};
+
+#define SR_CLASS_A_ID	6
+#define SR_CLASS_B_ID	5
+
+struct SRP_domain_class {
+	u8 id;
+	u8 priority;
+	u16 vlan_id;
+} __packed;
+
+union mrp_data {
+	struct MRP_mac mac;
+	struct MRP_vlan vlan;
+	struct SRP_talker talker;
+	struct SRP_listener listener;
+	struct SRP_domain_class domain;
+} __packed;
+
+enum {
+	MRP_ACTION_RX,
+	MRP_ACTION_TX,
+	MRP_ACTION_LV,
+	MRP_ACTION_ON,
+	MRP_ACTION_OFF,
+
+	MRP_ACTION_DBG,
+};
+
+enum {
+	MRP_TYPE_UNKNOWN,
+	MRP_TYPE_MAC,
+	MRP_TYPE_VLAN,
+	MRP_TYPE_TALKER,
+	MRP_TYPE_LISTENER,
+	MRP_TYPE_DOMAIN,
+};
+
+struct mrp_cfg_options {
+	u8 action;
+	u8 type;
+	u8 port;
+	u8 reserved;
+	union mrp_data data;
+} __packed;
+
+#define SIZEOF_MRP_mac	\
+	(sizeof(struct MRP_mac) + sizeof(u8) * 4)
+
+#define SIZEOF_MRP_vlan	\
+	(sizeof(struct MRP_vlan) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_talker	\
+	(sizeof(struct SRP_talker) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_listener	\
+	(sizeof(struct SRP_listener) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_domain_class	\
+	(sizeof(struct SRP_domain_class) + sizeof(u8) * 4)
+
+
+int set_mac_lv(void *fd,
+	int port, struct MRP_mac *mac);
+int set_mac_rx(void *fd,
+	int port, struct MRP_mac *mac);
+int set_vlan_lv(void *fd,
+	int port, struct MRP_vlan *vlan);
+int set_vlan_rx(void *fd,
+	int port, struct MRP_vlan *vlan);
+int set_domain_lv(void *fd,
+	int port, struct SRP_domain_class *domain);
+int set_domain_rx(void *fd,
+	int port, struct SRP_domain_class *domain);
+int set_listener_lv(void *fd,
+	int port, struct SRP_listener *listener);
+int set_listener_rx(void *fd,
+	int port, struct SRP_listener *listener);
+int set_talker_lv(void *fd,
+	int port, struct SRP_talker *talker);
+int set_talker_rx(void *fd,
+	int port, struct SRP_talker *talker);
+int get_attribute(void *fd,
+	int *port, int *type, int *action, void *buf, size_t size);
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_ptp_9897.c b/drivers/net/ethernet/micrel/ksz_ptp_9897.c
new file mode 100644
index 0000000..494ef65
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_ptp_9897.c
@@ -0,0 +1,6497 @@
+/**
+ * Micrel PTP common code
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 0
+#define ENABLE_10_MHZ_CLK
+#endif
+
+
+#define FMT_NSEC_SIZE			12
+
+static char *format_nsec(char *str, u32 nsec)
+{
+	u32 nsec0;
+	u32 nsec1;
+	u32 nsec2;
+	char str0[4];
+
+	nsec0 = nsec % 1000;
+	nsec1 = (nsec / 1000) % 1000;
+	nsec2 = (nsec / 1000000) % 1000;
+	sprintf(str0, "%03u", nsec0);
+	if (nsec2)
+		sprintf(str, "%3u.%03u.%s", nsec2, nsec1, str0);
+	else if (nsec1)
+		sprintf(str, "    %3u.%s", nsec1, str0);
+	else
+		sprintf(str, "        %3u", nsec0);
+	return str;
+}  /* format_nsec */
+
+struct pseudo_iphdr {
+	__u8 ttl;
+	__u8 protocol;
+	__be16 tot_len;
+	__be32 saddr;
+	__be32 daddr;
+};
+
+struct pseudo_ip6hdr {
+	__be16 payload_len;
+	__u8 hop_limit;
+	__u8 nexthdr;
+	struct in6_addr saddr;
+	struct in6_addr daddr;
+};
+
+static u32 timestamp_val(u32 timestamp, u8 *sec)
+{
+	*sec = timestamp >> 30;
+	timestamp <<= 2;
+	timestamp >>= 2;
+	return timestamp;
+}  /* timestamp_val */
+
+static void calc_diff(struct ksz_ptp_time *prev, struct ksz_ptp_time *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time diff;
+	int prev_nsec = prev->nsec;
+	int cur_nsec = cur->nsec;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= NANOSEC_IN_SEC) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	} else if (diff.nsec <= -NANOSEC_IN_SEC) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff */
+
+static void calc_udiff(struct ptp_utime *prev, struct ptp_utime *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time t1;
+	struct ksz_ptp_time t2;
+
+	if (prev->sec > (1UL << 31) || cur->sec > (1UL << 31)) {
+		s64 t3;
+		s64 t4;
+		s64 diff;
+		s32 rem;
+
+		t3 = (u64) prev->sec * NANOSEC_IN_SEC + prev->nsec;
+		t4 = (u64) cur->sec * NANOSEC_IN_SEC + cur->nsec;
+		diff = t4 - t3;
+		t3 = div_s64_rem(diff, NSEC_PER_SEC, &rem);
+		result->sec = (s32) t3;
+		result->nsec = rem;
+		return;
+	}
+	t1.sec = prev->sec;
+	t1.nsec = prev->nsec;
+	t2.sec = cur->sec;
+	t2.nsec = cur->nsec;
+	calc_diff(&t1, &t2, result);
+}  /* calc_udiff */
+
+#ifdef PTP_PROCESS
+static void calc_diff64(struct ptp_ltime *prev, struct ptp_ltime *cur,
+	struct ptp_ltime *result)
+{
+	struct ptp_ltime diff;
+	s64 prev_nsec = prev->nsec;
+	s64 cur_nsec = cur->nsec;
+	s64 scaled_nsec = (s64) NANOSEC_IN_SEC << SCALED_NANOSEC_S;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= scaled_nsec) {
+		diff.nsec -= scaled_nsec;
+		diff.sec++;
+	} else if (diff.nsec <= -scaled_nsec) {
+		diff.nsec += scaled_nsec;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += scaled_nsec;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= scaled_nsec;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff64 */
+#endif
+
+static void ptp_write_index(struct ptp_info *ptp, int shift, u8 unit)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+#ifndef USE_OLD_PTP_UNIT_INDEX
+	u32 index = sw->cached.ptp_unit_index;
+
+	index &= ~(PTP_UNIT_M << shift);
+	index |= (u32) unit << shift;
+	sw->cached.ptp_unit_index = index;
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, index);
+#else
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, unit);
+#endif
+}  /* ptp_write_index */
+
+static void add_nsec(struct ptp_utime *t, u32 nsec)
+{
+	t->nsec += nsec;
+	if (t->nsec >= NANOSEC_IN_SEC) {
+		t->nsec -= NANOSEC_IN_SEC;
+		t->sec++;
+	}
+}  /* add_nsec */
+
+static void sub_nsec(struct ptp_utime *t, u32 nsec)
+{
+	if (t->nsec < nsec) {
+		t->nsec += NANOSEC_IN_SEC;
+		t->sec--;
+	}
+	t->nsec -= nsec;
+}  /* sub_nsec */
+
+static void update_ts(struct ptp_ts *ts, u32 cur_sec)
+{
+	int sec;
+	u8 sec_chk;
+
+	ts->t.nsec = timestamp_val(ts->timestamp, &sec_chk);
+	if (ts->timestamp)
+		sec = (cur_sec - sec_chk) & 3;
+	else
+		sec = 0;
+	if (sec >= 3)
+		sec -= 4;
+	ts->t.sec = cur_sec - sec;
+}  /* update_ts */
+
+#define INIT_NSEC			40
+#define MIN_CYCLE_NSEC			8
+#define MIN_GAP_NSEC			120
+#define PULSE_NSEC			8
+
+static int check_cascade(struct ptp_info *ptp, int first, int total,
+	u16 *repeat, u32 sec, u32 nsec)
+{
+	struct ptp_output *cur;
+	struct ptp_output *next;
+	struct ptp_output *prev;
+	int diff;
+	int i;
+	int tso;
+	int min_cnt;
+	int cnt;
+
+	tso = first;
+	cur = &ptp->outputs[tso];
+	next = &ptp->outputs[first + total];
+	next->start = cur->start;
+	add_nsec(&next->start, cur->iterate);
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->stop = cur->start;
+		add_nsec(&cur->stop, cur->len);
+		next = &ptp->outputs[tso + 1];
+		calc_udiff(&cur->stop, &next->start, &cur->gap);
+		if ((cur->gap.sec < 0 || (!cur->gap.sec && cur->gap.nsec < 0))
+				&& (i < total - 1 || 1 != *repeat)) {
+			dbg_msg("gap too small: %d=%d\n", i, cur->gap.nsec);
+			return 1;
+		}
+	}
+	if (1 == *repeat)
+		goto check_cascade_done;
+
+	min_cnt = *repeat;
+	tso = first + 1;
+	for (i = 1; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		prev = &ptp->outputs[tso - 1];
+		if (cur->iterate < prev->iterate) {
+			diff = prev->iterate - cur->iterate;
+			cnt = prev->gap.nsec / diff + 1;
+		} else if (cur->iterate > prev->iterate) {
+			diff = cur->iterate - prev->iterate;
+			cnt = cur->gap.nsec / diff + 1;
+		} else
+			cnt = *repeat;
+		if (min_cnt > cnt)
+			min_cnt = cnt;
+	}
+	if (*repeat > min_cnt)
+		*repeat = min_cnt;
+	prev = &ptp->outputs[first + tso];
+	for (cnt = 0; cnt < *repeat; cnt++) {
+		tso = first;
+		for (i = 0; i < total; i++, tso++) {
+			cur = &ptp->outputs[tso];
+			next = &ptp->outputs[tso + 1];
+			dbg_msg("%d: %d:%9d %d %d:%9d %d: %d:%9d\n",
+				i, cur->start.sec, cur->start.nsec, cur->len,
+				cur->gap.sec, cur->gap.nsec, cur->iterate,
+				cur->stop.sec, cur->stop.nsec);
+			if (cur->stop.sec > next->start.sec ||
+					(cur->stop.sec == next->start.sec &&
+					cur->stop.nsec > next->stop.nsec))
+				dbg_msg("> %d %d:%9d %d:%9d\n", i,
+					cur->stop.sec, cur->stop.nsec,
+					next->start.sec, next->start.nsec);
+			add_nsec(&cur->start, cur->iterate);
+			cur->stop = cur->start;
+			add_nsec(&cur->stop, cur->len);
+			if (!i)
+				prev->start = cur->start;
+		}
+		dbg_msg("%d:%9d\n", prev->start.sec, prev->start.nsec);
+	}
+
+check_cascade_done:
+	tso = first;
+	cur = &ptp->outputs[tso];
+	if (cur->trig.sec >= sec)
+		return 0;
+
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->trig.sec += sec;
+		add_nsec(&cur->trig, nsec);
+	}
+	return 0;
+}
+
+#define MAX_DRIFT_CORR			6250000
+#define LOW_DRIFT_CORR			2499981
+#define MAX_U32_S			32
+#define MAX_DIVIDER_S			31
+
+static u32 drift_in_sec(u32 abs_offset, u64 interval64)
+{
+	u64 drift64;
+
+	drift64 = abs_offset;
+	drift64 *= NANOSEC_IN_SEC;
+	drift64 = div64_u64(drift64, interval64);
+	return (u32) drift64;
+}
+
+static u32 clk_adjust_val(int diff, u32 interval)
+{
+	u32 adjust;
+	u32 rem;
+	u64 adjust64;
+
+	if (0 == diff)
+		return 0;
+	if (diff < 0)
+		adjust = -diff;
+	else
+		adjust = diff;
+
+	/* 2^32 * adjust * 1000000000 / interval / 25000000 */
+	if (interval != NANOSEC_IN_SEC)
+		adjust = drift_in_sec(adjust, interval);
+
+	if (adjust >= MAX_DRIFT_CORR)
+		adjust = 0x3fffffff;
+	else {
+		adjust64 = 1LL << 32;
+		adjust64 *= adjust;
+		adjust64 = div_u64_rem(adjust64, 25000000, &rem);
+		adjust = (u32) adjust64;
+		if (adjust >= 0x40000000)
+			adjust = 0x3fffffff;
+	}
+	if (diff < 0)
+		adjust |= PTP_RATE_DIR;
+	return adjust;
+}  /* clk_adjust_val */
+
+static void ptp_tso_off(struct ptp_info *ptp, u8 tso, u16 tso_bit)
+{
+	ptp->reg->tx_off(ptp, tso);
+	ptp->tso_intr &= ~tso_bit;
+	ptp->tso_used &= ~tso_bit;
+	if (ptp->tso_sys & tso_bit) {
+		printk(KERN_INFO "tso %d off!\n", tso);
+		ptp->tso_sys &= ~tso_bit;
+	}
+	ptp->tso_dev[tso] = NULL;
+}  /* ptp_tso_off */
+
+static inline void ptp_tx_reset(struct ptp_info *ptp, u8 tso, u32 *ctrl_ptr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (!ctrl_ptr) {
+		ctrl_ptr = &ctrl;
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		*ctrl_ptr = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	}
+	*ctrl_ptr &= ~TS_RESET;
+	*ctrl_ptr |= TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+	*ctrl_ptr &= ~TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+}  /* ptp_tx_reset */
+
+static inline void ptp_gpo_reset(struct ptp_info *ptp, int gpo, u8 tso,
+	u32 *ctrl_ptr)
+{
+	ptp_tx_reset(ptp, tso, ctrl_ptr);
+	ptp->cascade_gpo[gpo].tso &= ~(1 << tso);
+}  /* ptp_gpo_reset */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_acquire(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->ops->acquire(sw);
+}  /* ptp_acquire */
+
+static void ptp_release(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->ops->release(sw);
+}  /* ptp_release */
+
+static void get_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	u16 subnsec;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	data = sw->cached.ptp_clk_ctrl;
+	data |= PTP_READ_TIME;
+	sw_w16(sw, REG_PTP_CLK_CTRL, data);
+	do {
+		u8 buf[12];
+		u16 *w_ptr;
+		u32 *d_ptr;
+
+		sw_r(sw, REG_PTP_RTC_SUB_NANOSEC__2, buf, 10);
+		w_ptr = (u16 *) buf;
+		subnsec = be16_to_cpu(*w_ptr);
+		++w_ptr;
+		d_ptr = (u32 *) w_ptr;
+		t->nsec = be32_to_cpu(*d_ptr);
+		++d_ptr;
+		t->sec = be32_to_cpu(*d_ptr);
+	} while (0);
+#if 1
+if (subnsec > PTP_RTC_SUB_NANOSEC_M)
+printk(" ?%x ", subnsec);
+#endif
+	subnsec &= PTP_RTC_SUB_NANOSEC_M;
+	add_nsec(t, subnsec * 8);
+}  /* get_ptp_time */
+
+static void set_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	data = sw->cached.ptp_clk_ctrl;
+	sw_w16(sw, REG_PTP_RTC_SUB_NANOSEC__2, 0);
+	sw_w32(sw, REG_PTP_RTC_NANOSEC, t->nsec);
+	sw_w32(sw, REG_PTP_RTC_SEC, t->sec);
+	data |= PTP_LOAD_TIME;
+	sw_w16(sw, REG_PTP_CLK_CTRL, data);
+}  /* set_ptp_time */
+
+static void adjust_ptp_time(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+	int adj_hack)
+{
+	u16 ctrl;
+	u16 adj = 0;
+	u32 val = nsec;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_ADJ;
+	sw_w32(sw, REG_PTP_RTC_SEC, sec);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		sw_w32(sw, REG_PTP_RTC_NANOSEC, nsec);
+		sw_w16(sw, REG_PTP_CLK_CTRL, ctrl);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		sw_w16(sw, REG_PTP_CLK_CTRL, adj);
+}  /* adjust_ptp_time */
+
+static void adjust_sync_time(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	u32 adjust;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE;
+	sw_w32(sw, REG_PTP_RATE_DURATION, duration);
+	sw_w32(sw, REG_PTP_SUBNANOSEC_RATE, adjust);
+}  /* adjust_sync_time */
+
+static void ptp_rx_reset(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (!ctrl_ptr) {
+		ctrl_ptr = &ctrl;
+		ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+		*ctrl_ptr = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	}
+	*ctrl_ptr &= ~TRIG_RESET;
+	*ctrl_ptr |= TS_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+	*ctrl_ptr &= ~TS_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+}  /* ptp_rx_reset */
+
+static void ptp_rx_off(struct ptp_info *ptp, u8 tsi)
+{
+	u32 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	u32 ts_intr = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		ctrl &= ~TS_INT_ENABLE;
+		ts_intr = tsi_bit;
+	}
+
+	/* Disable previous timestamp detection. */
+	ctrl &= ~TS_ENABLE;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		ptp_rx_reset(ptp, tsi, &ctrl);
+		sw_w32(sw, REG_TS_CTRL_STAT__4, 0);
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+	if (ts_intr)
+		sw_w32(sw, REG_PTP_INT_STATUS__4, ts_intr);
+}  /* ptp_rx_off */
+
+static inline void ptp_rx_intr(struct ptp_info *ptp, u16 tsi_bit, u32 *ctrl)
+{
+	ptp->ts_intr |= tsi_bit;
+	*ctrl |= TS_INT_ENABLE;
+}  /* ptp_rx_intr */
+
+static inline void ptp_rx_on(struct ptp_info *ptp, u8 tsi, int intr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+
+	ctrl |= TS_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_rx_on */
+
+static void ptp_rx_restart(struct ptp_info *ptp, u8 tsi)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+#ifdef CHK_SPI_ACCESS
+if ((ctrl & (TS_INT_ENABLE | TS_ENABLE)) != (TS_INT_ENABLE | TS_ENABLE)) {
+printk(" rx: %08x; %x\n", ctrl, sw_r32(sw, REG_PTP_UNIT_INDEX__4));
+#if 1
+ctrl |= TS_INT_ENABLE;
+#endif
+}
+#endif
+	ctrl &= ~TS_ENABLE;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	ctrl |= TS_ENABLE;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+#ifdef CHK_SPI_ACCESS
+do {
+u32 status;
+status = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+if ((status & (TS_INT_ENABLE | TS_ENABLE)) != (TS_INT_ENABLE | TS_ENABLE)) {
+printk(" rx: %s %08x %08x; %x\n", __func__, status, ctrl, sw_r32(sw, REG_PTP_UNIT_INDEX__4));
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}
+} while (0);
+#endif
+}  /* ptp_rx_restart */
+
+static u32 ts_event_gpi(u8 gpi, u8 event)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = event;
+	ctrl <<= TS_DETECT_S;
+	data = gpi & TS_GPI_M;
+	data <<= TS_GPI_S;
+	ctrl |= data;
+	return ctrl;
+}
+
+static u32 ts_cascade(int prev)
+{
+	u32 ctrl;
+
+	ctrl = prev & TS_CASCADE_UPS_M;
+	ctrl <<= TS_CASCADE_UPS_S;
+	ctrl |= TS_CASCADE_ENABLE;
+	return ctrl;
+}
+
+static void ptp_rx_event(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Config pattern. */
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = ts_event_gpi(gpi, event);
+	sw_w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, tsi, intr);
+}  /* ptp_rx_event */
+
+static void ptp_rx_cascade_event(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	int last;
+	int tsi;
+	u32 ctrl;
+	u32 tail;
+	int i;
+	int prev;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = ts_event_gpi(gpi, event);
+		ctrl |= ts_cascade(prev);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		sw_w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+		/* Enable timestamp interrupt. */
+		if (intr) {
+			ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+			ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+			sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		}
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, first);
+	ctrl = ts_event_gpi(gpi, event);
+	ctrl |= ts_cascade(last);
+	ptp->cascade_rx |= (1 << first);
+	sw_w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, first, intr);
+}  /* ptp_rx_cascade_event */
+
+static u32 ptp_get_event_cnt(struct ptp_info *ptp, u8 tsi, void *ptr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	return sw_r32(sw, REG_TS_CTRL_STAT__4);
+}  /* ptp_get_event_cnt */
+
+static void ptp_get_events(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr)
+{
+	int i;
+	u32 *data = buf;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_r(sw, reg_ns, buf, len);
+	for (i = 0; i < len / sizeof(u32); i++)
+		data[i] = be32_to_cpu(data[i]);
+}  /* ptp_get_events */
+
+static void ptp_read_event_func(struct ptp_info *ptp, u8 tsi, void *ptr,
+	u32 (*get_event_cnt)(struct ptp_info *ptp, u8 tsi, void *ptr),
+	void (*get_events)(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr))
+{
+	u32 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	u8 buf[96];
+	u32 *d_ptr;
+	u32 reg_ns;
+	struct ptp_utime t;
+	u32 sub;
+	int max_ts;
+	int num;
+	int i;
+	int edge;
+	struct ptp_event *event = &ptp->events[tsi];
+	int last = event->num;
+
+	ctrl = get_event_cnt(ptp, tsi, ptr);
+	num = (ctrl >> TS_EVENT_DETECT_S) & TS_EVENT_DETECT_M;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (event->num >= max_ts)
+		return;
+	i = event->num;
+
+	reg_ns = REG_TS_EVENT_0_NANOSEC + TS_EVENT_SAMPLE * i;
+	get_events(ptp, reg_ns, 12 * (max_ts - event->num), buf, ptr);
+	d_ptr = (u32 *) buf;
+	for (; i < max_ts; i++) {
+		t.nsec = (*d_ptr);
+		++d_ptr;
+		t.sec = (*d_ptr);
+		++d_ptr;
+		sub = (*d_ptr) & TS_EVENT_SUB_NANOSEC_M;
+		++d_ptr;
+		edge = ((t.nsec >> TS_EVENT_EDGE_S) & TS_EVENT_EDGE_M);
+#if 0
+printk("edge: %x %x\n", event->event, edge);
+#endif
+		t.nsec &= TS_EVENT_NANOSEC_M;
+		add_nsec(&t, sub * 8);
+#if 1
+/*
+ * THa  2011/10/06
+ * Unit sometimes detects rising edge when it is configured to detect falling
+ * edge only.  This happens in the case of hooking up the output pin to an
+ * input pin and using two units running opposite cycle in cascade mode.  The
+ * 8 ns switch pulse before the cycle is too short to detect properly,
+ * resulting in missing edges.
+ * When detecting events directly from the output pin, the minimum pulse time
+ * is 24 ns for proper detection without missing any edge.
+ */
+		if (event->event < 2 && edge != event->event)
+			edge = event->event;
+#endif
+		event->edge |= edge << i;
+		event->t[i] = t;
+	}
+	event->num = max_ts;
+
+	/* Indicate there is new event. */
+	if (event->num > last)
+		ptp->ts_status |= tsi_bit;
+}  /* ptp_read_event_func */
+
+static void ptp_read_event(struct ptp_info *ptp, u8 tsi)
+{
+	ptp_read_event_func(ptp, tsi, NULL, ptp_get_event_cnt,
+		ptp_get_events);
+}  /* ptp_read_event */
+
+static u32 trig_cascade(int prev)
+{
+	u32 ctrl;
+
+	ctrl = prev & TRIG_CASCADE_UPS_M;
+	ctrl <<= TRIG_CASCADE_UPS_S;
+	return ctrl;
+}
+
+static void ptp_tx_off(struct ptp_info *ptp, u8 tso)
+{
+	u32 ctrl;
+	u16 tso_bit = (1 << tso);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+
+	/* Disable previous trigger out if not already completed. */
+	ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	if (ctrl & TRIG_ENABLE) {
+		ctrl &= ~TRIG_ENABLE;
+		sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	}
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		ptp_gpo_reset(ptp, ptp->outputs[tso].gpo, tso, &ctrl);
+		ptp->cascade_tx &= ~tso_bit;
+	} else {
+		ctrl = sw_r32(sw, REG_TRIG_CTRL__4);
+		if (ctrl & TRIG_CASCADE_ENABLE) {
+			ctrl &= ~TRIG_CASCADE_ENABLE;
+			ctrl &= ~TRIG_CASCADE_TAIL;
+			ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+			sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+		}
+	}
+}  /* ptp_tx_off */
+
+static void ptp_tx_on(struct ptp_info *ptp, u8 tso)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	ctrl |= TRIG_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_tx_on */
+
+static void ptp_tx_trigger_time(struct ptp_info *ptp, u8 tso, u32 sec, u32 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_w32(sw, REG_TRIG_TARGET_SEC, sec);
+	sw_w32(sw, REG_TRIG_TARGET_NANOSEC, nsec);
+}  /* ptp_tx_trigger_time */
+
+static u32 trig_event_gpo(u8 gpo, u8 event)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = event & TRIG_PATTERN_M;
+	ctrl <<= TRIG_PATTERN_S;
+	data = gpo & TRIG_GPO_M;
+	data <<= TRIG_GPO_S;
+	ctrl |= data;
+	return ctrl;
+}
+
+static void ptp_tx_event(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+	int intr, int now, int opt)
+{
+	u32 ctrl;
+	u32 pattern = 0;
+	u16 tso_bit = (1 << tso);
+	struct ptp_output *cur = &ptp->outputs[tso];
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Hardware immediately keeps level high on new GPIO if not reset. */
+	if (cur->level && gpo != cur->gpo)
+		ptp_gpo_reset(ptp, cur->gpo, tso, NULL);
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	if (intr)
+		ctrl |= TRIG_NOTIFY;
+	if (now)
+		ctrl |= TRIG_NOW;
+	if (opt)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (TRIG_REG_OUTPUT == event) {
+		pattern = pulse & TRIG_BIT_PATTERN_M;
+		cur->level = 0;
+		if (cnt) {
+			u32 reg;
+
+			reg = cnt - 1;
+			reg %= 16;
+			while (reg) {
+				pulse >>= 1;
+				reg--;
+			}
+			if (pulse & 1)
+				cur->level = 1;
+		}
+		pulse = 0;
+	} else if (event >= TRIG_NEG_PULSE) {
+		if (0 == pulse)
+			pulse = 1;
+		else if (pulse > TRIG_PULSE_WIDTH_M)
+			pulse = TRIG_PULSE_WIDTH_M;
+		sw_w24(sw, REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+	}
+
+	/* Config cycle width. */
+	if (event >= TRIG_NEG_PERIOD) {
+		u32 data = cnt;
+		int min_cycle = pulse * PULSE_NSEC + MIN_CYCLE_NSEC;
+
+		if (cycle < min_cycle)
+			cycle = min_cycle;
+		sw_w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+		/* Config trigger count. */
+		data <<= TRIG_CYCLE_CNT_S;
+		pattern |= data;
+		sw_w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+	}
+
+	cur->len = 0;
+	if (event >= TRIG_NEG_PERIOD) {
+		if (cnt)
+			cur->len += cycle * cnt;
+		else
+			cur->len += 0xF0000000;
+	} else if (event >= TRIG_NEG_PULSE)
+		cur->len += pulse * PULSE_NSEC;
+	else
+		cur->len += MIN_CYCLE_NSEC;
+
+	cur->start.sec = sec;
+	cur->start.nsec = nsec;
+	cur->iterate = iterate;
+	cur->trig = cur->start;
+	cur->stop = cur->start;
+	add_nsec(&cur->stop, cur->len);
+	cur->gpo = gpo;
+
+	switch (event) {
+	case TRIG_POS_EDGE:
+	case TRIG_NEG_PULSE:
+	case TRIG_NEG_PERIOD:
+		cur->level = 1;
+		break;
+	case TRIG_REG_OUTPUT:
+		break;
+	default:
+		cur->level = 0;
+		break;
+	}
+
+	if (ptp->cascade)
+		return;
+
+	/*
+	 * Need to reset after completion.  Otherwise, this output pattern
+	 * does not behave consistently in cascade mode.
+	 */
+	if (TRIG_NEG_EDGE == event)
+		ptp->cascade_tx |= tso_bit;
+
+	ptp->cascade_gpo[gpo].total = 0;
+	if (cur->level)
+		ptp->cascade_gpo[gpo].tso |= tso_bit;
+	else
+		ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+
+	/* Config trigger time. */
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_tx_event */
+
+static void ptp_pps_event(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	u32 pattern;
+	u32 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 tso = ptp->pps_tso;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_tx_off(ptp, tso);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	sw_w24(sw, REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+
+	/* Config cycle width. */
+	sw_w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	sw_w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_pps_event */
+
+static void cfg_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec, u32 nsec)
+{
+	u32 pattern;
+	u32 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	if (1 == tso)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	sw_w32(sw, REG_TRIG_PULSE_WIDTH__4 + 0, pulse);
+
+	/* Config cycle width. */
+	sw_w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	sw_w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+}  /* cfg_10MHz */
+
+static void ptp_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	int i;
+	u32 nsec;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	for (i = 0; i < 2; i++) {
+		ptp_tx_off(ptp, tso);
+
+		cfg_10MHz(ptp, tso, gpo, sec, nsec);
+
+		/* Enable trigger. */
+		ptp_tx_on(ptp, tso);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz */
+
+static void ptp_tx_cascade_cycle(struct ptp_info *ptp, u8 tso, u32 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_w32(sw, REG_TRIG_ITERATE_TIME, nsec);
+}  /* ptp_tx_cascade_cycle */
+
+static void ptp_tx_cascade_on(struct ptp_info *ptp, u8 tso, u8 first, u8 last,
+	u16 repeat)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw_r32(sw, REG_TRIG_CTRL__4);
+	ctrl |= TRIG_CASCADE_ENABLE;
+	ctrl &= ~trig_cascade(TRIG_CASCADE_UPS_M);
+	if (tso == first)
+		ctrl |= trig_cascade(last);
+	else
+		ctrl |= trig_cascade(tso - 1);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		ctrl |= repeat - 1;
+	}
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+}  /* ptp_tx_cascade_on */
+
+static int ptp_tx_cascade(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	int i;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	if (check_cascade(ptp, first, total, &repeat, sec, nsec)) {
+		dbg_msg("cascade repeat timing is not right\n");
+		return 1;
+	}
+	tso = last;
+	for (i = 0; i < total; i++, tso--) {
+		cur = &ptp->outputs[tso];
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		ptp_tx_trigger_time(ptp, tso, cur->trig.sec,
+			cur->trig.nsec);
+		ptp_tx_cascade_cycle(ptp, tso, cur->iterate);
+		ptp_tx_cascade_on(ptp, tso, first, last, repeat);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	/* Do not reset last unit to keep level high. */
+	if (ptp->outputs[last].level) {
+		ptp->cascade_tx &= ~(1 << last);
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso |= (1 << last);
+	} else
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso &= ~(1 << last);
+	ptp_tx_on(ptp, first);
+	return 0;
+}  /* ptp_tx_cascade */
+
+/* -------------------------------------------------------------------------- */
+
+static void set_ptp_domain(struct ptp_info *ptp, u8 domain)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) & ~PTP_DOMAIN_M;
+	ctrl |= domain;
+	sw->reg->w16(sw, REG_PTP_DOMAIN_VERSION, ctrl);
+}  /* set_ptp_domain */
+
+static void set_ptp_mode(struct ptp_info *ptp, u16 mode)
+{
+	u16 val;
+	u16 sav;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	val = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	sav = val;
+	val &= ~(PTP_1STEP | PTP_TC_P2P | PTP_MASTER);
+	val |= mode;
+	if (val != sav)
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, val);
+}  /* set_ptp_mode */
+
+static void synchronize_clk(struct ptp_info *ptp)
+{
+	u32 sec;
+	int inc;
+
+	if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+		ptp->adjust_offset = -ptp->adjust_offset;
+		ptp->adjust_sec = -ptp->adjust_sec;
+		inc = false;
+	} else
+		inc = true;
+	sec = (u32) ptp->adjust_sec;
+	ptp->reg->adjust_time(ptp, inc, sec, ptp->adjust_offset,
+		ptp->features & PTP_ADJ_HACK);
+	ptp->offset_changed = ptp->adjust_offset;
+	ptp->adjust_offset = 0;
+	ptp->adjust_sec = 0;
+}  /* synchronize_clk */
+
+static void set_ptp_adjust(struct ptp_info *ptp, u32 adjust)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, adjust);
+}  /* set_ptp_adjust */
+
+static inline void unsyntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+}  /* unsyntonize_clk */
+
+static void syntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl |= PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+}  /* syntonize_clk */
+
+static u16 get_ptp_delay(struct ptp_info *ptp, int port, u32 reg)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = PORT_CTRL_ADDR(port, reg);
+	return sw->reg->r16(sw, reg);
+}  /* get_ptp_delay */
+
+static void set_ptp_delay(struct ptp_info *ptp, int port, u32 reg, u16 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = PORT_CTRL_ADDR(port, reg);
+	sw->reg->w16(sw, reg, nsec);
+}  /* set_ptp_delay */
+
+static u16 get_ptp_ingress(struct ptp_info *ptp, int port)
+{
+	return get_ptp_delay(ptp, port, REG_PTP_PORT_RX_DELAY__2);
+}
+
+static u16 get_ptp_egress(struct ptp_info *ptp, int port)
+{
+	return get_ptp_delay(ptp, port, REG_PTP_PORT_TX_DELAY__2);
+}
+
+static short get_ptp_asym(struct ptp_info *ptp, int port)
+{
+	short val;
+
+	val = get_ptp_delay(ptp, port, REG_PTP_PORT_ASYM_DELAY__2);
+	if (val & 0x8000)
+		val = -(val & ~0x8000);
+	return val;
+}
+
+static u32 get_ptp_link(struct ptp_info *ptp, int port)
+{
+	u32 reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_LINK_DELAY__4);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	return sw->reg->r32(sw, reg);
+}
+
+static void set_ptp_ingress(struct ptp_info *ptp, int port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, REG_PTP_PORT_RX_DELAY__2, nsec);
+}
+
+static void set_ptp_egress(struct ptp_info *ptp, int port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, REG_PTP_PORT_TX_DELAY__2, nsec);
+}
+
+static void set_ptp_asym(struct ptp_info *ptp, int port, short nsec)
+{
+	if (nsec < 0)
+		nsec = -nsec | 0x8000;
+	set_ptp_delay(ptp, port, REG_PTP_PORT_ASYM_DELAY__2, nsec);
+}
+
+static void set_ptp_link(struct ptp_info *ptp, int port, u32 nsec)
+{
+	u32 reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_LINK_DELAY__4);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->reg->w32(sw, reg, nsec);
+}
+
+static inline void dbp_tx_ts(char *name, u8 port, u32 timestamp)
+{
+	u8 overflow;
+	char ts[FMT_NSEC_SIZE];
+
+	timestamp = timestamp_val(timestamp, &overflow);
+	format_nsec(ts, timestamp);
+	dbg_msg("%s p:%d c:%u %08x:%s\n", name, port, overflow, timestamp, ts);
+}  /* dbp_tx_ts */
+
+static void ptp_setup_udp_msg(struct ptp_dev_info *info, u8 *data, int len,
+	void (*func)(u8 *data, void *param), void *param)
+{
+	u8 buf[MAX_TSM_UDP_LEN];
+	int in_intr = in_interrupt();
+
+	if (len > MAX_TSM_UDP_LEN)
+		len = MAX_TSM_UDP_LEN;
+	if (!in_intr)
+		mutex_lock(&info->lock);
+	memcpy(buf, data, len);
+	if (func)
+		func(buf, param);
+	len += 2;
+	if (info->read_len + len <= info->read_max) {
+		u16 *udp_len = (u16 *) &info->read_buf[info->read_len];
+
+		*udp_len = len;
+		udp_len++;
+		memcpy(udp_len, buf, len - 2);
+		info->read_len += len;
+	}
+	if (!in_intr)
+		mutex_unlock(&info->lock);
+	wake_up_interruptible(&info->wait_udp);
+}  /* ptp_setup_udp_msg */
+
+static void ptp_tsm_resp(u8 *data, void *param)
+{
+	struct tsm_db *db = (struct tsm_db *) data;
+	struct ptp_ts *ts = param;
+	u32 timestamp;
+	u8 sec_chk;
+
+	db->cmd |= TSM_CMD_RESP;
+	db->cur_sec = htonl(ts->t.sec);
+	db->cur_nsec = htonl(ts->t.nsec);
+	timestamp = timestamp_val(ts->timestamp, &sec_chk);
+	db->timestamp = htonl(timestamp);
+	db->cur_nsec = db->timestamp;
+}  /* ptp_tsm_resp */
+
+static void ptp_tsm_get_time_resp(u8 *data, void *param)
+{
+	struct tsm_get_time *get = (struct tsm_get_time *) data;
+	struct ptp_utime *t = param;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->sec = htonl(t->sec);
+	get->nsec = htonl(t->nsec);
+}  /* ptp_tsm_get_time_resp */
+
+static void add_tx_delay(struct ptp_ts *ts, int delay, u32 cur_sec)
+{
+	update_ts(ts, cur_sec);
+
+	/*
+	 * Save timestamp without transmit latency for PTP stack that adjusts
+	 * transmit latency itself.
+	 */
+	ts->r = ts->t;
+	add_nsec(&ts->t, delay);
+	ts->timestamp = ts->t.nsec;
+}  /* add_tx_delay */
+
+static void save_tx_ts(struct ptp_info *ptp, struct ptp_tx_ts *tx,
+	struct ptp_hw_ts *htx, int delay, int port)
+{
+	unsigned long diff = 0;
+
+	add_tx_delay(&htx->ts, delay, ptp->cur_time.sec);
+	if (!htx->sim_2step) {
+		struct tsm_db *db = (struct tsm_db *) tx->data.buf;
+		u8 msg = tx->data.buf[0] & 3;
+
+		tx->ts = htx->ts;
+		tx->resp_time = jiffies;
+		if (tx->req_time)
+			diff = tx->resp_time - tx->req_time;
+		if (diff < 4 * ptp->delay_ticks) {
+			if (tx->missed) {
+				if (diff > 2 * ptp->delay_ticks)
+					dbg_msg("  caught: %d, %lu; %x=%04x\n",
+						port, diff, msg,
+						ntohs(db->seqid));
+				if (tx->dev) {
+					ptp_setup_udp_msg(tx->dev,
+						tx->data.buf, tx->data.len,
+						ptp_tsm_resp, &tx->ts);
+					tx->dev = NULL;
+				}
+
+				/* Invalidate the timestamp. */
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		} else {
+			dbg_msg("  new: %d, %lu; %x=%04x\n", port, diff,
+				msg, ntohs(db->seqid));
+		}
+		tx->missed = false;
+		if (tx->skb) {
+			int len;
+			u64 ns;
+			struct skb_shared_hwtstamps shhwtstamps;
+
+			if (ptp->tx_en & (1 << 8))
+				ns = (u64) tx->ts.t.sec * NANOSEC_IN_SEC +
+					tx->ts.t.nsec;
+			else
+				ns = (u64) tx->ts.r.sec * NANOSEC_IN_SEC +
+					tx->ts.r.nsec;
+			memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+			shhwtstamps.hwtstamp = ns_to_ktime(ns);
+
+			/* Indicate which port message is sent out. */
+			tx->msg->hdr.reserved2 = port + 1;
+			len = (unsigned char *) tx->msg - tx->skb->data;
+			__skb_pull(tx->skb, len);
+			skb_tstamp_tx(tx->skb, &shhwtstamps);
+
+			/* buffer not released yet. */
+			if (skb_shinfo(tx->skb)->tx_flags & SKBTX_HW_TSTAMP)
+				skb_shinfo(tx->skb)->tx_flags &=
+					~SKBTX_IN_PROGRESS;
+			else
+				dev_kfree_skb_irq(tx->skb);
+			tx->skb = NULL;
+		}
+	}
+	htx->sending = false;
+}  /* save_tx_ts */
+
+static int get_speed_index(struct ptp_info *ptp, int port)
+{
+	int index;
+
+	if (1000 == ptp->linked[port])
+		index = 0;
+	else if (100 == ptp->linked[port])
+		index = 1;
+	else
+		index = 2;
+	return index;
+}  /* get_speed_index */
+
+static int get_tx_time(struct ptp_info *ptp, int port, u16 status)
+{
+	int delay;
+	int index;
+	u32 reg;
+	struct ptp_tx_ts *tx = NULL;
+	struct ptp_hw_ts *htx = NULL;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	index = get_speed_index(ptp, port);
+	while (status) {
+		delay = ptp->tx_latency[port][index];
+		if (status & PTP_PORT_XDELAY_REQ_INT) {
+			reg = REG_PTP_PORT_XDELAY_TS;
+			tx = &ptp->tx_dreq[port];
+			htx = &ptp->hw_dreq[port];
+			status &= ~PTP_PORT_XDELAY_REQ_INT;
+		} else if (status & PTP_PORT_SYNC_INT) {
+			reg = REG_PTP_PORT_SYNC_TS;
+			tx = &ptp->tx_sync[port];
+			htx = &ptp->hw_sync[port];
+			status &= ~PTP_PORT_SYNC_INT;
+		} else {
+			reg = REG_PTP_PORT_PDRESP_TS;
+			tx = &ptp->tx_resp[port];
+			htx = &ptp->hw_resp[port];
+			status &= ~PTP_PORT_PDELAY_RESP_INT;
+		}
+		htx->ts.timestamp = sw->reg->r32(sw,
+			PORT_CTRL_ADDR(port, reg));
+		save_tx_ts(ptp, tx, htx, delay, port);
+	}
+	if (!htx)
+		return false;
+
+	return true;
+}  /* get_tx_time */
+
+static void generate_tx_event(struct ptp_info *ptp, int gpo)
+{
+	struct ptp_utime t;
+
+	if (gpo >= MAX_GPIO)
+		return;
+	ptp->first_sec = 0;
+	ptp->intr_sec = 0;
+	ptp->update_sec_jiffies = jiffies;
+	ptp->reg->get_time(ptp, &t);
+	t.sec += 1;
+	if (t.nsec >= (NANOSEC_IN_SEC - ptp->delay_ticks * 50000000))
+		t.sec += 1;
+	ptp->reg->pps_event(ptp, gpo, t.sec);
+#ifdef ENABLE_10_MHZ_CLK
+	ptp->reg->ptp_10MHz(ptp, ptp->mhz_tso, ptp->mhz_gpo, t.sec);
+#endif
+	schedule_delayed_work(&ptp->update_sec, (1000000 - t.nsec / 1000) * HZ
+		/ 1000000);
+}  /* generate_tx_event */
+
+static void ptp_check_pps(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, check_pps);
+
+	if (ptp->update_sec_jiffies) {
+		ptp->ops->acquire(ptp);
+		generate_tx_event(ptp, ptp->pps_gpo);
+		ptp->ops->release(ptp);
+	}
+}  /* ptp_check_pps */
+
+static void prepare_gps(struct ptp_info *ptp)
+{
+	ptp->ops->acquire(ptp);
+	ptp->tsi_used |= (1 << ptp->gps_tsi);
+	ptp->events[ptp->gps_tsi].event = 1;
+	ptp->events[ptp->gps_tsi].timeout = 0;
+	ptp->reg->rx_event(ptp, ptp->gps_tsi, ptp->gps_gpi, DETECT_RISE, true);
+	ptp->ops->release(ptp);
+}  /* prepare_gps */
+
+static void prepare_pps(struct ptp_info *ptp)
+{
+	if (ptp->pps_gpo >= MAX_GPIO)
+		return;
+	ptp->ops->acquire(ptp);
+	ptp->tso_used |= (1 << ptp->pps_tso);
+	ptp->tso_sys |= (1 << ptp->pps_tso);
+#ifdef ENABLE_10_MHZ_CLK
+	ptp->tso_used |= (1 << ptp->mhz_tso);
+	ptp->tso_used |= (1 << 1);
+	ptp->tso_sys |= (1 << ptp->mhz_tso);
+	ptp->tso_sys |= (1 << 1);
+#endif
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->tsi_used |= (1 << ptp->pps_tsi);
+	ptp->tsi_sys |= (1 << ptp->pps_tsi);
+	ptp->events[ptp->pps_tsi].num = 0;
+	ptp->events[ptp->pps_tsi].event = 1;
+	ptp->events[ptp->pps_tsi].edge = 0;
+	ptp->events[ptp->pps_tsi].expired = 0;
+	ptp->reg->rx_event(ptp, ptp->pps_tsi, ptp->pps_gpo, DETECT_RISE, true);
+	ptp->ops->release(ptp);
+}  /* prepare_pps */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_tx_intr_enable(struct ptp_info *ptp)
+{
+	u32 reg;
+	int i;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	for (i = 0; i < ptp->ports; i++) {
+		reg = PORT_CTRL_ADDR(i, REG_PTP_PORT_TX_INT_STATUS__2);
+		sw->reg->w16(sw, reg, 0xffff);
+		reg = PORT_CTRL_ADDR(i, REG_PTP_PORT_TX_INT_ENABLE__2);
+		sw->reg->w16(sw, reg, ptp->tx_intr);
+	}
+}  /* ptp_tx_intr_enable */
+
+/* -------------------------------------------------------------------------- */
+
+static int ptp_poll_event(struct ptp_info *ptp, u8 tsi)
+{
+	int max_ts;
+	int num;
+	u32 status;
+	u16 tsi_bit = (1 << tsi);
+	struct ptp_event *event = &ptp->events[tsi];
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	status = sw->reg->r32(sw, REG_TS_CTRL_STAT__4);
+	ptp->ops->release(ptp);
+	num = (status >> TS_EVENT_DETECT_S) & TS_EVENT_DETECT_M;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (max_ts > event->num) {
+		ptp->ops->acquire(ptp);
+		status = sw->reg->r32(sw, REG_PTP_INT_STATUS__4);
+		if (status & tsi_bit)
+			sw->reg->w32(sw, REG_PTP_INT_STATUS__4, tsi_bit);
+		ptp->reg->read_event(ptp, tsi);
+		ptp->ts_status = 0;
+		ptp->ops->release(ptp);
+		return true;
+	}
+	return false;
+}  /* ptp_poll_event */
+
+static void convert_scaled_nsec(s64 scaled_nsec, int s, s64 *sec, int *nsec)
+{
+	int sign;
+	u64 quot;
+	u32 rem;
+
+	/* Convert to positive number first. */
+	if (scaled_nsec < 0) {
+		sign = -1;
+		scaled_nsec = -scaled_nsec;
+	} else
+		sign = 1;
+	scaled_nsec >>= s;
+	quot = div_u64_rem(scaled_nsec, NSEC_PER_SEC, &rem);
+	*sec = quot;
+	*nsec = (int) rem;
+
+	/* Positive number means clock is faster. */
+	if (1 == sign) {
+		*sec = -*sec;
+		*nsec = -*nsec;
+	}
+}  /* convert_scaled_nsec */
+
+static void adj_cur_time(struct ptp_info *ptp)
+{
+	if (ptp->adjust_offset || ptp->adjust_sec) {
+		synchronize_clk(ptp);
+		if (ptp->sec_changed)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else {
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+	}
+	if (ptp->sec_changed) {
+		struct timespec ts;
+		struct ptp_utime cur;
+
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ts = ktime_to_timespec(ktime_get_real());
+		cur.sec = ts.tv_sec;
+		cur.nsec = ts.tv_nsec;
+		calc_udiff(&ptp->cur_time, &cur, &ptp->time_diff);
+		ptp->sec_changed = 0;
+	}
+}  /* adj_cur_time */
+
+static void set_before_adj(struct ptp_info *ptp, struct ptp_utime *cur)
+{
+	ptp->adjust_offset += cur->nsec;
+	ptp->adjust_offset += ptp->set_delay;
+	ptp->adjust_offset += ptp->get_delay;
+	cur->nsec = 0;
+	if (ptp->adjust_offset > NANOSEC_IN_SEC) {
+		ptp->adjust_offset -= NANOSEC_IN_SEC;
+		cur->sec++;
+	}
+	ptp->reg->set_time(ptp, cur);
+}   /* set_before_adj */
+
+static void set_cur_time(struct ptp_info *ptp, struct ptp_ts *ts)
+{
+	struct ptp_utime cur;
+	s64 diff_sec;
+	int diff_nsec;
+
+	ptp->adjust_offset = ts->t.nsec - ts->timestamp;
+	ptp->reg->get_time(ptp, &cur);
+	diff_nsec = ts->t.nsec - ts->timestamp;
+	diff_sec = (s64) ts->t.sec - cur.sec;
+	if (ptp->features & PTP_ADJ_SEC) {
+		if (diff_sec) {
+			s64 nsec;
+
+			nsec = diff_sec;
+			nsec *= NANOSEC_IN_SEC;
+			nsec += diff_nsec;
+			convert_scaled_nsec(-nsec, 0, &ptp->adjust_sec,
+				&ptp->adjust_offset);
+		} else {
+			ptp->adjust_offset = diff_nsec;
+			ptp->adjust_sec = 0;
+		}
+		ptp->sec_changed = ptp->adjust_sec;
+	} else {
+		if (abs(diff_sec) <= 1) {
+			diff_nsec += diff_sec * NANOSEC_IN_SEC;
+			if (abs(diff_nsec) < NANOSEC_IN_SEC) {
+				ptp->adjust_offset = diff_nsec;
+				diff_sec = 0;
+			}
+		}
+		if (diff_sec) {
+			cur.sec = ts->t.sec;
+			set_before_adj(ptp, &cur);
+			ptp->sec_changed = diff_sec;
+		}
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+}  /* set_cur_time */
+
+static void adj_clock(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, adj_clk);
+	struct ptp_utime cur;
+
+	ptp->ops->acquire(ptp);
+
+	ptp->sec_changed = ptp->adjust_sec;
+	if (!(ptp->features & PTP_ADJ_SEC)) {
+
+		/* Need to adjust second. */
+		if (abs(ptp->adjust_sec) > 1) {
+			ptp->reg->get_time(ptp, &cur);
+			cur.sec += ptp->adjust_sec;
+			set_before_adj(ptp, &cur);
+		} else
+			ptp->adjust_offset += ptp->adjust_sec * NANOSEC_IN_SEC;
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+	ptp->ops->release(ptp);
+}  /* adj_clock */
+
+static void set_latency(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, set_latency);
+	int index;
+	int p;
+
+	ptp->ops->acquire(ptp);
+	for (p = 0; p < ptp->ports; p++) {
+		if (ptp->linked[p] & 0x80000000) {
+			ptp->linked[p] &= ~0x80000000;
+			index = get_speed_index(ptp, p);
+			set_ptp_ingress(ptp, p, ptp->rx_latency[p][index]);
+			set_ptp_egress(ptp, p, ptp->tx_latency[p][index]);
+		}
+	}
+	ptp->ops->release(ptp);
+}  /* set_latency */
+
+static void execute(struct ptp_info *ptp, struct work_struct *work)
+{
+	queue_work(ptp->access, work);
+}  /* execute */
+
+static void ptp_hw_disable(struct ptp_info *ptp)
+{
+	int i;
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	for (i = 0; i < 2; i++) {
+		sw->cached.ptp_unit_index =
+			(i << PTP_TSI_INDEX_S) |
+			(i << PTP_TOU_INDEX_S);
+		sw_w32(sw, REG_PTP_UNIT_INDEX__4, sw->cached.ptp_unit_index);
+		ctrl = TS_RESET | TRIG_RESET;
+		sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		sw_w32(sw, REG_PTP_CTRL_STAT__4, 0);
+		sw_w32(sw, REG_TRIG_CTRL__4, 0);
+		sw_w32(sw, REG_TS_CTRL_STAT__4, 0);
+	}
+	sw->cached.ptp_unit_index = (i << PTP_TOU_INDEX_S);
+	sw_w32(sw, REG_PTP_UNIT_INDEX__4, sw->cached.ptp_unit_index);
+	ctrl = TRIG_RESET;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, 0);
+	sw_w32(sw, REG_TRIG_CTRL__4, 0);
+}  /* ptp_hw_disable */
+
+static void ptp_hw_enable(struct ptp_info *ptp)
+{
+	int i;
+	u16 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp_hw_disable(ptp);
+#ifndef ACL_TEST
+	data = sw_r16(sw, REG_PTP_MSG_CONF1);
+dbg_msg("msg_conf1: %x\n", data);
+	data = ptp->mode;
+	sw_w16(sw, REG_PTP_MSG_CONF1, data);
+	data = sw_r16(sw, REG_PTP_MSG_CONF1);
+	if ((sw->overrides & TAIL_TAGGING) && (data & PTP_ENABLE))
+		sw->overrides |= PTP_TAG;
+#endif
+	data = sw_r16(sw, REG_PTP_MSG_CONF2);
+dbg_msg("msg_conf2: %x\n", data);
+	sw_w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	data = sw_r16(sw, REG_PTP_CLK_CTRL);
+dbg_msg("clk_ctrl: %x\n", data);
+	data |= PTP_CLK_ENABLE;
+	data &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = data;
+#if 1
+	if (!(sw->features & NEW_CAP))
+		data |= PTP_CLK_RESET;
+#endif
+	sw_w16(sw, REG_PTP_CLK_CTRL, data);
+
+	ptp->drift_set = ptp->drift = 0;
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, 0);
+
+	sw->reg->w16(sw, REG_SW_HSR_TPID__2, 0x892F);
+
+	for (i = 0; i < ptp->ports; i++) {
+		ptp->rx_latency[i][0] = 390;
+		ptp->tx_latency[i][0] = 140;
+		ptp->rx_latency[i][1] = 580;
+		ptp->rx_latency[i][1] = 620;
+		ptp->tx_latency[i][1] = 140;
+		ptp->rx_latency[i][2] = 580;
+		ptp->rx_latency[i][2] = 620;
+		ptp->tx_latency[i][2] = 140;
+	}
+	for (i = 0; i < ptp->ports; i++) {
+		set_ptp_ingress(ptp, i, ptp->rx_latency[i][0]);
+		set_ptp_egress(ptp, i, ptp->tx_latency[i][0]);
+	}
+
+	ptp->ops->release(ptp);
+}  /* ptp_hw_enable */
+
+static void ptp_start(struct ptp_info *ptp, int init)
+{
+	u32 ctrl;
+	struct timespec ts;
+	struct ptp_utime t;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+#if defined(NO_DIRECT_ACCESS)
+do {
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+	ptp->started = true;
+return;
+}
+} while (0);
+#endif
+	if (init && (sw->features & NEW_CAP))
+		ptp_hw_enable(ptp);
+	ptp->ops->acquire(ptp);
+	ctrl = sw_r16(sw, REG_PTP_MSG_CONF1);
+	if (ctrl == ptp->mode) {
+		ptp->cfg = sw_r16(sw, REG_PTP_MSG_CONF2);
+		ptp->domain = sw_r16(sw, REG_PTP_DOMAIN_VERSION) &
+			PTP_DOMAIN_M;
+		if (!init) {
+			ptp->ops->release(ptp);
+			return;
+		}
+	} else if (!init)
+		ptp->mode = ctrl;
+	if (ptp->mode != ptp->def_mode) {
+		dbg_msg("mode changed: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->ptp_synt = false;
+	}
+	dbg_msg("ptp_start: %04x %04x\n",
+		ptp->mode, ptp->cfg);
+	sw_w16(sw, REG_PTP_MSG_CONF1, ptp->mode);
+	sw_w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	sw_w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	if (sw->overrides & TAIL_TAGGING)
+		sw->overrides |= PTP_TAG;
+	ptp_tx_intr_enable(ptp);
+	ptp->ops->release(ptp);
+
+	ts = ktime_to_timespec(ktime_get_real());
+	t.sec = ts.tv_sec;
+	t.nsec = ts.tv_nsec;
+	ptp->ops->acquire(ptp);
+	set_ptp_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->ops->release(ptp);
+
+	prepare_pps(ptp);
+	ptp->started = true;
+}  /* ptp_start */
+
+static void save_msg_info(struct ptp_info *ptp, struct ptp_msg_info *info,
+	struct ptp_msg_hdr *hdr, u32 port, u32 timestamp)
+{
+	struct ptp_msg_options *data = &info->data;
+
+#if 0
+printk("save %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x.%04x %x %04x %x\n",
+hdr->sourcePortIdentity.clockIdentity.addr[0],
+hdr->sourcePortIdentity.clockIdentity.addr[1],
+hdr->sourcePortIdentity.clockIdentity.addr[2],
+hdr->sourcePortIdentity.clockIdentity.addr[3],
+hdr->sourcePortIdentity.clockIdentity.addr[4],
+hdr->sourcePortIdentity.clockIdentity.addr[5],
+hdr->sourcePortIdentity.clockIdentity.addr[6],
+hdr->sourcePortIdentity.clockIdentity.addr[7],
+hdr->sourcePortIdentity.port,
+hdr->messageType, hdr->sequenceId, hdr->domainNumber);
+#endif
+	memcpy(&data->id, &hdr->sourcePortIdentity,
+		sizeof(struct ptp_port_identity));
+	data->seqid = hdr->sequenceId;
+	data->domain = hdr->domainNumber;
+	data->port = port;
+	data->ts.timestamp = timestamp;
+	update_ts(&data->ts, ptp->cur_time.sec);
+	info->sec = ptp->sec_lo;
+}  /* save_msg_info */
+
+static void exit_msg_info(struct ptp_msg_info info[])
+{
+	struct ptp_msg_info *msg;
+	struct ptp_msg_info *prev;
+	int msg_type;
+
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		prev = &info[msg_type];
+		msg = prev->next;
+		while (msg) {
+			prev->next = msg->next;
+			kfree(msg);
+			msg = prev->next;
+		}
+		info[msg_type].data.port = 0;
+	}
+}  /* exit_msg_info */
+
+static void init_msg_info(struct ptp_msg_info info[], spinlock_t *lock)
+{
+	int msg_type;
+
+	spin_lock_init(lock);
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		info[msg_type].data.port = 0;
+		info[msg_type].next = NULL;
+	}
+}  /* init_msg_info */
+
+static void check_expired_msg(struct ptp_info *ptp, struct ptp_msg_info info[],
+	spinlock_t *lock, int *cnt)
+{
+	struct ptp_msg_info *msg;
+	struct ptp_msg_info *prev;
+	int msg_type;
+	int diff;
+
+	spin_lock(lock);
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		prev = &info[msg_type];
+		msg = prev->next;
+		while (msg) {
+			diff = abs(ptp->sec_lo - msg->sec);
+			if (diff >= 4) {
+				if (cnt && *cnt > 0)
+					(*cnt)--;
+				prev->next = msg->next;
+				kfree(msg);
+				msg = prev;
+			}
+			prev = msg;
+			msg = msg->next;
+		}
+	}
+	spin_unlock(lock);
+}  /* check_expired_msg */
+
+static int find_msg_info(struct ptp_msg_info *msg_info, spinlock_t *lock,
+	struct ptp_msg_hdr *hdr, struct ptp_port_identity *id, int remove,
+	struct ptp_msg_options *info)
+{
+	struct ptp_msg_info *rx_msg = msg_info;
+	struct ptp_msg_info *prev;
+	struct ptp_msg_options *data;
+	int ret = false;
+
+	spin_lock(lock);
+	prev = rx_msg;
+	rx_msg = rx_msg->next;
+	while (rx_msg) {
+		data = &rx_msg->data;
+		if (!memcmp(&data->id, id, sizeof(struct ptp_port_identity))
+				&& data->seqid == hdr->sequenceId &&
+				data->domain == hdr->domainNumber) {
+			info->port = data->port;
+			info->ts = data->ts;
+			if (remove) {
+				prev->next = rx_msg->next;
+				kfree(rx_msg);
+			}
+			ret = true;
+			break;
+		}
+		prev = rx_msg;
+		rx_msg = rx_msg->next;
+	}
+	spin_unlock(lock);
+	return ret;
+}  /* find_msg_info */
+
+static int ptp_stop(struct ptp_info *ptp)
+{
+	flush_work(&ptp->adj_clk);
+	flush_work(&ptp->set_latency);
+	flush_workqueue(ptp->access);
+	ptp->update_sec_jiffies = 0;
+	ptp->ops->acquire(ptp);
+	exit_msg_info(ptp->rx_msg_info);
+	exit_msg_info(ptp->tx_msg_info);
+
+	/* Stop triggered outputs and timestamp inputs. */
+	ptp_hw_disable(ptp);
+
+	/* S2 chip can be reset. */
+	ptp->ptp_synt = false;
+
+	/* Stop processing PTP interrupts. */
+	ptp->started = false;
+	ptp->first_drift = 0;
+	ptp->ops->release(ptp);
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+	return false;
+}  /* ptp_stop */
+
+static void init_tx_ts(struct ptp_tx_ts *ts)
+{
+	ts->ts.timestamp = 0;
+	ts->req_time = 0;
+	ts->resp_time = 0;
+	ts->missed = false;
+	ts->hdr.messageType = 7;
+}  /* init_tx_ts */
+
+static struct ptp_dev_info *find_minor_dev(struct ptp_dev_info *info)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_dev_info *dev;
+	struct ptp_dev_info *prev;
+
+	dev = ptp->dev[info->minor ^ 1];
+	prev = ptp->dev[info->minor];
+	while (prev != info && prev && dev) {
+		prev = prev->next;
+		dev = dev->next;
+	}
+	if (prev != info)
+		dev = NULL;
+	return dev;
+}  /* find_minor_dev */
+
+static void ptp_init_hw(struct ptp_info *ptp)
+{
+	int port;
+	u32 reg;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+#if defined(NO_DIRECT_ACCESS)
+do {
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+return;
+}
+} while (0);
+#endif
+	ptp->ops->acquire(ptp);
+	for (port = 0; port < ptp->ports; port++) {
+		int index;
+
+		ptp->hw_sync[port].ts.timestamp = 0;
+		ptp->hw_sync[port].sending = false;
+		ptp->hw_dreq[port].ts.timestamp = 0;
+		ptp->hw_dreq[port].sending = false;
+		ptp->hw_resp[port].ts.timestamp = 0;
+		ptp->hw_resp[port].sending = false;
+		init_tx_ts(&ptp->tx_sync[port]);
+		init_tx_ts(&ptp->tx_dreq[port]);
+		init_tx_ts(&ptp->tx_resp[port]);
+		reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_XDELAY_TS);
+		ptp->xdelay_ts[port] = sw->reg->r32(sw, reg);
+		reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_PDRESP_TS);
+		ptp->pdresp_ts[port] = sw->reg->r32(sw, reg);
+		index = get_speed_index(ptp, port);
+		ptp->rx_latency[port][index] = get_ptp_ingress(ptp, port);
+		ptp->tx_latency[port][index] = get_ptp_egress(ptp, port);
+		ptp->asym_delay[port][index] = get_ptp_asym(ptp, port);
+		ptp->peer_delay[port] = get_ptp_link(ptp, port);
+		dbg_msg("%d = %d %d %d; %u\n", port,
+			ptp->rx_latency[port][index],
+			ptp->tx_latency[port][index],
+			ptp->asym_delay[port][index],
+			ptp->peer_delay[port]);
+		set_ptp_link(ptp, port, 0);
+		ptp->peer_delay[port] = 0;
+	}
+	ptp->ops->release(ptp);
+}  /* ptp_init_hw */
+
+static void ptp_init_state(struct ptp_info *ptp)
+{
+	u32 reg;
+	struct ptp_utime t;
+	struct ptp_msg_info *tx_msg;
+
+	mutex_lock(&ptp->lock);
+	ptp->udp_head = ptp->udp_tail = 0;
+	for (reg = 0; reg < MAX_TSM_UDP_CNT; reg++)
+		ptp->udp[reg].len = 0;
+	tx_msg = &ptp->tx_msg_info[7];
+	tx_msg->data.port = 0;
+	tx_msg->data.ts.timestamp = 0;
+	ptp->tx_msg_cnt = 0;
+	ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+	mutex_unlock(&ptp->lock);
+
+	if (!ptp->started)
+		return;
+	ptp->reg->start(ptp, false);
+
+	ptp_init_hw(ptp);
+
+	ptp->ops->acquire(ptp);
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+
+	if (!ptp->ptp_synt) {
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+	ptp->reg->get_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->op_state = 1;
+	ptp->ops->release(ptp);
+}  /* ptp_init_state */
+
+static void ptp_exit_state(struct ptp_info *ptp)
+{
+	if (ptp->mode & PTP_MASTER) {
+		u16 data;
+		struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+		ptp->ops->acquire(ptp);
+		data = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+		data &= ~PTP_MASTER;
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, data);
+		ptp->ops->release(ptp);
+		ptp->mode &= ~PTP_MASTER;
+		ptp->def_mode &= ~PTP_MASTER;
+	}
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	ptp->tx_msg_cnt = 0;
+	ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+	ptp->tx_en = ptp->rx_en = 0;
+	ptp->cap = 0;
+	ptp->op_mode = 0;
+	ptp->op_state = 0;
+}  /* ptp_exit_state */
+
+static struct ptp_msg *check_ptp_msg(u8 *data, u16 **udp_check_ptr)
+{
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct iphdr *iph = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	struct udphdr *udp;
+	int ipv6;
+	struct ptp_msg *msg;
+
+	if (eth->h_proto == htons(0x88F7)) {
+		msg = (struct ptp_msg *)(eth + 1);
+		goto check_ptp_version;
+	}
+
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+
+			ptr += VLAN_HLEN;
+			vlan = (struct vlan_ethhdr *) ptr;
+		}
+		if (vlan->h_vlan_encapsulated_proto == htons(0x88F7)) {
+			msg = (struct ptp_msg *)(vlan + 1);
+			goto check_ptp_version;
+		}
+		ipv6 = vlan->h_vlan_encapsulated_proto == htons(ETH_P_IPV6);
+		if (vlan->h_vlan_encapsulated_proto != htons(ETH_P_IP) &&
+				!ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(vlan + 1);
+		iph = (struct iphdr *)(vlan + 1);
+	} else {
+		ipv6 = eth->h_proto == htons(ETH_P_IPV6);
+		if (eth->h_proto != htons(ETH_P_IP) && !ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(eth + 1);
+		iph = (struct iphdr *)(eth + 1);
+	}
+
+	if (ipv6) {
+		if (ip6h->nexthdr != IPPROTO_UDP)
+			return NULL;
+
+		udp = (struct udphdr *)(ip6h + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	} else {
+		if (iph->protocol != IPPROTO_UDP)
+			return NULL;
+		if (ntohs(iph->frag_off) & IP_OFFSET)
+			return NULL;
+
+		udp = (struct udphdr *)(iph + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	}
+
+	if (udp->dest != htons(319) && udp->dest != htons(320))
+		return NULL;
+
+	msg = (struct ptp_msg *)(udp + 1);
+
+check_ptp_version:
+	if (msg->hdr.versionPTP >= 2)
+		return msg;
+	return NULL;
+}  /* check_ptp_msg */
+
+static struct ptp_msg *check_ptp_event(u8 *data)
+{
+	struct ptp_msg *msg;
+
+	msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return NULL;
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		break;
+	case DELAY_REQ_MSG:
+		break;
+	case PDELAY_REQ_MSG:
+		break;
+	case PDELAY_RESP_MSG:
+		break;
+	default:
+		msg = NULL;
+		break;
+	}
+	return msg;
+}
+
+static struct ptp_msg *update_ptp_msg(u8 *data, u8 *port, u32 *timestamp,
+	u32 overrides)
+{
+	struct ptp_msg *msg;
+	u16 *udp_check_loc = NULL;
+	int udp_check = 0;
+
+	msg = check_ptp_msg(data, &udp_check_loc);
+	if (!msg)
+		return NULL;
+	if (msg->hdr.reserved2 != *port) {
+		u8 data = msg->hdr.reserved2;
+
+		if (overrides & PTP_ZERO_RESERVED_FIELD) {
+			udp_check += data;
+			msg->hdr.reserved2 = *port;
+			udp_check -= *port;
+		}
+		*port = data;
+	}
+	if (msg->hdr.reserved3 != htonl(*timestamp)) {
+		u32 tmp = ntohl(msg->hdr.reserved3);
+
+		if (overrides & PTP_ZERO_RESERVED_FIELD) {
+			int i;
+			u16 *data = (u16 *) &msg->hdr.reserved3;
+
+			for (i = 0; i < 2; i++)
+				udp_check += ntohs(data[i]);
+			msg->hdr.reserved3 = htonl(*timestamp);
+			for (i = 0; i < 2; i++)
+				udp_check -= ntohs(data[i]);
+		}
+		*timestamp = tmp;
+	}
+	if ((overrides & PTP_VERIFY_TIMESTAMP) &&
+			PDELAY_RESP_MSG == msg->hdr.messageType &&
+			msg->hdr.flagField.flag.twoStepFlag) {
+		struct ptp_utime rx;
+
+		rx.nsec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			nsec);
+		rx.sec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			sec.lo);
+		*timestamp = (rx.sec << 30) | rx.nsec;
+	}
+	if (udp_check && udp_check_loc) {
+		u16 check;
+
+		check = ntohs(*udp_check_loc);
+		udp_check += check;
+		udp_check = (udp_check >> 16) + (udp_check & 0xffff);
+		udp_check += (udp_check >> 16);
+		check = (u16) udp_check;
+		if (!check)
+			check = -1;
+		*udp_check_loc = htons(check);
+	}
+	return msg;
+}  /* update_ptp_msg */
+
+static void get_rx_tstamp(void *ptr, struct sk_buff *skb)
+{
+	struct ptp_info *ptp = ptr;
+	struct ptp_msg *msg;
+	struct ptp_msg_options *rx_msg;
+	struct ptp_ts ts;
+	u64 ns;
+	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+
+	if (!shhwtstamps)
+		return;
+
+	/* Received PTP messages are saved in database. */
+	if (ptp->op_mode > 1) {
+
+		/* Use previously parsed PTP message if available. */
+		msg = ptp->rx_msg;
+		if (!msg)
+			msg = check_ptp_msg(skb->data, NULL);
+		if (!msg || msg->hdr.messageType & 0x8)
+			return;
+	}
+
+	rx_msg = &ptp->rx_msg_info[7].data;
+	ts = rx_msg->ts;
+
+	ns = (u64) ts.t.sec * NANOSEC_IN_SEC + ts.t.nsec;
+	memset(shhwtstamps, 0, sizeof(*shhwtstamps));
+	shhwtstamps->hwtstamp = ns_to_ktime(ns);
+}  /* get_rx_tstamp */
+
+static void get_tx_tstamp(struct ptp_info *ptp, struct sk_buff *skb)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int cnt;
+	int p;
+	struct ptp_msg *msg;
+	u32 port;
+	u32 intr;
+	struct ptp_tx_ts *tx;
+	struct sk_buff *orig_skb = skb;
+
+	if (ptp->tx_msg_parsed)
+		msg = ptp->tx_msg;
+	else
+		msg = check_ptp_msg(skb->data, NULL);
+	ptp->tx_msg_parsed = false;
+	if (!msg || msg->hdr.messageType & 0x8)
+		return;
+
+	if (ptp->tx_ports & sw->TAIL_TAG_LOOKUP)
+		port = (1 << ptp->ports) - 1;
+	else
+		port = ptp->tx_ports & ((1 << ptp->ports) - 1);
+	if (SYNC_MSG == msg->hdr.messageType) {
+		tx = ptp->tx_sync;
+		intr = PTP_PORT_SYNC_INT;
+	} else if (PDELAY_RESP_MSG == msg->hdr.messageType) {
+		tx = ptp->tx_resp;
+		intr = PTP_PORT_PDELAY_RESP_INT;
+	} else {
+		tx = ptp->tx_dreq;
+		intr = PTP_PORT_XDELAY_REQ_INT;
+	}
+	if (!(ptp->tx_intr & intr))
+		return;
+	cnt = 0;
+	for (p = 0; p < ptp->ports; p++, tx++) {
+		if (!(port & (1 << p)))
+			continue;
+		if (tx->skb) {
+			if (skb_shinfo(tx->skb)->tx_flags & SKBTX_HW_TSTAMP)
+				skb_shinfo(tx->skb)->tx_flags &=
+					~SKBTX_IN_PROGRESS;
+			else {
+				dev_kfree_skb_irq(tx->skb);
+				tx->skb = NULL;
+			}
+		}
+
+		/* Need to create socket buffer for more than 1 port. */
+		if (cnt++) {
+			skb = skb_copy(orig_skb, GFP_ATOMIC);
+			if (!skb)
+				break;
+			skb->sk = orig_skb->sk;
+			msg = check_ptp_event(skb->data);
+		}
+		tx->skb = skb;
+		tx->msg = msg;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+	}
+}  /* get_tx_tstamp */
+
+static int ptp_hwtstamp_ioctl(struct ptp_info *ptp, struct ifreq *ifr)
+{
+	struct hwtstamp_config config;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	/* reserved for future extensions */
+	if (config.flags)
+		return -EINVAL;
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		ptp->tx_en &= ~1;
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+	case HWTSTAMP_TX_ON:
+		ptp->tx_en |= 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		if (!ptp->cap && (ptp->rx_en & 1) && (ptp->rx_en & (1 << 8))) {
+			ptp->tx_en &= ~(1 << 8);
+			ptp->rx_en &= ~(1 << 8);
+		}
+		ptp->rx_en &= ~1;
+		break;
+	case HWTSTAMP_FILTER_ALL:
+		ptp->rx_en |= 1;
+		break;
+	default:
+		if (!ptp->cap && !(ptp->rx_en & 1) && (ptp->tx_en & 1)) {
+			ptp->tx_en |= (1 << 8);
+			ptp->rx_en |= (1 << 8);
+		}
+		ptp->rx_en |= 1;
+		break;
+	}
+
+	return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
+		-EFAULT : 0;
+}
+
+#ifdef CONFIG_KSZ_STP
+#if 0
+static int ptp_chk_rx_msg(struct ptp_info *ptp, u8 *data, u8 port)
+{
+	struct ptp_msg *msg;
+	struct ptp_msg_options *last;
+
+	/* Use previously parsed PTP message if available. */
+	msg = ptp->rx_msg;
+	if (!msg)
+		msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return false;
+	ptp->rx_msg = msg;
+
+	last = &ptp->rx_msg_chk[msg->hdr.messageType];
+	if (!memcmp(&last->id, &msg->hdr.sourcePortIdentity,
+	    sizeof(struct ptp_port_identity)) &&
+	    last->seqid == ntohs(msg->hdr.sequenceId) &&
+	    last->domain == msg->hdr.domainNumber) {
+dbg_msg(" drop: %x %d %d; %04x %04x\n", last->msg, last->port, port,
+last->seqid, ntohs(msg->hdr.sequenceId));
+		return true;
+	}
+	memcpy(&last->id, &msg->hdr.sourcePortIdentity,
+		sizeof(struct ptp_port_identity));
+	last->seqid = ntohs(msg->hdr.sequenceId);
+	last->domain = msg->hdr.domainNumber;
+	last->msg = msg->hdr.messageType;
+	last->port = port;
+	return false;
+}  /* ptp_chk_rx_msg */
+#endif
+#endif
+
+static int ptp_drop_pkt(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+	int *tag, int *ptp_tag)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Not PTP message. */
+	if (!(sw->tag.ports & 0x80))
+		return false;
+	do {
+		u16 vid;
+		u16 *protocol;
+
+		if (!(ptp->vid))
+			break;
+		if (vlan_get_tag(skb, &vid))
+			break;
+		vid &= VLAN_VID_MASK;
+		protocol = (u16 *) &skb->data[VLAN_ETH_HLEN - 2];
+
+		if (!vid)
+			break;
+		if (*protocol == ntohs(0x88F7) && vid != ptp->vid)
+			return true;
+	} while (0);
+	if (!(vlan_id & (1 << *tag)))
+		*tag = 0;
+	*ptp_tag = sw->tag.ports & ~0x80;
+	ptp->ops->get_rx_info(ptp, skb->data, *ptp_tag, sw->tag.timestamp);
+	if (!ptp->op_state) {
+		*ptp_tag = 0;
+		return false;
+	}
+#ifdef CONFIG_KSZ_STP_
+	if (ptp_chk_rx_msg(ptp, skb->data, *ptp_tag))
+		return true;
+#endif
+	if (ptp->rx_en & 1)
+		ptp->ops->get_rx_tstamp(ptp, skb);
+	(*ptp_tag)++;
+	return false;
+}  /* ptp_drop_pkt */
+
+static void set_msg_info(struct ptp_info *ptp, struct ptp_msg_hdr *hdr,
+	u32 port, u32 timestamp)
+{
+	struct ptp_msg_info *tx_msg;
+	struct ptp_msg_info *info;
+
+	tx_msg = &ptp->tx_msg_info[hdr->messageType];
+	info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+	if (info) {
+		spin_lock(&ptp->tx_msg_lock);
+		save_msg_info(ptp, info, hdr, port, timestamp);
+		info->next = tx_msg->next;
+		tx_msg->next = info;
+		if (ptp->tx_msg_cnt >= 0)
+			ptp->tx_msg_cnt++;
+		spin_unlock(&ptp->tx_msg_lock);
+	}
+}  /* set_msg_info */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct ptp_dev_info *info,
+	int *output, int wait);
+
+static struct ptp_utime last_rcv;
+static s64 first_sync;
+static s64 first_recv;
+
+static void handle_sync(struct ptp_info *ptp, struct ptp_msg *msg)
+{
+	struct ptp_utime recv;
+	struct ptp_utime sync;
+	struct ksz_ptp_time drift;
+	struct ksz_ptp_time interval;
+	struct ksz_ptp_time offset;
+	s64 corr;
+	u64 nsec;
+	s64 drift_per_sec;
+	s64 avg;
+	s64 cur_recv;
+	s64 cur_sync;
+static s64 sync_corr;
+static struct ptp_ts ts;
+static struct ptp_utime last_sync;
+static struct ksz_ptp_time last_offset;
+
+	sync.sec = sync.nsec = 0;
+	if (SYNC_MSG == msg->hdr.messageType) {
+		struct ptp_msg_info *rx_msg;
+
+		rx_msg = &ptp->rx_msg_info[7];
+		ts = rx_msg->data.ts;
+
+		corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+		corr <<= 32;
+		corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+		corr >>= 16;
+		sync_corr = corr;
+
+		/* This is one-step Sync. */
+		if (!msg->hdr.flagField.flag.twoStepFlag) {
+			sync.sec = ntohl(msg->data.sync.
+				originTimestamp.sec.lo);
+			sync.nsec = ntohl(msg->data.sync.
+				originTimestamp.nsec);
+		}
+	} else if (FOLLOW_UP_MSG == msg->hdr.messageType) {
+		corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+		corr <<= 32;
+		corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+		corr >>= 16;
+		sync_corr += corr;
+		sync.sec = ntohl(msg->data.follow_up.
+			preciseOriginTimestamp.sec.lo);
+		sync.nsec = ntohl(msg->data.follow_up.
+			preciseOriginTimestamp.nsec);
+	}
+
+	/* Sync transmit timestamp not received. */
+	if (!sync.sec)
+		return;
+	if (sync_corr) {
+		u32 rem;
+
+		corr = ts.t.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += ts.t.nsec;
+		corr -= sync_corr;
+		corr = div_s64_rem(corr, NANOSEC_IN_SEC, &rem);
+		ts.t.sec = (u32) corr;
+		ts.t.nsec = rem;
+	}
+#if 0
+	ts.t.nsec += 5;
+	ts.t.nsec /= 10;
+	ts.t.nsec *= 10;
+	sync.nsec += 5;
+	sync.nsec /= 10;
+	sync.nsec *= 10;
+#endif
+	calc_udiff(&sync, &ts.t, &offset);
+	cur_recv = cur_sync = avg = 0;
+	if (first_sync) {
+		nsec = sync.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += sync.nsec;
+		cur_sync = nsec;
+		nsec = ts.t.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += ts.t.nsec;
+		cur_recv = nsec;
+		cur_sync -= first_sync;
+		cur_recv -= first_recv;
+		cur_recv -= cur_sync;
+		cur_recv = abs(cur_recv);
+		cur_recv *= NANOSEC_IN_SEC;
+	} else {
+		nsec = sync.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += sync.nsec;
+		first_sync = nsec;
+		nsec = ts.t.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += ts.t.nsec;
+		first_recv = nsec;
+	}
+	nsec = 0;
+	drift_per_sec = 0;
+	if (last_rcv.sec) {
+		calc_udiff(&last_sync, &sync, &interval);
+		calc_diff(&last_offset, &offset, &drift);
+		nsec = interval.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += interval.nsec;
+		drift_per_sec = abs(drift.sec);
+		drift_per_sec *= NANOSEC_IN_SEC;
+		drift_per_sec += abs(drift.nsec);
+		drift_per_sec *= NANOSEC_IN_SEC;
+		drift_per_sec = div64_s64(drift_per_sec, nsec);
+		avg = div64_s64(cur_recv, cur_sync);
+	}
+	if (sync_corr) {
+		u32 rem;
+
+		cur_recv = avg * nsec;
+		cur_recv = div_s64_rem(cur_recv, NANOSEC_IN_SEC, &rem);
+		cur_recv += nsec;
+		corr = last_rcv.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += last_rcv.nsec;
+		cur_recv += corr;
+		corr = recv.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += recv.nsec;
+		corr -= cur_recv;
+printk(" corr: %lld %lld %lld\n", sync_corr, corr, corr - sync_corr);
+		sync_corr = 0;
+	}
+dbg_msg("sync: %x:%9u %x:%9u p:%10lld d:%lld a:%lld %lld\n",
+ts.t.sec, ts.t.nsec,
+sync.sec, sync.nsec, nsec, drift_per_sec, avg, drift_per_sec - avg);
+dbg_msg("o: %d\n", offset.nsec);
+#if 1
+	if (nsec && nsec < 5000000000) {
+		struct ptp_clk_options clk_opt;
+		int output;
+		int err;
+
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+
+		ptp->drift = (int) drift_per_sec;
+		if (ptp->drift < 5 && abs(offset.nsec) > 1000) {
+			clk_opt.sec = offset.sec;
+			clk_opt.nsec = offset.nsec;
+			if (offset.nsec < 0)
+				output = 2;
+			last_rcv.sec = 0;
+			ts.t.sec = 0;
+			first_recv = 0;
+			first_sync = 0;
+		}
+
+		if (drift.nsec < 0)
+			ptp->drift = -ptp->drift;
+		ptp->drift += ptp->drift_set;
+
+		clk_opt.drift = ptp->drift;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			false);
+	}
+#endif
+	last_sync = sync;
+	last_rcv = ts.t;
+	last_offset = offset;
+}  /* handle_sync */
+
+static struct ptp_msg *ptp_set_rx_info(struct ptp_info *ptp, u8 *data, u8 port,
+	u32 timestamp)
+{
+	struct ptp_msg *msg;
+	u32 overrides = ptp->overrides;
+
+	/* Do not need to parse PTP message. */
+	if (1 == ptp->op_mode)
+		return NULL;
+
+	/* Set receive port and timestamp inside the PTP message. */
+	if (0 == ptp->op_mode && 1 == ptp->op_state)
+		overrides |= PTP_ZERO_RESERVED_FIELD;
+	else
+		overrides &= ~PTP_ZERO_RESERVED_FIELD;
+	port++;
+	msg = update_ptp_msg(data, &port, &timestamp, overrides);
+#if 1
+	if (ptp->overrides & PTP_CHECK_SYNC_TIME)
+		handle_sync(ptp, msg);
+#endif
+
+	/* Do not need to save PTP messages. */
+	if (0 == ptp->op_mode)
+		msg = NULL;
+	return msg;
+}  /* ptp_set_rx_info */
+
+static struct ptp_msg *ptp_get_tx_info(struct ptp_info *ptp, u8 *data,
+	u32 *tx_port, u32 *tx_timestamp)
+{
+	struct ptp_msg *msg;
+	u32 overrides = ptp->overrides;
+	u32 timestamp = 0;
+	u8 port = 0;
+
+	if ((1 == ptp->op_mode || 2 == ptp->op_mode) && !ptp->tx_msg_cnt) {
+		/*
+		 * This packet is not parsed and will be checked again if
+		 * necessary.
+		 */
+		ptp->tx_msg_parsed = false;
+		return NULL;
+	}
+
+	/* Get receive port and timestamp inside the PTP message. */
+	if (0 == ptp->op_mode)
+		overrides |= PTP_ZERO_RESERVED_FIELD;
+	msg = update_ptp_msg(data, &port, &timestamp, overrides);
+	if (msg) {
+
+		/* Get transmit port and timestamp inside the PTP message. */
+		if (0 == ptp->op_mode) {
+			if (port)
+				*tx_port = port;
+			if (timestamp)
+				*tx_timestamp = timestamp;
+		}
+
+		/* Simulate passing transmit information from application. */
+		if (ptp->overrides & PTP_TEST_TX_INFO) {
+			u32 tx_ports = 0;
+
+			if (port)
+				tx_ports = port;
+			set_msg_info(ptp, &msg->hdr, tx_ports, timestamp);
+		}
+	}
+	return msg;
+}  /* ptp_get_tx_info */
+
+static void ptp_get_rx_info(struct ptp_info *ptp, u8 *data, u8 port,
+	u32 timestamp)
+{
+	int index;
+	struct ptp_msg *msg;
+	struct ptp_msg_info *rx_msg;
+	struct ptp_msg_info *info = NULL;
+
+	/* Indicate PTP message is not parsed yet. */
+	ptp->rx_msg = NULL;
+
+	/* Entry is not used for PTP message. */
+	rx_msg = &ptp->rx_msg_info[7];
+	rx_msg->data.port = port;
+	rx_msg->data.ts.timestamp = timestamp;
+	update_ts(&rx_msg->data.ts, ptp->cur_time.sec);
+
+	index = get_speed_index(ptp, port);
+	sub_nsec(&rx_msg->data.ts.t, ptp->rx_latency[port][index]);
+	timestamp = (rx_msg->data.ts.t.sec << 30) | rx_msg->data.ts.t.nsec;
+#if 0
+	if (ptp->overrides & PTP_CHECK_SYNC_TIME)
+dbg_msg(" %x; %08x; %x:%09u\n", ptp->cur_time.sec, timestamp,
+rx_msg->data.ts.t.sec, rx_msg->data.ts.t.nsec);
+#endif
+
+	msg = ptp_set_rx_info(ptp, data, port, timestamp);
+	if (!msg)
+		return;
+
+	/* Indicate PTP message is parsed. */
+	ptp->rx_msg = msg;
+
+	rx_msg = &ptp->rx_msg_info[msg->hdr.messageType];
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		if (!memcmp(&msg->hdr.sourcePortIdentity, &ptp->masterIdentity,
+		    sizeof(struct ptp_clock_identity))) {
+			rx_msg->data.port = port;
+			rx_msg->data.ts.timestamp = timestamp;
+		}
+		info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+		break;
+
+	/* General messages that do not need tracking. */
+	case DELAY_RESP_MSG:
+	case ANNOUNCE_MSG:
+	case SIGNALING_MSG:
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		break;
+	case MANAGEMENT_MSG:
+	{
+		u8 action = msg->data.management.b.actionField;
+
+		/* No need to track management response message. */
+		if (MANAGEMENT_RESPONSE == action ||
+				MANAGEMENT_ACKNOWLEDGE == action)
+			break;
+	}
+	default:
+		info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+		break;
+	}
+	if (info) {
+		spin_lock(&ptp->rx_msg_lock);
+		save_msg_info(ptp, info, &msg->hdr, port, timestamp);
+		info->next = rx_msg->next;
+		rx_msg->next = info;
+		spin_unlock(&ptp->rx_msg_lock);
+	}
+}  /* ptp_get_rx_info */
+
+static void ptp_set_tx_info(struct ptp_info *ptp, u8 *data, void *ptr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int found;
+	struct ptp_msg *msg;
+	struct ptp_msg_options tx_msg;
+	struct ksz_sw_tx_tag *tag = ptr;
+
+	tx_msg.port = 0;
+	tx_msg.ts.timestamp = 0;
+
+	/* Assume packet will be parsed to determine PTP message type. */
+	ptp->tx_msg_parsed = true;
+	ptp->tx_msg = ptp_get_tx_info(ptp, data, &tx_msg.port,
+		&tx_msg.ts.timestamp);
+	if (!ptp->tx_msg) {
+		/* Remember transmit ports for transmit timestamp report. */
+		ptp->tx_ports = tag->ports;
+		return;
+	}
+	msg = ptp->tx_msg;
+
+	/* Check whether application sets transmit information using API. */
+	found = find_msg_info(&ptp->tx_msg_info[msg->hdr.messageType],
+		&ptp->tx_msg_lock, &msg->hdr, &msg->hdr.sourcePortIdentity,
+		true, &tx_msg);
+	if (found) {
+		int port;
+		u32 bits;
+		struct ptp_tx_ts *tx = NULL;
+
+		port = 0;
+		bits = tx_msg.port;
+		while (bits) {
+			if ((bits & 1) && bits != 1) {
+				port = 0;
+				break;
+			}
+			++port;
+			bits >>= 1;
+		}
+		switch (msg->hdr.messageType) {
+		case SYNC_MSG:
+			if (port)
+				tx = &ptp->tx_sync[port - 1];
+			break;
+		case DELAY_REQ_MSG:
+			if (port)
+				tx = &ptp->tx_dreq[port - 1];
+			break;
+		case PDELAY_REQ_MSG:
+			if (port)
+				tx = &ptp->tx_dreq[port - 1];
+			break;
+		case PDELAY_RESP_MSG:
+			if (port)
+				tx = &ptp->tx_resp[port - 1];
+			break;
+		default:
+			tx = NULL;
+		}
+		if (tx) {
+			tx->ts.timestamp = 0;
+			tx->req_time = 0;
+			tx->hdr.messageType = 7;
+		}
+		found = 2;
+		if (ptp->tx_msg_cnt > 0)
+			ptp->tx_msg_cnt--;
+	}
+
+	/* Check whether a default port is set.  Only used in testing. */
+	if (!found && (ptp->overrides & PTP_USE_DEFAULT_PORT)) {
+		tx_msg.port = ptp->tx_msg_info[7].data.port;
+		tx_msg.ts.timestamp = ptp->tx_msg_info[7].data.ts.timestamp;
+		found = 2;
+	}
+
+	/* Only PDELAY_RESP_MSG requires timestamp in transmission. */
+	if (!found && PDELAY_RESP_MSG == msg->hdr.messageType) {
+		int two_step = msg->hdr.flagField.flag.twoStepFlag;
+		struct ptp_msg_pdelay_resp *resp = &msg->data.pdelay_resp;
+
+		found = find_msg_info(&ptp->rx_msg_info[PDELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr,
+			&resp->requestingPortIdentity, !two_step, &tx_msg);
+
+		/* Need to specify timestamp in 1-step mode. */
+		if (!two_step && found) {
+			int delay;
+			int index;
+			struct ptp_ts ts;
+
+			/* Calculate timestamp automatically. */
+			ts = tx_msg.ts;
+			index = get_speed_index(ptp, tx_msg.port);
+			delay = ptp->rx_latency[tx_msg.port][index];
+			sub_nsec(&ts.t, delay);
+			tx_msg.ts.timestamp = (ts.t.sec << 30) | ts.t.nsec;
+		}
+	}
+
+	tag->timestamp = tx_msg.ts.timestamp;
+
+	/* Specific ports are specified. */
+	if (!(tag->ports & sw->TAIL_TAG_LOOKUP))
+		goto set_tx_info_done;
+
+	/* No need to set outgoing port for unicast message. */
+	if (msg->hdr.flagField.flag.unicastFlag)
+		goto set_tx_info_done;
+
+	if (found || tx_msg.port) {
+		if (tx_msg.port) {
+			if (1 == found)
+				tag->ports = (1 << tx_msg.port);
+			else
+				tag->ports = tx_msg.port;
+		}
+		goto set_tx_info_done;
+	} else if (ptp->op_mode != 3)
+		goto set_tx_info_done;
+
+	/* Automatically find a port to send. */
+	switch (msg->hdr.messageType) {
+	case DELAY_REQ_MSG:
+		if (ptp->rx_msg_info[SYNC_MSG].data.ts.timestamp) {
+			tx_msg.port = ptp->rx_msg_info[SYNC_MSG].data.port;
+			found = true;
+		}
+		break;
+	case PDELAY_RESP_MSG:
+		/* Already determined from previous code. */
+		break;
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		found = find_msg_info(&ptp->rx_msg_info[PDELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			pdelay_resp_follow_up.requestingPortIdentity,
+			true, &tx_msg);
+		break;
+	case DELAY_RESP_MSG:
+		found = find_msg_info(&ptp->rx_msg_info[DELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			delay_resp.requestingPortIdentity,
+			true, &tx_msg);
+		break;
+	case MANAGEMENT_MSG:
+	{
+		u8 action = msg->data.management.b.actionField;
+
+		/* Not response to management request message. */
+		if (MANAGEMENT_GET == action || MANAGEMENT_SET == action ||
+		    MANAGEMENT_COMMAND == action)
+			break;
+
+		found = find_msg_info(&ptp->rx_msg_info[MANAGEMENT_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			management.b.targetPortIdentity,
+			true, &tx_msg);
+		break;
+	}
+	default:
+		break;
+	}
+
+	if (found)
+		tag->ports = (1 << tx_msg.port);
+dbg_msg("  tx m:%x f:%d p:%x\n", msg->hdr.messageType, found, tag->ports);
+
+set_tx_info_done:
+
+	/* Remember transmit ports for transmit timestamp report. */
+	ptp->tx_ports = tag->ports;
+
+	do {
+		int p;
+		u32 port;
+		u32 intr;
+		struct ptp_tx_ts *tx;
+
+		/* Not PTP event message. */
+		if (msg->hdr.messageType & 0x8)
+			break;
+		if (ptp->tx_ports & sw->TAIL_TAG_LOOKUP)
+			port = (1 << ptp->ports) - 1;
+		else
+			port = ptp->tx_ports & ((1 << ptp->ports) - 1);
+		if (SYNC_MSG == msg->hdr.messageType) {
+			tx = ptp->tx_sync;
+			intr = PTP_PORT_SYNC_INT;
+		} else if (PDELAY_RESP_MSG == msg->hdr.messageType) {
+			tx = ptp->tx_resp;
+			intr = PTP_PORT_PDELAY_RESP_INT;
+		} else {
+			tx = ptp->tx_dreq;
+			intr = PTP_PORT_XDELAY_REQ_INT;
+		}
+
+		/* Transmit timestamps are not retrieved. */
+		if (!(ptp->tx_intr & intr))
+			break;
+		for (p = 0; p < ptp->ports; p++, tx++) {
+			if (!(port & (1 << p)))
+				continue;
+			memcpy(&tx->hdr, &msg->hdr,
+				sizeof(struct ptp_msg_hdr));
+		}
+	} while (0);
+}  /* ptp_set_tx_info */
+
+static void proc_ptp_get_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp->mode = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	ptp->cfg = sw->reg->r16(sw, REG_PTP_MSG_CONF2);
+	ptp->domain = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) & PTP_DOMAIN_M;
+	ptp->ops->release(ptp);
+
+	/* Check mode in case the switch is reset outside of driver control. */
+	if (ptp->mode != ptp->def_mode && ptp->started) {
+		dbg_msg("mode mismatched: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->reg->start(ptp, false);
+	}
+	cmd->two_step = (ptp->mode & PTP_1STEP) ? 0 : 1;
+	cmd->master = (ptp->mode & PTP_MASTER) ? 1 : 0;
+	cmd->p2p = (ptp->mode & PTP_TC_P2P) ? 1 : 0;
+	cmd->as = (ptp->mode & PTP_802_1AS) ? 1 : 0;
+	cmd->unicast = (ptp->cfg & PTP_UNICAST_ENABLE) ? 1 : 0;
+	cmd->alternate = (ptp->cfg & PTP_ALTERNATE_MASTER) ? 1 : 0;
+	cmd->domain_check = (ptp->cfg & PTP_DOMAIN_CHECK) ? 1 : 0;
+	cmd->udp_csum = (ptp->cfg & PTP_UDP_CHECKSUM) ? 1 : 0;
+	cmd->delay_assoc = (ptp->cfg & PTP_DELAY_CHECK) ? 1 : 0;
+	cmd->pdelay_assoc = (ptp->cfg & PTP_PDELAY_CHECK) ? 1 : 0;
+	cmd->sync_assoc = (ptp->cfg & PTP_SYNC_CHECK) ? 1 : 0;
+	cmd->drop_sync = (ptp->cfg & PTP_DROP_SYNC_DELAY_REQ) ? 1 : 0;
+	cmd->priority = (ptp->cfg & PTP_ALL_HIGH_PRIO) ? 1 : 0;
+	cmd->reserved = ptp->started;
+	cmd->domain = ptp->domain;
+	cmd->access_delay = ptp->get_delay;
+}  /* proc_ptp_get_cfg */
+
+static int proc_ptp_set_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	u16 cfg;
+	u16 mode;
+	u8 domain;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	mode = ptp->mode;
+	cfg = ptp->cfg;
+	domain = ptp->domain;
+	if (cmd->domain_set) {
+		domain = cmd->domain;
+	} else {
+		if (cmd->two_step_set) {
+			if (cmd->two_step)
+				ptp->mode &= ~PTP_1STEP;
+			else
+				ptp->mode |= PTP_1STEP;
+		}
+		if (cmd->master_set) {
+			if (cmd->master)
+				ptp->mode |= PTP_MASTER;
+			else
+				ptp->mode &= ~PTP_MASTER;
+		}
+		if (cmd->p2p_set) {
+			if (cmd->p2p)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+		}
+		if (cmd->as_set) {
+			if (cmd->as)
+				ptp->mode |= PTP_802_1AS;
+			else
+				ptp->mode &= ~PTP_802_1AS;
+		}
+		if (cmd->unicast_set) {
+			if (cmd->unicast)
+				ptp->cfg |= PTP_UNICAST_ENABLE;
+			else
+				ptp->cfg &= ~PTP_UNICAST_ENABLE;
+		}
+		if (cmd->alternate_set) {
+			if (cmd->alternate)
+				ptp->cfg |= PTP_ALTERNATE_MASTER;
+			else
+				ptp->cfg &= ~PTP_ALTERNATE_MASTER;
+		}
+		if (cmd->domain_check_set) {
+			if (cmd->domain_check)
+				ptp->cfg |= PTP_DOMAIN_CHECK;
+			else
+				ptp->cfg &= ~PTP_DOMAIN_CHECK;
+		}
+		if (cmd->udp_csum_set) {
+			if (cmd->udp_csum)
+				ptp->cfg |= PTP_UDP_CHECKSUM;
+			else
+				ptp->cfg &= ~PTP_UDP_CHECKSUM;
+		}
+		if (cmd->delay_assoc_set) {
+			if (cmd->delay_assoc)
+				ptp->cfg |= PTP_DELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_DELAY_CHECK;
+		}
+		if (cmd->pdelay_assoc_set) {
+			if (cmd->pdelay_assoc)
+				ptp->cfg |= PTP_PDELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_PDELAY_CHECK;
+		}
+		if (cmd->sync_assoc_set) {
+			if (cmd->sync_assoc)
+				ptp->cfg |= PTP_SYNC_CHECK;
+			else
+				ptp->cfg &= ~PTP_SYNC_CHECK;
+		}
+		if (cmd->drop_sync_set) {
+			if (cmd->drop_sync)
+				ptp->cfg |= PTP_DROP_SYNC_DELAY_REQ;
+			else
+				ptp->cfg &= ~PTP_DROP_SYNC_DELAY_REQ;
+		}
+		if (cmd->priority_set) {
+			if (cmd->priority)
+				ptp->cfg |= PTP_ALL_HIGH_PRIO;
+			else
+				ptp->cfg &= ~PTP_ALL_HIGH_PRIO;
+		}
+	}
+	ptp->ops->acquire(ptp);
+	if (mode != ptp->mode) {
+		u16 tx_intr = ptp->tx_intr;
+
+		/* For efficiency. */
+		if ((ptp->mode & PTP_1STEP) &&
+		    !(ptp->overrides & PTP_VERIFY_TIMESTAMP))
+			ptp->tx_intr &= ~
+				(PTP_PORT_SYNC_INT | PTP_PORT_PDELAY_RESP_INT);
+		else
+			ptp->tx_intr |=
+				(PTP_PORT_SYNC_INT | PTP_PORT_PDELAY_RESP_INT);
+		dbg_msg("mode: %x %x\n", mode, ptp->mode);
+		mode = ptp->mode;
+		if (ptp->overrides & PTP_VERIFY_TIMESTAMP)
+			mode |= PTP_1STEP;
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, mode);
+		ptp->def_mode = mode;
+		if (tx_intr != ptp->tx_intr)
+			ptp_tx_intr_enable(ptp);
+	}
+	if (cfg != ptp->cfg) {
+		dbg_msg("cfg: %x %x\n", cfg, ptp->cfg);
+		sw->reg->w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	}
+	if (domain != ptp->domain) {
+		ptp->domain = domain;
+		set_ptp_domain(ptp, ptp->domain);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_cfg */
+
+static void cancel_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	ptp->ops->acquire(ptp);
+	first = tsi;
+	events = &ptp->events[tsi];
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	tsi = first;
+	ptp->events[tsi].timeout = 0;
+	do {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+		events = &ptp->events[tsi];
+		events->first = 0;
+		events->last = 0;
+		tsi_bit = 1 << tsi;
+		if (ptp->tsi_used & tsi_bit) {
+			if (events->num < events->max) {
+				ptp->reg->read_event(ptp, tsi);
+				ptp->ts_status = 0;
+			}
+			ptp->reg->rx_off(ptp, tsi);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			if (ptp->tsi_sys & tsi_bit) {
+				printk(KERN_INFO "tsi %d off!\n", tsi);
+				ptp->tsi_sys &= ~tsi_bit;
+				ptp->update_sec_jiffies = jiffies;
+				schedule_delayed_work(&ptp->update_sec,
+					1000 * HZ / 1000);
+			}
+			ptp->tsi_dev[tsi] = NULL;
+		}
+		++tsi;
+	} while (tsi != last);
+	ptp->ops->release(ptp);
+}  /* cancel_rx_unit */
+
+static int check_expired_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	u32 expired;
+	struct ptp_event *events;
+	struct ksz_ptp_time diff;
+	struct ptp_utime t;
+
+	events = &ptp->events[tsi];
+	first = tsi;
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	events = &ptp->events[first];
+	if (events->num && events->timeout) {
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &t);
+		ptp->ops->release(ptp);
+		calc_udiff(events->t, &t, &diff);
+		if (diff.sec >= 0 && diff.nsec >= 0) {
+			expired = diff.sec * 1000 + diff.nsec / 1000000;
+			expired = expired * HZ / 1000;
+			if (expired > events->timeout) {
+				cancel_rx_unit(ptp, first);
+				return 1;
+			}
+		}
+	}
+	return 0;
+}  /* check_expired_rx_unit */
+
+static int proc_dev_rx_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_tsi_options *cmd = (struct ptp_tsi_options *) data;
+	u8 event;
+	int first;
+	int i;
+	int intr;
+	int tsi;
+	int avail;
+	int total;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	tsi = cmd->tsi;
+	total = cmd->total;
+	if (!total)
+		total = 1;
+	first = tsi;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		cancel_rx_unit(ptp, tsi);
+		goto proc_ptp_rx_cascade_event_done;
+	}
+	if (tsi >= MAX_TIMESTAMP_UNIT) {
+		first = 0;
+		do {
+			for (tsi = first; tsi < MAX_TIMESTAMP_UNIT; tsi++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last)
+					break;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			first = tsi;
+			avail = 1;
+			for (i = 1; i < total; i++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last) {
+					++avail;
+					++tsi;
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+				} else {
+					++first;
+					break;
+				}
+		} while (avail < total);
+	} else {
+		for (i = 0; i < total; i++) {
+			if (ptp->tsi_used & (1 << tsi) ||
+					ptp->events[tsi].last)
+				if (!check_expired_rx_unit(ptp, tsi))
+					return DEV_IOC_UNIT_USED;
+			++tsi;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				tsi = 0;
+		}
+	}
+	if (cmd->gpi >= MAX_GPIO)
+		return -EINVAL;
+	if (0 == cmd->event)
+		event = DETECT_FALL;
+	else if (1 == cmd->event)
+		event = DETECT_RISE;
+	else {
+		event = DETECT_RISE | DETECT_FALL;
+		cmd->event = 2;
+	}
+	tsi = first;
+	last = first + total;
+	if (last > MAX_TIMESTAMP_UNIT)
+		last -= MAX_TIMESTAMP_UNIT;
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < total; i++) {
+		tsi_bit = 1 << tsi;
+		ptp->tsi_used |= tsi_bit;
+		if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+			ptp->tsi_intr |= tsi_bit;
+			ptp->tsi_dev[tsi] = info;
+		}
+		events = &ptp->events[tsi];
+		events->num = 0;
+		events->event = cmd->event;
+		events->edge = 0;
+		events->expired = 0;
+		if (total > 1) {
+			events->first = first;
+			events->last = last;
+		}
+		++tsi;
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+	}
+	tsi = first;
+	ptp->events[tsi].timeout = cmd->timeout * HZ / 1000;
+
+	/* Zero timeout means repeatable. */
+	if (!ptp->events[tsi].timeout && cmd->timeout)
+		ptp->events[tsi].timeout = 1;
+	if (total > 1)
+		ptp->reg->rx_cascade_event(ptp, tsi, total, cmd->gpi, event,
+			intr);
+	else
+		ptp->reg->rx_event(ptp, tsi, cmd->gpi, event, intr);
+	ptp->ops->release(ptp);
+
+proc_ptp_rx_cascade_event_done:
+	*data = tsi;
+	return 0;
+}  /* proc_dev_rx_event */
+
+static int find_avail_tx_unit(struct ptp_info *ptp, int total, int *unit)
+{
+	int avail;
+	int first;
+	int i;
+	int tso;
+
+	first = 0;
+	do {
+		for (tso = first; tso < MAX_TRIG_UNIT; tso++)
+			if (!(ptp->tso_used & (1 << tso)))
+				break;
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		first = tso++;
+		avail = 1;
+		for (i = 1; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (!(ptp->tso_used & (1 << tso))) {
+				++avail;
+				++tso;
+			} else {
+				++first;
+				break;
+			}
+		}
+	} while (avail < total);
+	*unit = first;
+	return 0;
+}  /* find_avail_tx_unit */
+
+static int proc_dev_tx_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int gpo;
+	int intr;
+	int tso;
+	int tso_bit;
+	struct ptp_utime t;
+	u16 active;
+	u32 status;
+	int err = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	if (cmd->event > TRIG_REG_OUTPUT)
+		return -EINVAL;
+	tso = cmd->tso;
+	tso_bit = 1 << tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		ptp->ops->acquire(ptp);
+
+		/* Reset the tso. */
+		ptp->cascade_tx |= tso_bit;
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+		goto proc_dev_tx_event_done;
+	}
+	if (ptp->cascade && (tso < ptp->cascade_gpo[gpo].first ||
+			tso >= ptp->cascade_gpo[gpo].first +
+			ptp->cascade_gpo[gpo].total))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available unit for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, 1, &tso);
+
+		if (rc)
+			return rc;
+	} else if (!ptp->cascade && (ptp->tso_used & tso_bit)) {
+
+		/* See whether previous operation is completed. */
+		ptp->ops->acquire(ptp);
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		active = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+		status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+		ptp->ops->release(ptp);
+		if (active & TRIG_ACTIVE) {
+			u16 error = (status >> TRIG_ERROR_S) &
+				PTP_TRIG_UNIT_M;
+
+			if (!(error & tso_bit))
+				return DEV_IOC_UNIT_USED;
+			dbg_msg("trig err: %d\n", tso);
+		}
+		if (!(active & TRIG_ACTIVE)) {
+			u16 done = status & PTP_TRIG_UNIT_M;
+
+			if (!(done & tso_bit)) {
+				/* Reset the unit. */
+				ptp->cascade_tx |= tso_bit;
+				dbg_msg(" !? trig done: %d\n", tso);
+			}
+		}
+		ptp->ops->acquire(ptp);
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+	}
+	ptp->ops->acquire(ptp);
+	if (!ptp->cascade && (cmd->flags & PTP_CMD_REL_TIME) &&
+			cmd->sec < 100) {
+		ptp->reg->get_time(ptp, &t);
+		if (0 == cmd->sec) {
+			cmd->nsec += t.nsec;
+			cmd->nsec += 500;
+			cmd->nsec /= 1000;
+			cmd->nsec *= 1000;
+			if (cmd->nsec >= NANOSEC_IN_SEC) {
+				cmd->nsec -= NANOSEC_IN_SEC;
+				cmd->sec++;
+			}
+		}
+		cmd->sec += t.sec;
+	}
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+		ptp->tso_intr |= tso_bit;
+		ptp->tso_dev[tso] = info;
+	}
+	ptp->tso_used |= tso_bit;
+	ptp->reg->tx_event(ptp, tso, cmd->gpo, cmd->event, cmd->pulse,
+		cmd->cycle, cmd->cnt, cmd->sec, cmd->nsec, cmd->iterate, intr,
+		!(cmd->flags & PTP_CMD_ON_TIME),
+		(cmd->flags & PTP_CMD_CLK_OPT));
+	if (cmd->flags & PTP_CMD_ON_TIME) {
+		status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+		status = (status >> TRIG_ERROR_S) & PTP_TRIG_UNIT_M;
+		if (status & tso_bit)
+			err = DEV_IOC_UNIT_ERROR;
+	}
+	ptp->ops->release(ptp);
+
+proc_dev_tx_event_done:
+	*data = tso;
+	return err;
+}  /* proc_dev_tx_event */
+
+static int proc_ptp_tx_cascade_init(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int i;
+	int tso;
+	int total;
+	u32 status;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	tso = cmd->tso;
+	gpo = cmd->gpo;
+	total = cmd->total;
+	if (!total)
+		return -EINVAL;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	first = tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		if (first != ptp->cascade_gpo[gpo].first ||
+				total != ptp->cascade_gpo[gpo].total) {
+			first = ptp->cascade_gpo[gpo].first;
+			total = ptp->cascade_gpo[gpo].total;
+		}
+
+		/* Reset the last unit in case it is used to raise the level. */
+		first = first + total - 1;
+		if (ptp->outputs[first].level) {
+			ptp->cascade_tx |= (1 << first);
+			ptp->tso_used |= (1 << first);
+		}
+		ptp->ops->acquire(ptp);
+		for (i = 0; i < total; i++, tso++) {
+			if (ptp->tso_used & (1 << tso))
+				ptp_tso_off(ptp, tso, (1 << tso));
+		}
+		tso = total;
+		ptp->cascade = false;
+		ptp->ops->release(ptp);
+		goto proc_ptp_tx_cascade_init_done;
+	}
+
+	if (ptp->cascade)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available units for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, total, &first);
+
+		if (rc)
+			return rc;
+	} else {
+		for (i = 0; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (ptp->tso_used & (1 << tso))
+				return DEV_IOC_UNIT_USED;
+			++tso;
+		}
+	}
+
+	if ((cmd->flags & PTP_CMD_CASCADE_RESET_OPER))
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* Last operation was not in cascade mode. */
+	if (!ptp->cascade_gpo[gpo].total)
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* previous last unit. */
+	i = ptp->cascade_gpo[gpo].first + ptp->cascade_gpo[gpo].total - 1;
+
+	/* current last unit. */
+	tso = first + total - 1;
+
+	/* Last operation not ended high. */
+	if (tso == i || !ptp->outputs[i].level)
+		goto proc_ptp_tx_cascade_init_set;
+
+	ptp->ops->acquire(ptp);
+	ptp_write_index(ptp, PTP_GPIO_INDEX_S, gpo);
+	status = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+
+	/* Current level is high. */
+	if (status & GPIO_IN) {
+
+		/* Set unit to hold the level high. */
+		ptp->reg->tx_event(ptp, tso, gpo, TRIG_POS_EDGE, 0, 0, 1, 0, 1,
+			0, PTP_CMD_INTR_OPER, 1, 0);
+
+		/* Release the signal from the previous last unit. */
+		ptp_gpo_reset(ptp, ptp->outputs[i].gpo, i, NULL);
+	}
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_init_set:
+	ptp->cascade = true;
+	ptp->cascade_gpo[gpo].first = first;
+	ptp->cascade_gpo[gpo].total = total;
+	tso = first;
+
+proc_ptp_tx_cascade_init_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade_init */
+
+static int proc_ptp_tx_cascade(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int tso;
+	int total;
+	struct ptp_utime t;
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	tso = cmd->tso;
+	total = cmd->total;
+	first = tso;
+	if (!ptp->cascade || tso != ptp->cascade_gpo[gpo].first ||
+			total != ptp->cascade_gpo[gpo].total)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		proc_ptp_tx_cascade_init(ptp, data);
+		goto proc_ptp_tx_cascade_done;
+	}
+	ptp->ops->acquire(ptp);
+	if ((cmd->flags & PTP_CMD_REL_TIME) && cmd->sec < 100) {
+		ptp->reg->get_time(ptp, &t);
+		cmd->sec += t.sec;
+	}
+	total = ptp->reg->tx_cascade(ptp, tso, total, cmd->cnt, cmd->sec,
+		cmd->nsec, cmd->flags & PTP_CMD_INTR_OPER);
+	if (!total)
+		ptp->cascade = false;
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade */
+
+static void proc_tsm_get_gps(struct ptp_info *ptp, u8 *data)
+{
+	struct tsm_get_gps *get = (struct tsm_get_gps *) data;
+
+	if (!ptp->gps_dev)
+		return;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->seqid = htons(ptp->gps_seqid);
+	get->sec = htonl(ptp->gps_time.sec);
+	get->nsec = htonl(ptp->gps_time.nsec);
+	ptp_setup_udp_msg(ptp->gps_dev, data, sizeof(struct tsm_get_gps),
+		NULL, NULL);
+	ptp->gps_dev = NULL;
+}  /* proc_tsm_get_gps */
+
+static int proc_dev_get_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) * MAX_TIMESTAMP_EVENT_UNIT +
+		sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp->events[in->unit].num)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	out->event = ptp->events[in->unit].event;
+	out->num = ptp->events[in->unit].num;
+	out->edge = ptp->events[in->unit].edge;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, ptp->events[in->unit].t, len);
+	len += sizeof(struct ptp_tsi_info);
+	ptp_setup_udp_msg(info, buf, len, NULL, NULL);
+	return 0;
+}  /* proc_dev_get_event */
+
+static int proc_ptp_get_event(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	int ret = -1;
+
+	if (ptp->tsi_dev[in->unit])
+		ret = proc_dev_get_event(ptp->tsi_dev[in->unit], data);
+	return ret;
+}  /* proc_ptp_get_event */
+
+static int proc_ptp_get_trig(struct ptp_info *ptp, u8 *data, u16 done,
+	u16 error)
+{
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) + sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+	struct ptp_output *cur;
+	int tso = in->unit;
+	int tso_bit = (1 << tso);
+
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	cur = &ptp->outputs[tso];
+	if (error & tso_bit)
+		out->event = 1;
+	else if (!(done & tso_bit))
+		out->event = 2;
+	else
+		out->event = 0;
+	out->num = 1;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, &cur->trig, len);
+	len += sizeof(struct ptp_tsi_info);
+	if (ptp->tso_dev[tso]) {
+		ptp_setup_udp_msg(ptp->tso_dev[tso], buf, len, NULL, NULL);
+		return 0;
+	}
+	return -1;
+}  /* proc_ptp_get_trig */
+
+static int proc_dev_poll_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp_poll_event(ptp, in->unit))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	return proc_dev_get_event(info, data);
+}  /* proc_dev_poll_event */
+
+static int proc_dev_get_event_info(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	in->unit = MAX_TIMESTAMP_UNIT;
+	in->event = MAX_TIMESTAMP_EVENT_UNIT;
+	in->num = 0;
+	return 0;
+}  /* proc_dev_get_event_info */
+
+static int proc_ptp_get_output(struct ptp_info *ptp, u8 *data)
+{
+	int *output = (int *) data;
+	struct ptp_tso_options *in = (struct ptp_tso_options *) data;
+
+	if (in->gpo >= MAX_GPIO)
+		return -EINVAL;
+	*output = ptp->cascade_gpo[in->gpo].tso;
+	return 0;
+}  /* proc_ptp_get_output */
+
+static void proc_ptp_get_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &t);
+	ptp->ops->release(ptp);
+	cmd->sec = t.sec;
+	cmd->nsec = t.nsec;
+}  /* proc_ptp_get_clk */
+
+static int proc_ptp_set_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+	struct timespec ts;
+	struct ptp_utime sys_time;
+
+	t.sec = cmd->sec;
+	t.nsec = cmd->nsec;
+	ptp->ops->acquire(ptp);
+	ts = ktime_to_timespec(ktime_get_real());
+	sys_time.sec = ts.tv_sec;
+	sys_time.nsec = ts.tv_nsec;
+	ptp->reg->set_time(ptp, &t);
+	ptp->cur_time = t;
+	calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->ops->release(ptp);
+	dbg_msg(" set clk: %x:%09u\n", cmd->sec, cmd->nsec);
+	return 0;
+}  /* proc_ptp_set_clk */
+
+static int proc_ptp_adj_clk(struct ptp_info *ptp, u8 *data, int adjust)
+{
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	adjust--;
+	if (cmd->sec > 1) {
+		ptp->adjust_sec = cmd->sec;
+		ptp->adjust_offset = cmd->nsec;
+		if (!adjust) {
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		}
+		dbg_msg("adj clk: %d %u:%09u\n", adjust, cmd->sec, cmd->nsec);
+		cmd->sec = cmd->nsec = 0;
+		ptp->adj_clk.func(&ptp->adj_clk);
+	}
+	ptp->ops->acquire(ptp);
+	if (cmd->nsec || cmd->sec) {
+		ptp->sec_changed = cmd->sec;
+		if (!(ptp->features & PTP_ADJ_SEC)) {
+			cmd->nsec += cmd->sec * NANOSEC_IN_SEC;
+			cmd->sec = 0;
+		}
+		ptp->reg->adjust_time(ptp, adjust, cmd->sec, cmd->nsec,
+			ptp->features & PTP_ADJ_HACK);
+		ptp->offset_changed = cmd->nsec;
+		if (!adjust)
+			ptp->offset_changed = -cmd->nsec;
+		if (ptp->sec_changed)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else {
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+		if (ptp->sec_changed) {
+			if (adjust)
+				ptp->cur_time.sec += cmd->sec;
+			else
+				ptp->cur_time.sec -= cmd->sec;
+			ptp->sec_changed = 0;
+		}
+		if (adjust)
+			add_nsec(&ptp->cur_time, cmd->nsec);
+		else
+			sub_nsec(&ptp->cur_time, cmd->nsec);
+		dbg_msg(" adj clk: %d %u:%09u\n", adjust, cmd->sec, cmd->nsec);
+	}
+	if (cmd->interval) {
+		ptp->drift = cmd->drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift, cmd->interval);
+		set_ptp_adjust(ptp, ptp->adjust);
+		if (!ptp->ptp_synt) {
+			syntonize_clk(ptp);
+			ptp->ptp_synt = true;
+		}
+if (!ptp->first_drift && ptp->drift_set)
+dbg_msg("first drift: %d\n", ptp->drift_set);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+		dbg_msg(" adj drift: %d\n", cmd->drift);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_adj_clk */
+
+static int proc_ptp_get_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = get_ptp_ingress(ptp, port);
+	delay->tx_latency = get_ptp_egress(ptp, port);
+	delay->asym_delay = get_ptp_asym(ptp, port);
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_delay */
+
+static int proc_ptp_set_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	int index;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	set_ptp_ingress(ptp, port, delay->rx_latency);
+	set_ptp_egress(ptp, port, delay->tx_latency);
+	set_ptp_asym(ptp, port, delay->asym_delay);
+	index = get_speed_index(ptp, port);
+	ptp->rx_latency[port][index] = delay->rx_latency;
+	ptp->tx_latency[port][index] = delay->tx_latency;
+	ptp->asym_delay[port][index] = delay->asym_delay;
+	ptp->ops->release(ptp);
+	dbg_msg("set delay: %d = %d %d %d\n", port,
+		ptp->rx_latency[port][index],
+		ptp->tx_latency[port][index],
+		ptp->asym_delay[port][index]);
+	return 0;
+}  /* proc_ptp_set_delay */
+
+static int proc_ptp_get_peer_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	u32 link;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = 0;
+	delay->tx_latency = 0;
+	delay->asym_delay = 0;
+	link = get_ptp_link(ptp, port);
+	delay->reserved = link;
+	delay->rx_latency = link >> 16;
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_peer_delay */
+
+static int proc_ptp_set_peer_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	u32 link;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	link = delay->rx_latency;
+	link <<= 16;
+	link |= delay->reserved;
+	if (link != ptp->peer_delay[port]) {
+		set_ptp_link(ptp, port, link);
+		ptp->peer_delay[port] = link;
+		dbg_msg("set delay: %d = %d\n", port,
+			ptp->peer_delay[port]);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_peer_delay */
+
+static void ptp_tx_done(struct ptp_info *ptp, int tso)
+{
+	int first;
+	int last;
+	int prev;
+	u32 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+	data = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+	if (data & TRIG_CASCADE_ENABLE) {
+		last = tso;
+		do {
+			--tso;
+			ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+			data = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+			prev = (data >> TRIG_CASCADE_UPS_S) &
+				TRIG_CASCADE_UPS_M;
+			if (prev == last)
+				break;
+		} while (tso > 0);
+		first = tso;
+		for (tso = last; tso > first; tso--)
+			ptp_tso_off(ptp, tso, (1 << tso));
+	}
+	ptp_tso_off(ptp, tso, (1 << tso));
+}  /* ptp_tx_done */
+
+static struct ptp_tx_ts *proc_get_ts(struct ptp_info *ptp, u8 port, u8 msg,
+	u16 seqid, u8 *mac, struct ptp_dev_info *info, int len)
+{
+	struct ptp_tx_ts *tx;
+	int from_stack = false;
+	u8 *data = NULL;
+
+	if (info)
+		data = info->write_buf;
+	if (SYNC_MSG == msg)
+		tx = &ptp->tx_sync[port];
+	else if (PDELAY_RESP_MSG == msg)
+		tx = &ptp->tx_resp[port];
+	else
+		tx = &ptp->tx_dreq[port];
+	if (seqid || mac[0] || mac[1])
+		from_stack = true;
+	if (data && tx->req_time && ptp->linked[port])
+		dbg_msg("  last %x=%04x: %d, %lu\n", msg, seqid, port,
+			jiffies - tx->req_time);
+	tx->missed = false;
+	tx->req_time = jiffies;
+	if (tx->ts.timestamp && from_stack) {
+		unsigned long diff = tx->req_time - tx->resp_time;
+
+		/* The timestamp is not valid. */
+		if (diff >= 4 * ptp->delay_ticks) {
+			dbg_msg("  invalid: %x=%04x: %d, %lu\n",
+				msg, seqid, port, diff);
+			tx->ts.timestamp = 0;
+		} else if (diff > 2 * ptp->delay_ticks)
+			dbg_msg("  ready? %x=%04x: %d, %lu\n",
+				msg, seqid, port, diff);
+	}
+	if (!tx->ts.timestamp && ptp->linked[port] && data) {
+		int rc = wait_event_interruptible_timeout(ptp->wait_ts[port],
+			0 != tx->ts.timestamp, ptp->delay_ticks);
+
+		if (rc < 0)
+			return NULL;
+	}
+	if (!tx->ts.timestamp) {
+		if (from_stack && data) {
+			tx->missed = true;
+			memcpy(tx->data.buf, data, len);
+			tx->data.len = len;
+			tx->dev = info;
+		}
+		if (ptp->linked[port])
+			dbg_msg("  missed %x=%04x: %d, %lu\n",
+				msg, seqid, port, jiffies - tx->req_time);
+		tx = NULL;
+	}
+	return tx;
+}  /* proc_get_ts */
+
+static int proc_ptp_get_timestamp(struct ptp_info *ptp, u8 *data,
+	struct ptp_dev_info *info)
+{
+	struct ptp_ts_options *opt = (struct ptp_ts_options *) data;
+
+	if (opt->timestamp) {
+		struct ptp_ts ts;
+
+		ts.timestamp = opt->timestamp;
+		update_ts(&ts, ptp->cur_time.sec);
+		opt->sec = ts.t.sec;
+		opt->nsec = ts.t.nsec;
+	} else {
+		struct ptp_tx_ts *tx;
+		struct tsm_db *db;
+
+		if (opt->port >= ptp->ports)
+			return DEV_IOC_INVALID_CMD;
+
+		/* Save timestamp information for later reporting. */
+		if (info) {
+			db = (struct tsm_db *) info->write_buf;
+			db->cmd = opt->msg;
+			db->cmd |= TSM_CMD_DB_GET;
+			db->index = opt->port << 1;
+			db->seqid = htons(opt->seqid);
+			db->mac[0] = opt->mac[0];
+			db->mac[1] = opt->mac[1];
+		}
+		tx = proc_get_ts(ptp, opt->port, opt->msg,
+			opt->seqid, opt->mac, info, sizeof(struct tsm_db));
+		if (!tx)
+			return -EAGAIN;
+		if (ptp->cap & PTP_KNOW_ABOUT_LATENCY) {
+			opt->sec = tx->ts.r.sec;
+			opt->nsec = tx->ts.r.nsec;
+		} else {
+			opt->sec = tx->ts.t.sec;
+			opt->nsec = tx->ts.t.nsec;
+		}
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+	}
+	return 0;
+}  /* proc_ptp_get_timestamp */
+
+static struct ptp_tx_ts *proc_get_ts_port(struct ptp_info *ptp, u8 msg,
+	int *port, int *more)
+{
+	int p;
+	struct ptp_tx_ts *tx;
+	struct ptp_tx_ts *xts = NULL;
+
+	*more = false;
+	for (p = 0; p < ptp->ports; p++) {
+		if (SYNC_MSG == msg)
+			tx = &ptp->tx_sync[p];
+		else if (PDELAY_RESP_MSG == msg)
+			tx = &ptp->tx_resp[p];
+		else
+			tx = &ptp->tx_dreq[p];
+
+		/* Same type of event message has been sent. */
+		if (tx->hdr.messageType != msg)
+			continue;
+
+		*more = true;
+
+		/* Have all information ready. */
+		if (xts)
+			break;
+
+		if (!tx->ts.timestamp && ptp->linked[p]) {
+			int rc = wait_event_interruptible_timeout(
+				ptp->wait_ts[p], 0 != tx->ts.timestamp,
+				ptp->delay_ticks);
+
+			if (rc < 0)
+				return NULL;
+		}
+
+		/* Transmit timestamp ready. */
+		if (tx->ts.timestamp) {
+			xts = tx;
+			*port = p;
+			*more = false;
+			tx->ts.timestamp = 0;
+			tx->req_time = 0;
+			tx->hdr.messageType = 7;
+		}
+	}
+	return xts;
+}  /* proc_get_ts_port */
+
+static int proc_ptp_get_msg_info(struct ptp_info *ptp, u8 *data,
+	struct ptp_dev_info *info, int *tx)
+{
+	struct ptp_msg_options *opt = (struct ptp_msg_options *) data;
+
+	if (!*tx) {
+		struct ptp_msg_hdr hdr;
+		struct ptp_msg_options tx_msg;
+		int found;
+
+		memcpy(&hdr.sourcePortIdentity, &opt->id,
+			sizeof(struct ptp_port_identity));
+		hdr.messageType = opt->msg;
+		hdr.sequenceId = opt->seqid;
+		hdr.domainNumber = opt->domain;
+		found = find_msg_info(&ptp->rx_msg_info[hdr.messageType],
+			&ptp->rx_msg_lock, &hdr, &hdr.sourcePortIdentity,
+			true, &tx_msg);
+		if (found) {
+			opt->port = tx_msg.port + 1;
+			opt->ts = tx_msg.ts;
+		} else
+			return DEV_IOC_UNIT_UNAVAILABLE;
+	}
+	else {
+		struct ptp_tx_ts *xts;
+		int port = 0;
+
+		/* Not event message. */
+		if (opt->msg & 0x8)
+			return DEV_IOC_INVALID_CMD;
+
+		xts = proc_get_ts_port(ptp, opt->msg, &port, tx);
+		if (xts) {
+			if (ptp->cap & PTP_KNOW_ABOUT_LATENCY) {
+				opt->ts.t.sec = xts->ts.r.sec;
+				opt->ts.t.nsec = xts->ts.r.nsec;
+			} else {
+				opt->ts.t.sec = xts->ts.t.sec;
+				opt->ts.t.nsec = xts->ts.t.nsec;
+			}
+			opt->port = port + 1;
+		} else
+			return -EAGAIN;
+	}
+	return 0;
+}  /* proc_ptp_get_msg_info */
+
+static int proc_ptp_set_msg_info(struct ptp_info *ptp, u8 *data,
+	struct ptp_dev_info *info)
+{
+	struct ptp_msg_options *opt = (struct ptp_msg_options *) data;
+
+	/* Used for testing. */
+	if (7 == opt->msg) {
+		struct ptp_msg_info *tx_msg;
+
+		tx_msg = &ptp->tx_msg_info[opt->msg];
+		tx_msg->data.port = opt->port;
+		tx_msg->data.ts.timestamp = opt->ts.timestamp;
+		if (opt->port) {
+			ptp->tx_msg_cnt = -1;
+			ptp->overrides |= PTP_USE_DEFAULT_PORT;
+		} else {
+			ptp->tx_msg_cnt = 0;
+			ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+		}
+	} else {
+		struct ptp_msg_hdr hdr;
+
+		memcpy(&hdr.sourcePortIdentity, &opt->id,
+			sizeof(struct ptp_port_identity));
+		hdr.messageType = opt->msg;
+		hdr.sequenceId = opt->seqid;
+		hdr.domainNumber = opt->domain;
+		opt->ts.timestamp = (opt->ts.t.sec << 30) |
+			opt->ts.t.nsec;
+		set_msg_info(ptp, &hdr, opt->port, opt->ts.timestamp);
+	}
+	return 0;
+}  /* proc_ptp_set_msg_info */
+
+static int parse_tsm_msg(struct ptp_dev_info *info, int len)
+{
+	struct ptp_info *ptp = info->ptp;
+	u8 *data = info->write_buf;
+	u8 cmd = data[0] & 0xf0;
+	u8 msg = data[0] & 0x03;
+	int result = 0;
+
+	switch (cmd) {
+	case TSM_CMD_DB_GET_TIME:
+	{
+		struct tsm_get_time *get = (struct tsm_get_time *) data;
+		struct ptp_ts ts;
+
+		ts.timestamp = ntohl(get->nsec);
+		if (ts.timestamp) {
+			update_ts(&ts, ptp->cur_time.sec);
+		} else {
+			ptp->ops->acquire(ptp);
+			ptp->reg->get_time(ptp, &ts.t);
+			ptp->ops->release(ptp);
+		}
+		ptp_setup_udp_msg(info, data, len, ptp_tsm_get_time_resp,
+			&ts.t);
+		break;
+	}
+	case TSM_CMD_DB_GET:
+	{
+		struct tsm_db *db = (struct tsm_db *) data;
+
+		if (db->index <= (1 << 7)) {
+			struct ptp_tx_ts *tx;
+			int port = db->index >> 1;
+
+			if (port > ptp->ports)
+				break;
+			tx = proc_get_ts(ptp, port, msg, ntohs(db->seqid),
+				db->mac, info, len);
+			if (tx) {
+				ptp_setup_udp_msg(info, data, len,
+					ptp_tsm_resp, &tx->ts);
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		}
+		break;
+	}
+	case TSM_CMD_GET_GPS_TS:
+	{
+		/* First time to get GPS timestamp. */
+		if (MAX_TIMESTAMP_UNIT == ptp->gps_tsi) {
+			ptp->gps_tsi = DEFAULT_GPS_TSI;
+			if (ptp->tsi_used & (1 << ptp->gps_tsi))
+				ptp->reg->rx_off(ptp, ptp->gps_tsi);
+			prepare_gps(ptp);
+			ptp->gps_seqid = 0;
+		}
+		ptp->gps_req_time = jiffies;
+		ptp->gps_dev = info;
+		if (ptp->gps_resp_time) {
+			unsigned long diff = ptp->gps_req_time -
+				ptp->gps_resp_time;
+
+			/* The timestamp is not valid. */
+			if (diff >= 2 * ptp->delay_ticks) {
+				dbg_msg("  invalid gps: %lu\n", diff);
+				ptp->gps_time.sec = 0;
+			}
+		}
+		if (ptp->gps_time.sec) {
+			proc_tsm_get_gps(ptp, data);
+			ptp->gps_time.sec = 0;
+			ptp->gps_req_time = 0;
+		} else
+			dbg_msg("  missed gps\n");
+		break;
+	}
+	case TSM_CMD_CNF_SET:
+	{
+		struct tsm_cfg *cfg = (struct tsm_cfg *) data;
+		u32 ingress = htonl(cfg->ingress_delay);
+
+		ptp->ops->acquire(ptp);
+		if (0xFF == cfg->port) {
+			u16 mode;
+
+			if (cfg->enable & 0x04)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+			mode = ptp->mode;
+			if (ptp->overrides & PTP_VERIFY_TIMESTAMP)
+				mode |= PTP_1STEP;
+			set_ptp_mode(ptp, mode);
+			ptp->def_mode = ptp->mode;
+		} else {
+			u8 port = cfg->port - 1;
+
+			if ((cfg->enable & 0x10) && port < ptp->ports &&
+					ptp->peer_delay[port] != ingress) {
+				ptp->peer_delay[port] = ingress;
+				set_ptp_link(ptp, port, ingress);
+			}
+		}
+		ptp->ops->release(ptp);
+		break;
+	}
+	case TSM_CMD_CLOCK_SET:
+	{
+		struct tsm_clock_set *clk = (struct tsm_clock_set *) data;
+		struct ptp_ts ts;
+
+		ts.t.sec = ntohl(clk->sec);
+		ts.t.nsec = ntohl(clk->nsec);
+		ts.timestamp = ntohl(clk->timestamp);
+		ptp->ops->acquire(ptp);
+		set_cur_time(ptp, &ts);
+		ptp->ops->release(ptp);
+		ptp->state = 2;
+		break;
+	}
+	case TSM_CMD_CLOCK_CORRECT:
+	{
+		struct tsm_clock_correct *clk = (struct tsm_clock_correct *)
+			data;
+		u32 drift;
+		u32 nsec;
+		int ptp_offset;
+
+		drift = ntohl(clk->drift);
+		nsec = ntohl(clk->nsec);
+		ptp_offset = ntohl(clk->offset);
+		if (2 == (clk->add >> 4))
+			break;
+
+		ptp->ops->acquire(ptp);
+		if (nsec) {
+			ptp->reg->adjust_time(ptp, !ptp_offset, 0, nsec,
+				ptp->features & PTP_ADJ_HACK);
+			ptp->offset_changed = nsec;
+			if (ptp_offset)
+				ptp->offset_changed = -nsec;
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+		if (clk->add & 1)
+			ptp->drift = drift;
+		else
+			ptp->drift = -drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift,
+			NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+if (!ptp->first_drift)
+dbg_msg("  first drift: %d\n", ptp->drift_set);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+		ptp->ops->release(ptp);
+		break;
+	}
+	default:
+		dbg_msg("tsm cmd: %02X, %d\n", cmd, len);
+	}
+	return result;
+}  /* parse_tsm_msg */
+
+static struct ptp_info *ptp_priv;
+
+static struct ptp_dev_info *alloc_dev_info(unsigned int minor)
+{
+	struct ptp_dev_info *info;
+
+	info = kzalloc(sizeof(struct ptp_dev_info), GFP_KERNEL);
+	if (info) {
+		info->ptp = ptp_priv;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_udp);
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+		info->read_max = 60000;
+		info->read_buf = kzalloc(info->read_max, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = ptp_priv->dev[minor];
+		ptp_priv->dev[minor] = info;
+	}
+	return info;
+}  /* alloc_dev_info */
+
+static void free_dev_info(struct ptp_dev_info *info)
+{
+	if (info) {
+		struct ptp_info *ptp = info->ptp;
+		unsigned int minor = info->minor;
+		struct ptp_dev_info *prev = ptp->dev[minor];
+
+		if (prev == info) {
+			ptp->dev[minor] = info->next;
+		} else {
+			while (prev && prev->next != info)
+				prev = prev->next;
+			if (prev)
+				prev->next = info->next;
+		}
+		kfree(info->read_buf);
+		kfree(info->write_buf);
+		kfree(info);
+	}
+}  /* free_dev_info */
+
+static int ptp_dev_open(struct inode *inode, struct file *filp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	unsigned int minor = MINOR(inode->i_rdev);
+
+	if (minor > 1)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_dev_info(minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* ptp_dev_open */
+
+static int ptp_dev_release(struct inode *inode, struct file *filp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+
+	free_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* ptp_dev_release */
+
+static int execute_wait(struct ptp_work *work)
+{
+	int rc = 0;
+
+	execute(work->ptp, &work->work);
+	wait_for_completion(&work->done);
+	return rc;
+}  /* execute_wait */
+
+static void proc_ptp_work(struct work_struct *work)
+{
+	struct ptp_work *parent =
+		container_of(work, struct ptp_work, work);
+	struct ptp_info *ptp = parent->ptp;
+	struct ptp_dev_info *info = parent->dev_info;
+	u8 *data = parent->param.data;
+	int port;
+	u32 reg;
+	u32 val;
+	size_t width;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+			if (ptp->op_mode)
+				ptp->tx_en = ptp->rx_en = 0;
+			ptp->cap = ptp->op_mode = 0;
+			if ('1' == data[0] &&
+                            '5' == data[1] &&
+                            '8' == data[2] &&
+                            '8' == data[3] &&
+                            'v' == data[4] &&
+                            '2' == data[5]) {
+				int cap = parent->option;
+
+/*
+ * op_mode 0	Original mode of using reserved fields to specify port.
+ * op_mode 1	Have multiple devices and use standard Linux PTP API to receive
+ * 		timestamps.  Do not need to know about PTP messages except the
+ * 		only case of sending 1-step Pdelay_Resp timestamp.  The
+ * 		destination port is already known by other means.
+ * op_mode 2	The destination port is communicated well before sending the
+ * 		PTP message.
+ * op_mode 3	Use single device and do not know about multiple ports.
+ */
+				if ((cap & PTP_HAVE_MULT_DEVICES) &&
+				    (cap & PTP_CAN_RX_TIMESTAMP))
+					ptp->op_mode = 1;
+				else if (cap & PTP_HAVE_MULT_DEVICES)
+					ptp->op_mode = 2;
+				else if (!(cap & (PTP_KNOW_ABOUT_MULT_PORTS |
+				    PTP_HAVE_MULT_PORTS)))
+					ptp->op_mode = 3;
+				else if (cap & PTP_USE_RESERVED_FIELDS)
+					ptp->op_mode = 0;
+				else
+					ptp->op_mode = 2;
+				if (cap & PTP_CAN_RX_TIMESTAMP) {
+					if (cap & PTP_KNOW_ABOUT_LATENCY) {
+						ptp->rx_en &= ~(1 << 8);
+						ptp->tx_en &= ~(1 << 8);
+					} else {
+						ptp->rx_en |= (1 << 8);
+						ptp->tx_en |= (1 << 8);
+					}
+				}
+				ptp->cap = cap;
+dbg_msg("op_mode: %x %d %x\n", ptp->cap, ptp->op_mode, ptp->tx_en);
+			}
+			ptp_init_state(ptp);
+			parent->output = ptp->drift_set;
+			break;
+		case DEV_INFO_EXIT:
+			ptp_exit_state(ptp);
+			break;
+		case DEV_INFO_RESET:
+			reg = parent->option;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			result = proc_ptp_set_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			result = proc_dev_rx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_TOUT:
+			result = proc_dev_tx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CASCADE:
+			if (parent->option)
+				result = proc_ptp_tx_cascade(ptp, data);
+			else
+				result = proc_ptp_tx_cascade_init(ptp, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CLK:
+			if (parent->option)
+				result = proc_ptp_adj_clk(ptp, data,
+					parent->option);
+			else
+				result = proc_ptp_set_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			if (ptp->op_mode > 0 || ptp->cap) {
+				u32 *param = (u32 *) data;
+
+				switch (param[0]) {
+				case 4:
+					width = 4;
+					break;
+				case 1:
+					width = 1;
+					break;
+				default:
+					width = 2;
+					break;
+				}
+				reg = param[1];
+				val = param[2];
+			} else {
+				val = reg >> 16;
+				reg &= 0xffff;
+				width = 2;
+			}
+			ptp->ops->acquire(ptp);
+			switch (width) {
+			case 4:
+				sw->reg->w32(sw, reg, val);
+				break;
+			case 1:
+				sw->reg->w8(sw, reg, val);
+				break;
+			default:
+				sw->reg->w16(sw, reg, val);
+			}
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_peer_delay(ptp, port, data);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			proc_ptp_get_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			if (2 == parent->option)
+				result = proc_dev_get_event_info(info, data);
+			else if (1 == parent->option)
+				result = proc_dev_poll_event(info, data);
+
+			/* Not actually used. */
+			else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			break;
+		case DEV_PTP_CLK:
+			if ((ptp->op_mode > 0 || ptp->cap) && parent->option) {
+				parent->output = ptp->drift_set;
+				break;
+			}
+			proc_ptp_get_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			width = 2;
+			if (ptp->op_mode > 0 || ptp->cap) {
+				u32 *param = (u32 *) data;
+
+				switch (param[0]) {
+				case 4:
+					width = 4;
+					break;
+				case 1:
+					width = 1;
+					break;
+				default:
+					width = 2;
+					break;
+				}
+				reg = param[1];
+				val = param[2];
+			}
+			ptp->ops->acquire(ptp);
+			switch (width) {
+			case 4:
+				parent->output = sw->reg->r32(sw, reg);
+				break;
+			case 1:
+				parent->output = sw->reg->r8(sw, reg);
+				break;
+			default:
+				parent->output = sw->reg->r16(sw, reg);
+			}
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_peer_delay(ptp, port, data);
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_ptp_work */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct ptp_dev_info *info,
+	int *output, int wait)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int ret = 0;
+
+	access = &ptp->hw_access;
+	work = &access->works[access->index];
+	if (work->used) {
+		pr_alert("work full\n");
+		return -EFAULT;
+	}
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->dev_info = info;
+	work->used = true;
+	access->index++;
+	access->index &= PTP_WORK_LAST;
+	init_completion(&work->done);
+	if (!wait) {
+		execute(ptp, &work->work);
+		goto hw_access_end;
+	}
+	ret = execute_wait(work);
+
+	/* Cannot continue if ERESTARTSYS. */
+	if (ret < 0)
+		return ret;
+
+	ret = work->result;
+	if (DEV_IOC_OK == ret && DEV_CMD_GET == work->cmd)
+		memcpy(data, work->param.data, len);
+	*output = work->output;
+
+hw_access_end:
+	return ret;
+}  /* proc_ptp_hw_access */
+
+static void init_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->ptp = ptp;
+		INIT_WORK(&work->work, proc_ptp_work);
+		init_completion(&work->done);
+	}
+}  /* init_ptp_work */
+
+#ifdef CONFIG_PTP_1588_CLOCK
+#include "micrel_ptp.c"
+#endif
+
+static void ptp_chk_rx_events(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	u16 tsi_bit;
+	int i;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		int stop;
+
+		stop = false;
+		tsi_bit = 1 << i;
+		event = &ptp->events[i];
+		if (ptp->tsi_used & tsi_bit) {
+
+			/* At least one event. */
+			if (event->num || event->expired) {
+				if (event->num >= event->max)
+					stop = true;
+				else if (event->expired &&
+					 time_after_eq(jiffies,
+					 event->expired)) {
+					ptp->reg->read_event(ptp, i);
+					stop = true;
+				}
+			}
+		}
+		if ((ptp->ts_status & ptp->ts_intr) & tsi_bit) {
+			u8 data[24];
+
+			if (ptp->tsi_intr & tsi_bit) {
+				data[0] = PTP_CMD_GET_EVENT;
+				data[1] = i;
+				proc_ptp_get_event(ptp, data);
+			}
+			if (i == ptp->gps_tsi && ptp->gps_req_time) {
+				unsigned long diff = jiffies -
+					ptp->gps_req_time;
+
+				if (diff < 2 * ptp->delay_ticks) {
+					data[0] = TSM_CMD_GET_GPS_TS;
+					proc_tsm_get_gps(ptp, data);
+					ptp->gps_time.sec = 0;
+				}
+				ptp->gps_req_time = 0;
+			}
+
+			/* Not used in cascade mode. */
+			if (!event->timeout && !event->last) {
+				event->num = 0;
+				ptp->reg->rx_restart(ptp, i);
+				stop = false;
+			}
+		}
+		if (stop) {
+			ptp->reg->rx_off(ptp, i);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			ptp->tsi_dev[i] = NULL;
+			ptp->events[i].timeout = 0;
+			if (i + 1 == event->last) {
+				int tsi;
+				int last;
+
+				tsi = event->first;
+				last = event->last;
+				do {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].first = 0;
+					ptp->events[tsi].last = 0;
+					++tsi;
+				} while (tsi != last);
+			}
+		}
+	}
+}  /* ptp_chk_rx_events */
+
+static void ptp_update_sec(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, update_sec);
+#if defined(NO_DIRECT_ACCESS)
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+#endif
+
+	if (ptp->update_sec_jiffies) {
+#ifndef NO_SEC_TIMESTAMP
+		ptp->cur_time.sec++;
+#else
+#if defined(NO_DIRECT_ACCESS)
+if (sw->info->iba.use_iba) {
+#endif
+#if 1
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ptp->ops->release(ptp);
+#endif
+#if defined(NO_DIRECT_ACCESS)
+}
+#endif
+#endif
+		ptp->sec_lo++;
+		if (!(ptp->sec_lo & 3)) {
+			check_expired_msg(ptp, ptp->rx_msg_info,
+				&ptp->rx_msg_lock, NULL);
+			check_expired_msg(ptp, ptp->tx_msg_info,
+				&ptp->tx_msg_lock, &ptp->tx_msg_cnt);
+		}
+		ptp->ops->acquire(ptp);
+		ptp_chk_rx_events(ptp);
+		ptp->ops->release(ptp);
+		schedule_delayed_work(&ptp->update_sec, 1000 * HZ / 1000);
+	}
+}  /* ptp_update_sec */
+
+static u32 _get_clk_cnt(void)
+{
+	return 0;
+}
+
+#define ACCESS_VAL			1000
+
+static void test_get_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	ptp->reg->get_time(ptp, cur);
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+	*now_cnt = ptp->get_clk_cnt();
+}
+
+static void test_set_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->set_time(ptp, cur);
+	*now_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+}
+
+static u32 test_avg_time(struct ptp_info *ptp,
+	void (*test_time)(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt))
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_ptp_time diff;
+	int i;
+	int clk_delay[6];
+	u32 cur_cnt;
+	u32 now_cnt;
+	u32 hw_delay[6];
+	u64 clk;
+	u32 rem;
+
+	cur.sec = 5;
+	cur.nsec = 0x12345678;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < 5; i++) {
+		test_time(ptp, &cur, &now, &cur_cnt, &now_cnt);
+		calc_udiff(&cur, &now, &diff);
+		clk_delay[i] = (diff.nsec + (ACCESS_VAL / 2)) / ACCESS_VAL *
+			ACCESS_VAL;
+		hw_delay[i] = now_cnt - cur_cnt;
+	}
+	ptp->ops->release(ptp);
+	clk_delay[5] = 20000000;
+	hw_delay[5] = 50000000;
+	for (i = 0; i < 5; i++) {
+		clk = hw_delay[i];
+		clk *= 1000000;
+		if (ptp->clk_divider)
+			clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+		dbg_msg(" %u %u=%llu\n", clk_delay[i], hw_delay[i], clk);
+		if (clk_delay[i] < clk_delay[5])
+			clk_delay[5] = clk_delay[i];
+		if (hw_delay[i] < hw_delay[5])
+			hw_delay[5] = hw_delay[i];
+	}
+	clk = hw_delay[5];
+	clk *= 1000000;
+	if (ptp->clk_divider)
+		clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+	dbg_msg("%u %llu\n", clk_delay[5], clk);
+	return clk_delay[5];
+}
+
+static void _test_access_time(struct ptp_info *ptp)
+{
+	ptp->get_delay = test_avg_time(ptp, test_get_time);
+	ptp->set_delay = test_avg_time(ptp, test_set_time);
+	if (ptp->get_delay < 10000)
+		ptp->delay_ticks = 10 * HZ / 1000;
+	else if (ptp->get_delay < 12000000)
+		ptp->delay_ticks = 20 * HZ / 1000;
+	else
+		ptp->delay_ticks = 30 * HZ / 1000;
+	dbg_msg("delay_ticks: %lu\n", ptp->delay_ticks);
+}  /* test_access_time */
+
+static void set_ptp_drift(struct ptp_info *ptp, int drift)
+{
+	drift /= 100;
+	drift *= 100;
+	drift = -drift;
+	ptp->first_drift = ptp->drift_set = ptp->drift = drift;
+	ptp->first_sec = 0;
+	ptp->adjust = clk_adjust_val(drift, NANOSEC_IN_SEC);
+	set_ptp_adjust(ptp, ptp->adjust);
+	syntonize_clk(ptp);
+	ptp->ptp_synt = true;
+	dbg_msg("drift: %d\n", drift);
+}  /* set_ptp_drift */
+
+static void check_sys_time(struct ptp_info *ptp, unsigned long cur_jiffies,
+	union ktime cur_ktime)
+{
+	int diff;
+	int interval;
+	u32 cur_clk_cnt;
+
+	cur_clk_cnt = ptp->get_clk_cnt();
+	if (!ptp->first_drift) {
+		interval = 8;
+		diff = ptp->cur_time.sec - ptp->intr_sec;
+
+		/*
+		 * The second interval is not accurate after first setting up
+		 * the clock until later.
+		 */
+		if (diff < 6)
+			ptp->first_sec = 0;
+	} else
+		interval = 10;
+
+	if (!ptp->first_sec) {
+		ptp->last_clk_cnt = cur_clk_cnt;
+		ptp->total_clk_cnt = 0;
+		ptp->last_jiffies = cur_jiffies;
+		ptp->total_jiffies = 0;
+		ptp->first_ktime = cur_ktime;
+		ptp->first_sec = ptp->cur_time.sec;
+		return;
+	}
+
+	diff = ptp->cur_time.sec - ptp->first_sec;
+
+	if (diff >= 1 && !(diff % interval)) {
+		u32 rem;
+		u64 clk;
+		u64 clk_cnt;
+		s64 drift_clk;
+		s64 drift_jiffies;
+		s64 drift_ktime;
+		u32 passed_sec;
+		u64 passed_usec;
+		u64 passed_nsec;
+		u32 cnt;
+
+		cnt = cur_clk_cnt - ptp->last_clk_cnt;
+		ptp->total_clk_cnt += cnt;
+		ptp->last_clk_cnt = cur_clk_cnt;
+
+		passed_sec = ptp->cur_time.sec - ptp->first_sec;
+		passed_usec = passed_sec;
+		passed_usec *= 1000000;
+		passed_nsec = passed_usec;
+		passed_nsec *= 1000;
+
+		cnt = cur_jiffies - ptp->last_jiffies;
+		ptp->total_jiffies += cnt;
+		ptp->last_jiffies = cur_jiffies;
+
+		clk = ptp->total_jiffies * (1000000 / HZ);
+		drift_jiffies = clk - passed_usec;
+		drift_jiffies *= 1000;
+		drift_jiffies = div_s64_rem(drift_jiffies, passed_sec, &rem);
+
+		cur_ktime.tv64 -= ptp->first_ktime.tv64;
+		drift_ktime = cur_ktime.tv64 - passed_nsec;
+		drift_ktime = div_s64_rem(drift_ktime, passed_sec, &rem);
+
+		if (!ptp->clk_divider) {
+			if (!ptp->first_drift)
+				set_ptp_drift(ptp, (int) drift_ktime);
+			else
+				printk(KERN_INFO "%lld %lld\n",
+					drift_jiffies, drift_ktime);
+			return;
+		}
+
+		clk_cnt = div_u64_rem(ptp->total_clk_cnt, passed_sec, &rem);
+
+		clk = ptp->total_clk_cnt * 1000000;
+		clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+		drift_clk = clk;
+		if (drift_clk < 0)
+			ptp->overrides &= ~PTP_CHECK_SYS_TIME;
+		drift_clk -= passed_nsec;
+		drift_clk = div_s64_rem(drift_clk, passed_sec, &rem);
+
+		if (!ptp->first_drift)
+			set_ptp_drift(ptp, (int) drift_clk);
+		else
+			printk(KERN_INFO "%10llu %lld %lld %lld\n",
+				clk_cnt, drift_clk, drift_jiffies, drift_ktime);
+	}
+}  /* check_sys_time */
+
+static int dbg_ts_intr;
+static void proc_ptp_intr(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	u16 done;
+	u16 error;
+	u16 status;
+	u32 trig_status;
+	u32 int_status;
+	u16 tsi_bit;
+	u8 data[24];
+	int i;
+	int tsi;
+	union ktime cur_ktime;
+	struct timespec ts;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	cur_ktime = ktime_get_real();
+	ts = ktime_to_timespec(cur_ktime);
+
+proc_chk_trig_intr:
+	int_status = sw->reg->r32(sw, REG_PTP_INT_STATUS__4);
+	if (!int_status)
+		goto proc_ptp_intr_done;
+
+	sw->reg->w32(sw, REG_PTP_INT_STATUS__4, int_status);
+
+	status = (int_status >> TRIG_INT_S) & PTP_TRIG_UNIT_M;
+	if (!status)
+		goto proc_chk_ts_intr;
+
+	trig_status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+	error = (trig_status >> TRIG_ERROR_S) & PTP_TRIG_UNIT_M;
+	done = trig_status & PTP_TRIG_UNIT_M;
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+		if (status & (1 << i)) {
+			if (ptp->tso_intr & (1 << i)) {
+				data[0] = PTP_CMD_GET_OUTPUT;
+				data[1] = i;
+				proc_ptp_get_trig(ptp, data, done, error);
+			}
+			ptp_tx_done(ptp, i);
+		}
+	}
+
+proc_chk_ts_intr:
+	status = (int_status >> TS_INT_S) & PTP_TS_UNIT_M;
+	if (!status)
+		goto proc_ptp_port_intr;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		tsi_bit = 1 << i;
+		if (!(status & tsi_bit))
+			continue;
+if (!(status & ptp->ts_intr)) {
+printk("  !!\n");
+ptp->reg->rx_reset(ptp, i, NULL);
+}
+		ptp->reg->read_event(ptp, i);
+		event = &ptp->events[i];
+		if (event->timeout &&
+		    (event->num < event->max || event->last)) {
+			unsigned long expired;
+
+			expired = jiffies + event->timeout;
+			if (0 == expired)
+				expired = 1;
+			event->expired = expired;
+			if (event->last) {
+				tsi = i + 1;
+				while (tsi != event->last) {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].expired = expired;
+
+					/* Extend timeout for next unit. */
+					ptp->events[tsi].timeout =
+						event->timeout;
+					++tsi;
+				}
+			}
+		}
+		if (event->last && i != event->first) {
+			tsi = i - 1;
+			if (tsi < 0)
+				tsi = MAX_TIMESTAMP_UNIT - 1;
+			if (ptp->tsi_used & (1 << tsi))
+				ptp->events[tsi].expired = jiffies;
+		}
+
+		/* For system use only. */
+		if (!(ptp->tsi_sys & tsi_bit))
+			continue;
+		if (i == ptp->pps_tsi) {
+			struct ptp_utime sys_time;
+
+			if (event->num > 1)
+if (dbg_ts_intr < 20) {
+dbg_msg(" events %d %x:%9u %x:%9u; %x\n", event->num,
+event->t[0].sec, event->t[0].nsec,
+event->t[event->num - 1].sec, event->t[event->num - 1].nsec,
+ptp->cur_time.sec);
+++dbg_ts_intr;
+dbg_msg(" %d %d; %x %x %x\n", event->timeout, event->last,
+ptp->ts_status, ptp->ts_intr, ptp->tsi_used);
+}
+#ifndef NO_SEC_TIMESTAMP
+			ptp->cur_time.sec = event->t[0].sec;
+			ptp->cur_time.nsec = event->t[0].nsec;
+if (0 == ptp->cur_time.sec)
+printk("  ???  ");
+#if 1
+if (dbg_ts_intr < 3) {
+dbg_msg("%x:%9u\n", ptp->cur_time.sec, ptp->cur_time.nsec);
+++dbg_ts_intr;
+}
+#endif
+#ifndef NO_SEC_TIMESTAMP
+			ptp->update_sec_jiffies = 0;
+#endif
+#endif
+			ptp->sec_lo++;
+			sys_time.sec = ts.tv_sec;
+			sys_time.nsec = ts.tv_nsec;
+			calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+			if (!ptp->intr_sec)
+				ptp->intr_sec = ptp->cur_time.sec;
+#if 0
+			if ((ptp->overrides & PTP_CHECK_SYS_TIME) ||
+					!ptp->first_drift)
+				check_sys_time(ptp, jiffies, cur_ktime);
+#endif
+#ifdef CONFIG_PTP_1588_CLOCK
+			if (ptp->clock_events & (1 << 0))
+				ptp_event_trigger(ptp->clock_info, 0,
+					ptp->cur_time.sec, ptp->cur_time.nsec);
+			if (ptp->clock_events & (1 << 31))
+				ptp_event_pps(ptp->clock_info);
+#endif
+			if (!(ptp->sec_lo & 3)) {
+				check_expired_msg(ptp, ptp->rx_msg_info,
+					&ptp->rx_msg_lock, NULL);
+				check_expired_msg(ptp, ptp->tx_msg_info,
+					&ptp->tx_msg_lock, &ptp->tx_msg_cnt);
+			}
+		} else if (i == ptp->gps_tsi) {
+			ptp->gps_time.sec = event->t[0].sec;
+			ptp->gps_time.nsec = event->t[0].nsec;
+			++ptp->gps_seqid;
+			ptp->gps_resp_time = jiffies;
+		}
+	}
+	ptp_chk_rx_events(ptp);
+	ptp->ts_status = 0;
+
+proc_ptp_port_intr:
+
+	goto proc_chk_trig_intr;
+
+proc_ptp_intr_done:
+	return;
+}  /* proc_ptp_intr */
+
+static void proc_ptp_tx_intr(struct ptp_info *ptp, int port)
+{
+	u32 reg;
+	u16 status;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_TX_INT_STATUS__2);
+	status = sw->reg->r16(sw, reg);
+	if (status) {
+		sw->reg->w16(sw, reg, status);
+		status &= ptp->tx_intr;
+		if (get_tx_time(ptp, port, status))
+			wake_up_interruptible(&ptp->wait_ts[port]);
+	}
+}  /* proc_ptp_tx_intr */
+
+#define PTP_ENABLE_TXTS		SIOCDEVPRIVATE
+#define PTP_DISABLE_TXTS	(SIOCDEVPRIVATE + 1)
+#define PTP_ENABLE_RXTS		(SIOCDEVPRIVATE + 2)
+#define PTP_DISABLE_RXTS	(SIOCDEVPRIVATE + 3)
+#define PTP_GET_TX_TIMESTAMP	(SIOCDEVPRIVATE + 4)
+#define PTP_GET_RX_TIMESTAMP	(SIOCDEVPRIVATE + 5)
+#define PTP_SET_TIME		(SIOCDEVPRIVATE + 6)
+#define PTP_GET_TIME		(SIOCDEVPRIVATE + 7)
+#define PTP_SET_FIPER_ALARM	(SIOCDEVPRIVATE + 8)
+#define PTP_SET_ADJ		(SIOCDEVPRIVATE + 9)
+#define PTP_GET_ADJ		(SIOCDEVPRIVATE + 10)
+#define PTP_CLEANUP_TS		(SIOCDEVPRIVATE + 11)
+#define PTP_ADJ_TIME		(SIOCDEVPRIVATE + 12)
+
+struct ixxat_ptp_time {
+	/* just 48 bit used */
+	u64 sec;
+	u32 nsec;
+};
+
+struct ixxat_ptp_ident {
+	u8 vers;
+	u8 mType;
+	u16 netwProt;
+	u16 seqId;
+	struct ptp_port_identity portId;
+} __packed;
+
+/* needed for timestamp data over ioctl */
+struct ixxat_ptp_data {
+	struct ixxat_ptp_ident ident;
+	struct ixxat_ptp_time ts;
+};
+
+static int ixxat_ptp_ioctl(struct ptp_info *ptp, unsigned int cmd,
+	struct ifreq *ifr)
+{
+	struct ixxat_ptp_time ptp_time;
+	struct ixxat_ptp_data ptp_data;
+	struct ptp_clk_options clk_opt;
+	int output;
+	s64 scaled_nsec;
+	struct ptp_ts ts;
+	struct ptp_tx_ts *tx;
+	int drift;
+	int err = 0;
+	int port;
+
+	switch (cmd) {
+	case PTP_ENABLE_TXTS:
+		ptp->tx_en |= 2;
+		break;
+	case PTP_DISABLE_TXTS:
+		ptp->tx_en &= ~2;
+		break;
+	case PTP_ENABLE_RXTS:
+		ptp->rx_en |= 2;
+		break;
+	case PTP_DISABLE_RXTS:
+		ptp->rx_en &= ~2;
+		break;
+	case PTP_GET_TX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		err = -EINVAL;
+		port = htons(ptp_data.ident.portId.port);
+		if (port < 1 || port > ptp->ports)
+			break;
+		port--;
+		tx = proc_get_ts(ptp, port, ptp_data.ident.mType,
+			ptp_data.ident.seqId,
+			ptp_data.ident.portId.clockIdentity.addr,
+			NULL, 0);
+		if (!tx)
+			break;
+		ptp_data.ts.sec = tx->ts.r.sec;
+		ptp_data.ts.nsec = tx->ts.r.nsec;
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_RX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		ts.timestamp = ptp_data.ts.nsec;
+		if (ts.timestamp)
+			update_ts(&ts, ptp->cur_time.sec);
+		else {
+			struct ptp_msg_hdr hdr;
+			struct ptp_msg_options tx_msg;
+			int found;
+
+			ts.t.sec = ts.t.nsec = 0;
+			memcpy(&hdr.sourcePortIdentity, &ptp_data.ident.portId,
+				sizeof(struct ptp_port_identity));
+			hdr.messageType = ptp_data.ident.mType;
+			hdr.sequenceId = htons(ptp_data.ident.seqId);
+			hdr.domainNumber = ptp_data.ident.vers;
+#if 0
+printk("%02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x.%04x %x %04x %x\n",
+hdr.sourcePortIdentity.clockIdentity.addr[0],
+hdr.sourcePortIdentity.clockIdentity.addr[1],
+hdr.sourcePortIdentity.clockIdentity.addr[2],
+hdr.sourcePortIdentity.clockIdentity.addr[3],
+hdr.sourcePortIdentity.clockIdentity.addr[4],
+hdr.sourcePortIdentity.clockIdentity.addr[5],
+hdr.sourcePortIdentity.clockIdentity.addr[6],
+hdr.sourcePortIdentity.clockIdentity.addr[7],
+hdr.sourcePortIdentity.port,
+hdr.messageType, hdr.sequenceId, hdr.domainNumber);
+#endif
+			found = find_msg_info(&ptp->rx_msg_info[
+				hdr.messageType],
+				&ptp->rx_msg_lock, &hdr,
+				&hdr.sourcePortIdentity,
+				PDELAY_REQ_MSG != hdr.messageType, &tx_msg);
+			if (found) {
+				ts.t = tx_msg.ts.t;
+			}
+		}
+#if 0
+printk("%u:%09u\n", ts.t.sec, ts.t.nsec);
+#endif
+		ptp_data.ts.sec = ts.t.sec;
+		ptp_data.ts.nsec = ts.t.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_TIME:
+	{
+		struct timespec ts;
+		struct ksz_ptp_time cur_time;
+		struct ksz_ptp_time sys_time;
+
+		ts = ktime_to_timespec(ktime_get_real());
+		sys_time.sec = ts.tv_sec;
+		sys_time.nsec = ts.tv_nsec;
+		calc_diff(&ptp->time_diff, &sys_time, &cur_time);
+		ptp_time.sec = cur_time.sec;
+		ptp_time.nsec = cur_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_GET, DEV_PTP_CLK, 0,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		if (err)
+			break;
+		ptp_time.sec = clk_opt.sec;
+		ptp_time.nsec = clk_opt.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_time, sizeof(ptp_time));
+		break;
+	}
+	case PTP_SET_TIME:
+		if (copy_from_user(&ptp_time, ifr->ifr_data, sizeof(ptp_time)))
+			return -EFAULT;
+		output = 0;
+		clk_opt.sec = (u32) ptp_time.sec;
+		clk_opt.nsec = ptp_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_ADJ_TIME:
+		if (copy_from_user(&scaled_nsec, ifr->ifr_data, sizeof(s64)))
+			return -EFAULT;
+		convert_scaled_nsec(scaled_nsec, SCALED_NANOSEC_S,
+			&ptp->adjust_sec, &ptp->adjust_offset);
+		if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+			output = 1;
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		} else
+			output = 2;
+		clk_opt.sec = (u32) ptp->adjust_sec;
+		clk_opt.nsec = ptp->adjust_offset;
+		clk_opt.interval = 0;
+		ptp->adjust_sec = 0;
+		ptp->adjust_offset = 0;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_SET_ADJ:
+		if (copy_from_user(&drift, ifr->ifr_data, sizeof(drift)))
+			return -EFAULT;
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+		clk_opt.drift = drift;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_GET_ADJ:
+		drift = ptp->drift_set;
+		err = copy_to_user(ifr->ifr_data, &drift, sizeof(drift));
+		break;
+	case PTP_CLEANUP_TS:
+		break;
+	case PTP_SET_FIPER_ALARM:
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+	return err;
+}
+
+static int ptp_dev_req(struct ptp_info *ptp, char *arg,
+	struct ptp_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	struct ptp_dev_info *dev;
+	int err = 0;
+	int result = 0;
+	int v2 = 0;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+		{
+			struct ksz_sw *sw = container_of(ptp,
+				struct ksz_sw, ptp_hw);
+
+			if (chk_ioctl_size(len, 6,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (len < 6) {
+				result = DEV_IOC_INVALID_LEN;
+				break;
+			}
+
+			if (len >= 8 &&
+			    'v' == data[4] && '2' == data[5])
+				v2 = 1;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			data[0] = 'M';
+			data[1] = 'i';
+			data[2] = 'c';
+			data[3] = 'r';
+			data[4] = ptp->version;
+			data[5] = ptp->ports;
+			if (v2) {
+				data[6] = sw->HOST_PORT + 1;
+				data[7] = 0;
+			} else
+				data[5] = sw->HOST_PORT;
+			if (!access_ok(VERIFY_WRITE, req->param.data, len) ||
+			    copy_to_user(req->param.data, data, len)) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		case DEV_INFO_EXIT:
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, 0, info, &output,
+				true);
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+			if (!info)
+				break;
+			data[0] = 0xF0;
+			dev = find_minor_dev(info);
+			if (dev)
+				ptp_setup_udp_msg(dev, data, 4, NULL, NULL);
+			ptp_setup_udp_msg(info, data, 4, NULL, NULL);
+			break;
+		case DEV_INFO_RESET:
+			if (output < 3) {
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, 0, info, &output,
+					false);
+			} else
+				result = -EINVAL;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CASCADE:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_REG:
+			if ((ptp->op_mode > 0 || ptp->cap) &&
+			    chk_ioctl_size(len,
+					sizeof(u32) * 3,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_IDENTITY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_clock_identity),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if ((ptp->op_mode > 0 || ptp->cap) && output) {
+				memcpy(&ptp->masterIdentity, data,
+					sizeof(struct ptp_clock_identity));
+				break;
+			}
+			memcpy(&ptp->clockIdentity, data,
+				sizeof(struct ptp_clock_identity));
+			break;
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			ptp->utc_offset = output;
+			break;
+		case DEV_PTP_MSG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_msg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_set_msg_info(ptp, data, info);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_cfg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_cfg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_info),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			if (output) {
+				int wait = false;
+
+				if (2 == output)
+					wait = true;
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, len, info, &output,
+					wait);
+				if (!wait)
+					break;
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+				    len) || copy_to_user(req->param.data,
+				    data, len)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+			} else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_output(ptp, data);
+			output = *((int *) data);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			if (0 == ptp->op_mode && !ptp->cap)
+				output = 0;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_clk_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_clk_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_REG:
+			if ((ptp->op_mode > 0 || ptp->cap) &&
+			    chk_ioctl_size(len,
+					sizeof(u32) * 3,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_IDENTITY:
+		{
+			struct ptp_clock_identity *id = &ptp->clockIdentity;
+
+			if ((ptp->op_mode > 0 || ptp->cap) && output)
+				id = &ptp->masterIdentity;
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_clock_identity)) ||
+					copy_to_user(req->param.data, id,
+					sizeof(struct ptp_clock_identity))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			__put_user(ptp->utc_offset, &req->output);
+			break;
+		case DEV_PTP_TIMESTAMP:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_ts_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_timestamp(ptp, data, info);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_ts_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_ts_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_MSG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_msg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_msg_info(ptp, data, info,
+				&output);
+			__put_user(output, &req->output);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_msg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_msg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* ptp_dev_req */
+
+#ifdef HAVE_UNLOCKED_IOCTL
+static long ptp_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+#else
+static int ptp_dev_ioctl(struct inode *inode, struct file *filp,
+	unsigned int cmd, unsigned long arg)
+#endif
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	struct ptp_info *ptp = info->ptp;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok(VERIFY_WRITE, (void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok(VERIFY_READ, (void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = ptp_dev_req(ptp, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* ptp_dev_ioctl */
+
+static ssize_t ptp_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_udp,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_read */
+
+static ssize_t ptp_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+	u8 cmd;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	cmd = info->write_buf[0] & 0xf0;
+	switch (cmd) {
+	case TSM_CMD_GET_GPS_TS:
+		size = sizeof(struct tsm_get_gps);
+		break;
+	case TSM_CMD_DB_GET_TIME:
+		size = sizeof(struct tsm_get_time);
+		break;
+	case TSM_CMD_DB_GET:
+		size = sizeof(struct tsm_db);
+		break;
+	case TSM_CMD_CNF_SET:
+		size = sizeof(struct tsm_cfg);
+		break;
+	case TSM_CMD_CLOCK_SET:
+		size = sizeof(struct tsm_clock_set);
+		break;
+	case TSM_CMD_CLOCK_CORRECT:
+		size = sizeof(struct tsm_clock_correct);
+		break;
+	default:
+		dbg_msg("tsm: %x\n", info->write_buf[0]);
+		result = count;
+		goto dev_write_done;
+	}
+	if (count < size) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	result = size;
+	rc = parse_tsm_msg(info, count);
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_write */
+
+static const struct file_operations ptp_dev_fops = {
+	.read		= ptp_dev_read,
+	.write		= ptp_dev_write,
+#ifdef HAVE_UNLOCKED_IOCTL
+	.unlocked_ioctl	= ptp_dev_ioctl,
+#else
+	.ioctl		= ptp_dev_ioctl,
+#endif
+	.open		= ptp_dev_open,
+	.release	= ptp_dev_release,
+};
+
+static struct class *ptp_class;
+
+static int init_ptp_device(int dev_major, char *dev_name, char *minor_name)
+{
+	int result;
+
+	printk(KERN_INFO "  PTP driver %s %s\n", __DATE__, __TIME__);
+	result = register_chrdev(dev_major, dev_name, &ptp_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	ptp_class = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(ptp_class)) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(ptp_class, NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	device_create(ptp_class, NULL, MKDEV(dev_major, 1), NULL, minor_name);
+	return dev_major;
+}  /* init_ptp_device */
+
+static void exit_ptp_device(int dev_major, char *dev_name)
+{
+	device_destroy(ptp_class, MKDEV(dev_major, 1));
+	device_destroy(ptp_class, MKDEV(dev_major, 0));
+	class_destroy(ptp_class);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_ptp_device */
+
+static void ptp_check(struct ptp_info *ptp)
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+#if defined(NO_DIRECT_ACCESS)
+do {
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+	ptp->version = 2;
+return;
+}
+} while (0);
+#endif
+	ptp->features |= PTP_ADJ_HACK;
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &cur);
+	ptp->reg->adjust_time(ptp, true, 10, 0, true);
+	ptp->reg->get_time(ptp, &now);
+dbg_msg("%08x:%08x %08x:%08x\n", cur.sec, cur.nsec, now.sec, now.nsec);
+	if (now.sec - cur.sec >= 10) {
+		ptp->features &= ~PTP_ADJ_HACK;
+		ptp->features |= PTP_ADJ_SEC;
+		ptp->reg->adjust_time(ptp, false, 10, 0, true);
+		ptp->version = 1;
+	}
+/*
+ * THa  2013/01/08
+ * The Rev. D chip has a problem of decrementing nanosecond that is bigger than
+ * the current nanosecond when continual clock adjustment is enabled.  The
+ * workaround is to use the PTP_ADJ_HACK code although the actual problem
+ * avoided is now different.
+ */
+	if (!(ptp->features & PTP_ADJ_HACK)) {
+		u16 data;
+
+		data = sw->cached.ptp_clk_ctrl;
+		sw->cached.ptp_clk_ctrl |= PTP_CLK_ADJ_ENABLE;
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, sw->cached.ptp_clk_ctrl);
+		if (cur.sec < 1)
+			cur.sec = 1;
+		cur.nsec = 0;
+		ptp->reg->set_time(ptp, &cur);
+		ptp->reg->adjust_time(ptp, false, 0, 800000000, false);
+		ptp->reg->get_time(ptp, &now);
+		dbg_msg("%x:%u %x:%u\n", cur.sec, cur.nsec, now.sec, now.nsec);
+		if (abs(now.sec - cur.sec) > 2) {
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("! %x:%u\n", now.sec, now.nsec);
+			ptp->features |= PTP_ADJ_HACK;
+			sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+
+			sw->reg->w16(sw, REG_PTP_CLK_CTRL,
+				data | PTP_CLK_ADJ_ENABLE);
+			ptp->reg->set_time(ptp, &cur);
+			ptp->reg->adjust_time(ptp, false, 0, 800000000, true);
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("ok %x:%u\n", now.sec, now.nsec);
+		}
+		sw->cached.ptp_clk_ctrl = data;
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+	}
+	ptp->version = 2;
+	ptp->ops->release(ptp);
+}  /* ptp_check */
+
+static void ptp_set_identity(struct ptp_info *ptp, u8 *addr)
+{
+	memcpy(&ptp->clockIdentity.addr[0], &addr[0], 3);
+	ptp->clockIdentity.addr[3] = 0xFF;
+	ptp->clockIdentity.addr[4] = 0xFE;
+	memcpy(&ptp->clockIdentity.addr[5], &addr[3], 3);
+}  /* ptp_set_identity */
+
+static void ptp_init(struct ptp_info *ptp, u8 *mac_addr)
+{
+	int i;
+
+	ptp->get_delay = 100000;
+	ptp->set_delay = 100000;
+	ptp->delay_ticks = 2;
+	ptp->access = create_singlethread_workqueue("ptp_access");
+	init_ptp_work(ptp);
+	mutex_init(&ptp->lock);
+	for (i = 0; i < MAX_PTP_PORT; i++)
+		init_waitqueue_head(&ptp->wait_ts[i]);
+	init_waitqueue_head(&ptp->wait_intr);
+	INIT_WORK(&ptp->adj_clk, adj_clock);
+	INIT_WORK(&ptp->set_latency, set_latency);
+	INIT_DELAYED_WORK(&ptp->check_pps, ptp_check_pps);
+	INIT_DELAYED_WORK(&ptp->update_sec, ptp_update_sec);
+	ptp_set_identity(ptp, mac_addr);
+
+#ifndef ACL_TEST
+	ptp->mode = PTP_ENABLE |
+		PTP_IPV4_UDP_ENABLE |
+		PTP_1STEP;
+	ptp->mode |= PTP_IPV6_UDP_ENABLE;
+	ptp->mode |= PTP_ETH_ENABLE;
+#else
+	ptp->mode = PTP_1STEP;
+#endif
+	ptp->cfg = 0;
+	ptp->cfg |= PTP_UNICAST_ENABLE;
+	ptp->cfg |= PTP_UDP_CHECKSUM;
+#if 1
+	ptp->cfg |= PTP_DOMAIN_CHECK;
+	ptp->cfg |= PTP_PDELAY_CHECK | PTP_DELAY_CHECK;
+	ptp->cfg |= PTP_SYNC_CHECK;
+#endif
+	ptp->def_mode = ptp->mode;
+	ptp->def_cfg = ptp->cfg;
+	ptp->trig_intr = PTP_TRIG_UNIT_M;
+	ptp->tx_intr = PTP_PORT_XDELAY_REQ_INT;
+#if 0
+	ptp->tx_intr = PTP_PORT_XDELAY_REQ_INT | PTP_PORT_PDELAY_RESP_INT |
+		PTP_PORT_SYNC_INT;
+#endif
+
+#if !defined(NO_DIRECT_ACCESS)
+	ptp_hw_enable(ptp);
+#endif
+	if (!ptp->ports)
+		ptp->ports = MAX_PTP_PORT;
+	ptp_check(ptp);
+	if (!ptp->get_clk_cnt)
+		ptp->get_clk_cnt = _get_clk_cnt;
+	if (!ptp->test_access_time)
+		ptp->test_access_time = _test_access_time;
+#if !defined(NO_DIRECT_ACCESS)
+	if (ptp->test_access_time)
+		ptp->test_access_time(ptp);
+#endif
+
+	ptp->gps_tsi = MAX_TIMESTAMP_UNIT;
+	ptp->gps_gpi = DEFAULT_GPS_GPI;
+	ptp->pps_gpo = DEFAULT_PPS_GPO;
+	ptp->pps_tsi = DEFAULT_PPS_TSI;
+	ptp->pps_tso = DEFAULT_PPS_TSO;
+	ptp->mhz_gpo = DEFAULT_MHZ_GPO;
+	ptp->mhz_tso = DEFAULT_MHZ_TSO;
+#if 0
+	ptp->pps_gpo = MAX_GPIO;
+#endif
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++)
+		ptp->events[i].max = MAX_TIMESTAMP_EVENT_UNIT;
+
+	init_msg_info(ptp->rx_msg_info, &ptp->rx_msg_lock);
+	init_msg_info(ptp->tx_msg_info, &ptp->tx_msg_lock);
+	ptp_init_hw(ptp);
+	do {
+		struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+		for (i = sw->phy_port_cnt; i < ptp->ports; i++)
+			ptp->linked[i] = 1000;
+	} while (0);
+
+	ptp_priv = ptp;
+	sprintf(ptp->dev_name[0], "ptp_dev");
+	sprintf(ptp->dev_name[1], "ptp_event");
+	ptp->dev_major = init_ptp_device(0, ptp->dev_name[0],
+		ptp->dev_name[1]);
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	micrel_ptp_probe(ptp);
+#endif
+}  /* ptp_init */
+
+static void ptp_exit(struct ptp_info *ptp)
+{
+	ptp->ops->acquire(ptp);
+	ptp->tx_intr = 0;
+	ptp_tx_intr_enable(ptp);
+	ptp->ops->release(ptp);
+
+	if (ptp->access) {
+		destroy_workqueue(ptp->access);
+		ptp->access = NULL;
+	}
+	if (ptp->dev_major >= 0)
+		exit_ptp_device(ptp->dev_major, ptp->dev_name[0]);
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	micrel_ptp_remove(ptp);
+#endif
+}  /* ptp_exit */
+
+enum {
+	PROC_SET_PTP_FEATURES,
+	PROC_SET_PTP_OVERRIDES,
+	PROC_SET_PTP_VID,
+};
+
+static ssize_t sysfs_ptp_read(struct ptp_info *ptp, int proc_num, ssize_t len,
+	char *buf)
+{
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", ptp->features);
+		len += sprintf(buf + len, "\t%08x = adjust hack\n",
+			PTP_ADJ_HACK);
+		len += sprintf(buf + len, "\t%08x = adjust sec\n",
+			PTP_ADJ_SEC);
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", ptp->overrides);
+		len += sprintf(buf + len, "\t%08x = PTP port forwarding\n",
+			PTP_PORT_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP port TX forwarding\n",
+			PTP_PORT_TX_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP verify timestamp\n",
+			PTP_VERIFY_TIMESTAMP);
+		len += sprintf(buf + len, "\t%08x = PTP zero reserved field\n",
+			PTP_ZERO_RESERVED_FIELD);
+		len += sprintf(buf + len, "\t%08x = PTP check system time\n",
+			PTP_CHECK_SYS_TIME);
+		len += sprintf(buf + len, "\t%08x = PTP check sync time\n",
+			PTP_CHECK_SYNC_TIME);
+		break;
+	case PROC_SET_PTP_VID:
+		len += sprintf(buf + len, "0x%04x\n", ptp->vid);
+		break;
+	}
+	return len;
+}
+
+static void sysfs_ptp_write(struct ptp_info *ptp, int proc_num, int num,
+	const char *buf)
+{
+	int changes;
+
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->features ^ num;
+		ptp->features = num;
+#ifdef PTP_PROCESS
+		if ((changes & (PTP_SYNT | PTP_SIM_2_STEP))) {
+#ifdef PTP_2_STEP
+			if (num & PTP_SIM_2_STEP) {
+				ptp->sim_2_step = true;
+				ptp->mode &= ~PTP_1STEP;
+			} else {
+				ptp->sim_2_step = false;
+				ptp->mode |= PTP_1STEP;
+			}
+#endif
+			if (num & (PTP_SYNT | PTP_SIM_2_STEP)) {
+				ptp_init_state(ptp);
+				if (num & PTP_SYNT) {
+					ptp->sim = 1;
+					ptp->I = 0;
+					ptp->KP = 50;
+					ptp->KI = 5;
+				}
+			} else {
+				ptp_exit_state(ptp);
+				dbg_msg("exit ptp\n");
+			}
+		}
+#endif
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->overrides ^ num;
+		if ((changes & PTP_CHECK_SYS_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYS_TIME))
+			ptp->first_sec = 0;
+#if 1
+		if ((changes & PTP_CHECK_SYNC_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYNC_TIME)) {
+			last_rcv.sec = 0;
+			first_recv = 0;
+			first_sync = 0;
+		}
+#endif
+		ptp->overrides = num;
+		break;
+	case PROC_SET_PTP_VID:
+		ptp->vid = num;
+		break;
+	}
+}
+
+static struct ptp_reg_ops ptp_reg_ops = {
+	.get_time		= get_ptp_time,
+	.set_time		= set_ptp_time,
+	.adjust_time		= adjust_ptp_time,
+	.adjust_sync_time	= adjust_sync_time,
+
+	.rx_off			= ptp_rx_off,
+	.rx_reset		= ptp_rx_reset,
+	.rx_restart		= ptp_rx_restart,
+	.rx_event		= ptp_rx_event,
+	.rx_cascade_event	= ptp_rx_cascade_event,
+	.read_event		= ptp_read_event,
+
+	.tx_off			= ptp_tx_off,
+	.tx_event		= ptp_tx_event,
+	.pps_event		= ptp_pps_event,
+	.ptp_10MHz		= ptp_10MHz,
+	.tx_cascade		= ptp_tx_cascade,
+
+	.start			= ptp_start,
+};
+
+static struct ptp_ops ptp_ops = {
+	.acquire		= ptp_acquire,
+	.release		= ptp_release,
+
+	.init			= ptp_init,
+	.exit			= ptp_exit,
+
+	.stop			= ptp_stop,
+	.set_identity		= ptp_set_identity,
+
+	.check_msg		= check_ptp_msg,
+	.get_rx_tstamp		= get_rx_tstamp,
+	.get_tx_tstamp		= get_tx_tstamp,
+	.hwtstamp_ioctl		= ptp_hwtstamp_ioctl,
+	.ixxat_ioctl		= ixxat_ptp_ioctl,
+	.dev_req		= ptp_dev_req,
+	.proc_intr		= proc_ptp_intr,
+
+	.sysfs_read		= sysfs_ptp_read,
+	.sysfs_write		= sysfs_ptp_write,
+
+	.drop_pkt		= ptp_drop_pkt,
+	.get_rx_info		= ptp_get_rx_info,
+	.set_tx_info		= ptp_set_tx_info,
+};
+
+
diff --git a/drivers/net/ethernet/micrel/ksz_ptp_9897.h b/drivers/net/ethernet/micrel/ksz_ptp_9897.h
new file mode 100644
index 0000000..35339eb
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_ptp_9897.h
@@ -0,0 +1,1116 @@
+/**
+ * Micrel PTP common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_PTP_H
+#define KSZ_PTP_H
+
+#ifndef __KERNEL__
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef short s16;
+typedef long long s64;
+typedef unsigned long long u64;
+#endif
+
+struct ksz_ptp_time {
+	int sec;
+	int nsec;
+};
+
+struct ptp_utime {
+	u32 sec;
+	u32 nsec;
+};
+
+struct ptp_ts {
+	struct ptp_utime r;
+	struct ptp_utime t;
+	u32 timestamp;
+};
+
+struct ptp_second {
+	u16 hi;
+	u32 lo;
+} __packed;
+
+struct ptp_timestamp {
+	struct ptp_second sec;
+	u32 nsec;
+} __packed;
+
+#define SCALED_NANOSEC_S		16
+#define SCALED_NANOSEC_MULT		(1 << SCALED_NANOSEC_S)
+
+struct ptp_scaled_ns {
+	int hi;
+	s64 lo;
+} __packed;
+
+struct ptp_correction {
+	int scaled_nsec_hi;
+	int scaled_nsec_lo;
+} __packed;
+
+struct ptp_clock_identity {
+	u8 addr[8];
+};
+
+struct ptp_port_identity {
+	struct ptp_clock_identity clockIdentity;
+	u16 port;
+} __packed;
+
+struct ptp_clock_quality {
+	u8 clockClass;
+	u8 clockAccuracy;
+	u16 offsetScaledLogVariance;
+} __packed;
+
+struct ptp_port_address {
+	u16 networkProtocol;
+	u16 addressLength;
+	u8 addressField[1];
+} __packed;
+
+struct ptp_text {
+	u8 lengthField;
+	u8 textField[1];
+} __packed;
+
+#define SYNC_MSG			0x0
+#define DELAY_REQ_MSG			0x1
+#define PDELAY_REQ_MSG			0x2
+#define PDELAY_RESP_MSG			0x3
+#define FOLLOW_UP_MSG			0x8
+#define DELAY_RESP_MSG			0x9
+#define PDELAY_RESP_FOLLOW_UP_MSG	0xA
+#define ANNOUNCE_MSG			0xB
+#define SIGNALING_MSG			0xC
+#define MANAGEMENT_MSG			0xD
+
+struct ptp_msg_hdr {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 transportSpecific:4;
+	u8 messageType:4;
+	u8 reserved1:4;
+	u8 versionPTP:4;
+#else
+	u8 messageType:4;
+	u8 transportSpecific:4;
+	u8 versionPTP:4;
+	u8 reserved1:4;
+#endif
+	u16 messageLength;
+	u8 domainNumber;
+	u8 reserved2;
+	union {
+		struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+			u8 reservedFlag7:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag4:1;
+			u8 reservedFlag3:1;
+			u8 unicastFlag:1;
+			u8 twoStepFlag:1;
+			u8 alternateMasterFlag:1;
+			u8 reservedFlag6:1;
+			u8 reservedFlag5:1;
+			u8 frequencyTraceable:1;
+			u8 timeTraceable:1;
+			u8 ptpTimescale:1;
+			u8 utcOffsetValid:1;
+			u8 leap59:1;
+			u8 leap61:1;
+#else
+			u8 alternateMasterFlag:1;
+			u8 twoStepFlag:1;
+			u8 unicastFlag:1;
+			u8 reservedFlag3:1;
+			u8 reservedFlag4:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag7:1;
+			u8 leap61:1;
+			u8 leap59:1;
+			u8 utcOffsetValid:1;
+			u8 ptpTimescale:1;
+			u8 timeTraceable:1;
+			u8 frequencyTraceable:1;
+			u8 reservedFlag5:1;
+			u8 reservedFlag6:1;
+#endif
+		} __packed flag;
+		u16 data;
+	} __packed flagField;
+	struct ptp_correction correctionField;
+	u32 reserved3;
+	struct ptp_port_identity sourcePortIdentity;
+	u16 sequenceId;
+	u8 controlField;
+	char logMessageInterval;
+} __packed;
+
+struct ptp_msg_sync {
+	struct ptp_timestamp originTimestamp;
+} __packed;
+
+struct ptp_msg_follow_up {
+	struct ptp_timestamp preciseOriginTimestamp;
+} __packed;
+
+struct ptp_msg_delay_resp {
+	struct ptp_timestamp receiveTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_req {
+	struct ptp_timestamp originTimestamp;
+	struct ptp_port_identity reserved;
+} __packed;
+
+struct ptp_msg_pdelay_resp {
+	struct ptp_timestamp requestReceiptTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_resp_follow_up {
+	struct ptp_timestamp responseOriginTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+#define TLV_MANAGEMENT					0x0001
+#define TLV_MANAGEMENT_ERROR_STATUS			0x0002
+#define TLV_ORGANIZATION_EXTENSION			0x0003
+#define TLV_REQUEST_UNICAST_TRANSMISSION		0x0004
+#define TLV_GRANT_UNICAST_TRANSMISSION			0x0005
+#define TLV_CANCEL_UNICAST_TRANSMISSION			0x0006
+#define TLV_ACKNOWLEDGE_CANCEL_UNICAST_TRANSMISSION	0x0007
+#define TLV_PATH_TRACE					0x0008
+#define TLV_ALTERNATE_TIME_OFFSET_INDICATOR		0x0009
+
+struct ptp_tlv {
+	u16 tlvType;
+	u16 lengthField;
+} __packed;
+
+struct ptp_organization_ext_tlv {
+	struct ptp_tlv tlv;
+	u8 organizationId[3];
+	u8 organizationSubType[3];
+	u8 dataField[1];
+} __packed;
+
+struct IEEE_C37_238_data {
+	u16 grandmasterID;
+	u32 grandmasterTimeInaccuracy;
+	u32 networkTimeInaccuracy;
+	u16 reserved;
+} __packed;
+
+struct IEEE_802_1AS_data_1 {
+	int cumulativeScaledRateOffset;
+	u16 gmTimeBaseIndicator;
+	struct ptp_scaled_ns lastGmPhaseChange;
+	int scaledLastGmFreqChange;
+} __packed;
+
+struct IEEE_802_1AS_data_2 {
+	char linkDelayInterval;
+	char timeSyncInterval;
+	char announceInterval;
+	u8 flags;
+	u16 reserved;
+} __packed;
+
+struct ptp_request_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+} __packed;
+
+struct ptp_grant_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+	u8 reserved2;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved3:7;
+	u8 renewal:1;
+#else
+	u8 renewal:1;
+	u8 reserved3:7;
+#endif
+} __packed;
+
+struct ptp_cancel_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_alternate_time_offset_tlv {
+	struct ptp_tlv tlv;
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_msg_signaling_base {
+	struct ptp_port_identity targetPortIdentity;
+} __packed;
+
+struct ptp_msg_signaling {
+	struct ptp_msg_signaling_base b;
+	union {
+		struct ptp_request_unicast_tlv request[1];
+		struct ptp_grant_unicast_tlv grant[1];
+		struct ptp_cancel_unicast_tlv cancel[1];
+	} tlv;
+} __packed;
+
+#define M_NULL_MANAGEMENT				0x0000
+#define M_CLOCK_DESCRIPTION				0x0001
+#define M_DEFAULT_DATA_SET				0x2000
+#define M_CURRENT_DATA_SET				0x2001
+#define M_PARENT_DATA_SET				0x2002
+#define M_PORT_DATA_SET					0x2004
+#define M_PRIORITY1					0x2005
+#define M_PRIORITY2					0x2006
+#define M_DOMAIN					0x2007
+#define M_SLAVE_ONLY					0x2008
+#define M_VERSION_NUMBER				0x200C
+#define M_ENABLE_PORT					0x200D
+#define M_DISABLE_PORT					0x200E
+#define M_TIME						0x200F
+#define M_UNICAST_NEGOTIATION_ENABLE			0x2014
+#define M_PATH_TRACE_LIST				0x2015
+#define M_PATH_TRACE_ENABLE				0x2016
+#define M_GRANDMASTER_CLUSTER_TABLE			0x2017
+#define M_UNICAST_MASTER_TABLE				0x2018
+#define M_UNICAST_MASTER_MAX_TABLE_SIZE			0x2019
+#define M_ACCEPTABLE_MASTER_TABLE			0x201A
+#define M_ACCEPTABLE_MASTER_TABLE_ENABLED		0x201B
+#define M_ACCEPTABLE_MASTER_MAX_TABLE_SIZE		0x201C
+#define M_ALTERNATE_MASTER				0x201D
+#define M_ALTERNATE_TIME_OFFSET_ENABLE			0x201E
+#define M_ALTERNATE_TIME_OFFSET_NAME			0x201F
+#define M_ALTERNATE_TIME_OFFSET_MAX_KEY			0x2020
+#define M_ALTERNATE_TIME_OFFSET_PROPERTIES		0x2021
+
+struct ptp_management_unicast_negotiation {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_management_unicast_master_table {
+	u8 logQueryInterval;
+	u16 tableSize;
+	struct ptp_port_address unicastMasterTable[1];
+} __packed;
+
+struct ptp_management_unicast_master_max_table_size {
+	u16 maxTableSize;
+} __packed;
+
+struct ptp_management_alternate_time_offset {
+	u8 keyField;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+} __packed;
+
+struct ptp_management_alternate_time_offset_name {
+	u8 keyField;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_management_alternate_time_offset_max_key {
+	u8 keyField;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_alternate_time_offset_properties {
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_tlv {
+	struct ptp_tlv tlv;
+	u16 managementId;
+	u8 dataField[1];
+} __packed;
+
+#define M_RESPONSE_TOO_BIG		0x0001
+#define M_NO_SUCH_ID			0x0002
+#define M_WRONG_LENGTH			0x0003
+#define M_WRONG_VALUE			0x0004
+#define M_NOT_SETABLE			0x0005
+#define M_NOT_SUPPORTED			0x0006
+#define M_GENERAL_ERROR			0xFFFE
+
+struct ptp_management_error_tlv {
+	struct ptp_tlv tlv;
+	u16 managementErrorId;
+	u16 managementId;
+	u32 reserved1;
+	u8 data[1];
+} __packed;
+
+#define MANAGEMENT_GET			0
+#define MANAGEMENT_SET			1
+#define MANAGEMENT_RESPONSE		2
+#define MANAGEMENT_COMMAND		3
+#define MANAGEMENT_ACKNOWLEDGE		4
+
+struct ptp_msg_management_base {
+	struct ptp_port_identity targetPortIdentity;
+	u8 startingBoundaryHops;
+	u8 boundaryHops;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:4;
+	u8 actionField:4;
+#else
+	u8 actionField:4;
+	u8 reserved1:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_msg_management {
+	struct ptp_msg_management_base b;
+	union {
+		struct ptp_management_tlv normal[1];
+		struct ptp_management_error_tlv error[1];
+	} tlv;
+} __packed;
+
+struct ptp_msg_announce {
+	struct ptp_timestamp originTimestamp;
+	s16 currentUtcOffset;
+	u8 reserved;
+	u8 grandmasterPriority1;
+	struct ptp_clock_quality grandmasterClockQuality;
+	u8 grandmasterPriority2;
+	struct ptp_clock_identity grandmasterIdentity;
+	u16 stepsRemoved;
+	u8 timeSource;
+} __packed;
+
+union ptp_msg_data {
+	struct ptp_msg_sync sync;
+	struct ptp_msg_follow_up follow_up;
+	struct ptp_msg_delay_resp delay_resp;
+	struct ptp_msg_pdelay_req pdelay_req;
+	struct ptp_msg_pdelay_resp pdelay_resp;
+	struct ptp_msg_pdelay_resp_follow_up pdelay_resp_follow_up;
+	struct ptp_msg_signaling signaling;
+	struct ptp_msg_management management;
+	struct ptp_msg_announce announce;
+	u8 data[8];
+} __packed;
+
+struct ptp_msg {
+	struct ptp_msg_hdr hdr;
+	union ptp_msg_data data;
+} __packed;
+
+
+struct ptp_id {
+	u16 seq;
+	struct ptp_clock_identity clock;
+	u8 mac[2];
+	u8 msg;
+	u8 port;
+};
+
+struct ptp_cfg_options {
+	u8 master:1;
+	u8 two_step:1;
+	u8 p2p:1;
+	u8 as:1;
+	u8 domain_check:1;
+	u8 udp_csum:1;
+	u8 unicast:1;
+	u8 alternate:1;
+	u8 delay_assoc:1;
+	u8 pdelay_assoc:1;
+	u8 sync_assoc:1;
+	u8 drop_sync:1;
+	u8 priority:1;
+	u8 reserved:3;
+	u8 master_set:1;
+	u8 two_step_set:1;
+	u8 p2p_set:1;
+	u8 as_set:1;
+	u8 domain_check_set:1;
+	u8 udp_csum_set:1;
+	u8 unicast_set:1;
+	u8 alternate_set:1;
+	u8 delay_assoc_set:1;
+	u8 pdelay_assoc_set:1;
+	u8 sync_assoc_set:1;
+	u8 drop_sync_set:1;
+	u8 priority_set:1;
+	u8 reserved_set:2;
+	u8 domain_set:1;
+	u8 domain;
+	u8 reserved3;
+	u32 access_delay;
+} __packed;
+
+#define PTP_CMD_RESP			0x01
+#define PTP_CMD_GET_MSG			0x00
+#define PTP_CMD_GET_OUTPUT		0xE0
+#define PTP_CMD_GET_EVENT		0xF0
+
+#define PTP_CMD_INTR_OPER		0x01
+#define PTP_CMD_SILENT_OPER		0x02
+#define PTP_CMD_ON_TIME			0x04
+#define PTP_CMD_REL_TIME		0x08
+#define PTP_CMD_CLK_OPT			0x10
+#define PTP_CMD_CASCADE_RESET_OPER	0x40
+#define PTP_CMD_CANCEL_OPER		0x80
+
+struct ptp_tsi_info {
+	u8 cmd;
+	u8 unit;
+	u8 event;
+	u8 num;
+	u32 edge;
+	struct ptp_utime t[0];
+} __packed;
+
+struct ptp_tsi_options {
+	u8 tsi;
+	u8 gpi;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[3];
+	u32 timeout;
+} __packed;
+
+struct ptp_tso_options {
+	u8 tso;
+	u8 gpo;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[1];
+	u16 cnt;
+	u32 pulse;
+	u32 cycle;
+	u32 sec;
+	u32 nsec;
+	u32 iterate;
+} __packed;
+
+struct ptp_clk_options {
+	u32 sec;
+	u32 nsec;
+	int drift;
+	u32 interval;
+} __packed;
+
+struct ptp_ts_options {
+	u32 timestamp;
+	u32 sec;
+	u32 nsec;
+	u8 msg;
+	u8 port;
+	u16 seqid;
+	u8 mac[2];
+} __packed;
+
+struct ptp_delay_values {
+	u16 rx_latency;
+	u16 tx_latency;
+	short asym_delay;
+	u16 reserved;
+} __packed;
+
+struct ptp_msg_options {
+	struct ptp_port_identity id;
+	u16 seqid;
+	u8 domain;
+	u8 msg;
+	u8 reserved[2];
+	u32 port;
+	struct ptp_ts ts;
+} __packed;
+
+struct ptp_udp_msg {
+	u16 len;
+	u8 data[0];
+} __packed;
+
+#ifdef __KERNEL__
+#define NANOSEC_IN_SEC			1000000000
+
+/* Host port can be any one of the ports. */
+#define MAX_PTP_PORT			(SWITCH_PORT_NUM + 1)
+
+#define MAX_TSM_UDP_LEN			100
+#define MAX_TSM_UDP_CNT			(1 << 6)
+
+struct ptp_ltime {
+	s64 sec;
+	s64 nsec;
+};
+
+struct ptp_hw_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int sim_2step;
+	int update;
+	int sending;
+};
+
+struct ptp_dev_info {
+	void *ptp;
+	unsigned int minor;
+	u8 *write_buf;
+	u8 *read_buf;
+	size_t read_max;
+	size_t read_len;
+	size_t write_len;
+	struct semaphore sem;
+	struct mutex lock;
+	wait_queue_head_t wait_udp;
+	struct ptp_dev_info *next;
+};
+
+struct ptp_tx_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int missed;
+	unsigned long req_time;
+	unsigned long resp_time;
+	struct {
+		u8 buf[MAX_TSM_UDP_LEN];
+		int len;
+	} data;
+	struct ptp_dev_info *dev;
+	struct sk_buff *skb;
+	struct ptp_msg *msg;
+	struct ptp_msg_hdr hdr;
+};
+
+struct ptp_event {
+	int max;
+	int num;
+	int event;
+	int first;
+	int last;
+	u32 edge;
+	struct ptp_utime t[MAX_TIMESTAMP_EVENT_UNIT];
+	u32 timeout;
+	unsigned long expired;
+};
+
+struct ptp_output {
+	struct ptp_utime trig;
+	struct ptp_utime start;
+	struct ptp_utime stop;
+	struct ksz_ptp_time gap;
+	u32 iterate;
+	u32 len;
+	int gpo;
+	int level;
+};
+
+#define CLOCK_ENTRIES		2
+
+struct ptp_irig_info {
+	u32 pulse[100];
+	int index;
+	u8 tso[8];
+	int cur_tso;
+	int max_tso;
+	struct ptp_utime t;
+};
+
+struct ptp_msg_info {
+	struct ptp_msg_options data;
+	u32 sec;
+	struct ptp_msg_info *next;
+};
+
+struct ptp_info;
+
+struct ptp_work {
+	struct work_struct work;
+	struct completion done;
+	struct ptp_info *ptp;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	union {
+		struct ptp_cfg_options cfg;
+		struct ptp_tsi_info tsi;
+		struct ptp_tsi_options tsi_opt;
+		struct ptp_tso_options tso_opt;
+		struct ptp_clk_options clk_opt;
+		struct ptp_ts_options ts_opt;
+		struct ptp_delay_values delay;
+		u8 data[8];
+	} param;
+	struct ptp_dev_info *dev_info;
+};
+
+#define PTP_WORK_NUM			(1 << 4)
+#define PTP_WORK_LAST			(PTP_WORK_NUM - 1)
+
+struct ptp_access {
+	int index;
+	struct ptp_work works[PTP_WORK_NUM];
+};
+
+struct ptp_reg_ops {
+	void (*lock)(struct ptp_info *ptp);
+	void (*unlock)(struct ptp_info *ptp);
+
+	void (*get_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*set_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*adjust_time)(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+		int adj_hack);
+	void (*adjust_sync_time)(struct ptp_info *ptp, int diff, u32 interval,
+		u32 duration);
+
+	void (*rx_off)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_reset)(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr);
+	void (*rx_restart)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_event)(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+		int intr);
+	void (*rx_cascade_event)(struct ptp_info *ptp, u8 first, u8 total,
+		u8 gpi, u8 event, int intr);
+	void (*read_event)(struct ptp_info *ptp, u8 tsi);
+
+	void (*tx_off)(struct ptp_info *ptp, u8 tso);
+	void (*tx_event)(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+		u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+		int intr, int now, int opt);
+	void (*pps_event)(struct ptp_info *ptp, u8 gpo, u32 sec);
+	void (*ptp_10MHz)(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec);
+	int (*tx_cascade)(struct ptp_info *ptp, u8 first, u8 total,
+		u16 repeat, u32 sec, u32 nsec, int intr);
+
+	void (*start)(struct ptp_info *ptp, int init);
+};
+
+struct ptp_ops {
+	void (*acquire)(struct ptp_info *ptp);
+	void (*release)(struct ptp_info *ptp);
+
+	void (*init)(struct ptp_info *ptp, u8 *mac_addr);
+	void (*exit)(struct ptp_info *ptp);
+	int (*stop)(struct ptp_info *ptp);
+	void (*set_identity)(struct ptp_info *ptp, u8 *addr);
+	struct ptp_msg *(*check_msg)(u8 *data, u16 **udp_check_ptr);
+	int (*update_msg)(u8 *data, u32 port, u32 overrides);
+	void (*get_rx_tstamp)(void *ptr, struct sk_buff *skb);
+	void (*get_tx_tstamp)(struct ptp_info *ptp, struct sk_buff *skb);
+	int (*hwtstamp_ioctl)(struct ptp_info *ptp, struct ifreq *ifr);
+	int (*ixxat_ioctl)(struct ptp_info *ptp, unsigned int cmd,
+		struct ifreq *ifr);
+	int (*dev_req)(struct ptp_info *ptp, char *arg,
+		struct ptp_dev_info *info);
+	void (*proc_intr)(struct ptp_info *ptp);
+
+	ssize_t (*sysfs_read)(struct ptp_info *ptp, int proc_num, ssize_t len,
+		char *buf);
+	void (*sysfs_write)(struct ptp_info *ptp, int proc_num, int num,
+		const char *buf);
+
+	int (*drop_pkt)(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+		int *tag, int *ptp_tag);
+
+	void (*get_rx_info)(struct ptp_info *ptp, u8 *data, u8 port,
+		u32 timestamp);
+	void (*set_tx_info)(struct ptp_info *ptp, u8 *data, void *tag);
+};
+
+#define DEFAULT_GPS_GPI			1
+#define DEFAULT_GPS_TSI			1
+
+#define DEFAULT_PPS_TSI			1
+
+#if 1
+#define DEFAULT_MHZ_GPO			1
+#define DEFAULT_PPS_GPO			0
+#else
+#define DEFAULT_MHZ_GPO			0
+#define DEFAULT_PPS_GPO			1
+#endif
+
+/* TSO 1 is reserved if 10 MHz clock is used. */
+#define DEFAULT_MHZ_TSO			0
+#define DEFAULT_PPS_TSO			2
+
+/* Switch features and bug fixes. */
+#define PTP_ADJ_HACK			(1 << 0)
+#define PTP_ADJ_SEC			(1 << 1)
+#define PTP_PDELAY_HACK			(1 << 2)
+
+/* Software overrides. */
+
+#define PTP_PORT_FORWARD		(1 << 0)
+#define PTP_PORT_TX_FORWARD		(1 << 1)
+
+#define PTP_VERIFY_TIMESTAMP		(1 << 8)
+#define PTP_ZERO_RESERVED_FIELD		(1 << 9)
+#define PTP_CHECK_SYS_TIME		(1 << 16)
+#define PTP_CHECK_SYNC_TIME		(1 << 24)
+#define PTP_TEST_TX_INFO		(1 << 28)
+#define PTP_USE_DEFAULT_PORT		(1 << 29)
+#define PTP_KEEP_DST_PORT		(1 << 30)
+#define PTP_UPDATE_DST_PORT		(1 << 31)
+
+struct ptp_info {
+	struct mutex lock;
+	struct ptp_access hw_access;
+	struct {
+		u8 buf[MAX_TSM_UDP_LEN];
+		int len;
+	} udp[MAX_TSM_UDP_CNT];
+
+	/* current system time. */
+	struct ptp_utime cur_time;
+	struct ptp_utime gps_time;
+	struct ksz_ptp_time time_diff;
+	u32 sec_hi;
+	u32 sec_lo;
+	struct delayed_work check_pps;
+	struct delayed_work update_sec;
+	unsigned long update_sec_jiffies;
+
+	u32 adjust;
+	int drift;
+	int drift_set;
+
+	int adjust_offset;
+	int offset_changed;
+	s64 adjust_sec;
+	s64 sec_changed;
+
+	struct ptp_utime time_set;
+
+	u32 adj_delay;
+	u32 get_delay;
+	u32 set_delay;
+	int pps_offset;
+	struct ptp_dev_info *gps_dev;
+	unsigned long gps_req_time;
+	unsigned long gps_resp_time;
+	u8 gps_gpi;
+	u8 gps_tsi;
+	u16 gps_seqid;
+	u8 pps_tsi;
+	u8 pps_tso;
+	u8 pps_gpo;
+	u8 mhz_tso;
+	u8 mhz_gpo;
+	u8 version;
+	u8 ports;
+	u8 started;
+
+	/* hardware register values. */
+	u16 rx_latency[MAX_PTP_PORT][3];
+	u16 tx_latency[MAX_PTP_PORT][3];
+	short asym_delay[MAX_PTP_PORT][3];
+	u32 peer_delay[MAX_PTP_PORT];
+
+	spinlock_t rx_msg_lock;
+	spinlock_t tx_msg_lock;
+	struct ptp_msg_info rx_msg_info[MANAGEMENT_MSG + 1];
+	struct ptp_msg_info tx_msg_info[MANAGEMENT_MSG + 1];
+#if 0
+	struct ptp_msg_options rx_msg_chk[MANAGEMENT_MSG + 1];
+#endif
+	struct ptp_msg *rx_msg;
+	struct ptp_msg *tx_msg;
+	int tx_msg_cnt;
+	int tx_msg_parsed;
+	u32 tx_ports;
+	int cap;
+	int op_mode;
+	int op_state;
+
+	/* used to remember tx timestamp to differentiate between pdelay_req
+	 * and pdelay_resp.
+	 */
+	u32 xdelay_ts[MAX_PTP_PORT];
+	u32 pdresp_ts[MAX_PTP_PORT];
+
+	/* tx timestamp */
+	struct ptp_hw_ts hw_sync[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_dreq[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_resp[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_sync[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_dreq[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_resp[MAX_PTP_PORT];
+	int linked[MAX_PTP_PORT];
+
+	int state;
+	u16 def_mode;
+	u16 def_cfg;
+	u16 mode;
+	u16 cfg;
+	u16 domain;
+	u16 vid;
+	int ptp_synt;
+	u16 trig_intr;
+	u16 ts_intr;
+	u16 tx_intr;
+
+	int tsi_intr;
+	int tsi_used;
+	int tsi_sys;
+	int tso_intr;
+	int tso_used;
+	int tso_sys;
+	int ts_status;
+	int cascade;
+	int cascade_rx;
+	int cascade_tx;
+	struct {
+		int first;
+		int total;
+		int tso;
+	} cascade_gpo[MAX_GPIO];
+	struct ptp_clock_identity clockIdentity;
+	struct ptp_clock_identity masterIdentity;
+	struct ptp_event events[MAX_TIMESTAMP_UNIT];
+	struct ptp_output outputs[MAX_TRIG_UNIT + 1];
+	int udp_head;
+	int udp_tail;
+	int dev_major;
+	struct ptp_dev_info *dev[2];
+	struct ptp_dev_info *tsi_dev[MAX_TIMESTAMP_UNIT];
+	struct ptp_dev_info *tso_dev[MAX_TRIG_UNIT];
+	char dev_name[2][20];
+	wait_queue_head_t wait_ts[MAX_PTP_PORT];
+	wait_queue_head_t wait_intr;
+	unsigned long delay_ticks;
+	int rx_en;
+	int tx_en;
+	int utc_offset;
+
+	u32 clk_divider;
+	u32 (*get_clk_cnt)(void);
+	u32 last_clk_cnt;
+	u64 total_clk_cnt;
+	u32 first_sec;
+	u32 intr_sec;
+	unsigned long last_jiffies;
+	u64 total_jiffies;
+	union ktime first_ktime;
+	int first_drift;
+
+	uint features;
+	uint overrides;
+
+	struct work_struct adj_clk;
+	struct work_struct set_latency;
+
+	const struct ptp_ops *ops;
+	const struct ptp_reg_ops *reg;
+	void (*test_access_time)(struct ptp_info *ptp);
+
+	struct workqueue_struct *access;
+
+	struct device *parent;
+#ifdef CONFIG_PTP_1588_CLOCK
+	void *clock_info;
+	u32 clock_events;
+#endif
+};
+
+struct ksz_ptp_sysfs {
+	struct ksz_dev_attr *ksz_clock_attrs[CLOCK_ENTRIES];
+	struct attribute **clock_attrs[CLOCK_ENTRIES];
+};
+#endif
+
+enum {
+	DEV_IOC_UNIT_UNAVAILABLE = DEV_IOC_LAST,
+	DEV_IOC_UNIT_USED,
+	DEV_IOC_UNIT_ERROR,
+};
+
+enum {
+	DEV_INFO_MSG = DEV_INFO_LAST,
+	DEV_INFO_RESET,
+};
+
+enum {
+	DEV_PTP_CFG,
+	DEV_PTP_TEVT,
+	DEV_PTP_TOUT,
+	DEV_PTP_CLK,
+	DEV_PTP_CASCADE,
+	DEV_PTP_DELAY,
+	DEV_PTP_REG,
+	DEV_PTP_IDENTITY,
+	DEV_PTP_PEER_DELAY,
+	DEV_PTP_UTC_OFFSET,
+	DEV_PTP_TIMESTAMP,
+	DEV_PTP_MSG,
+};
+
+#ifndef TSM_CMD_CLOCK_SET
+#define TSM_CMD_RESP			0x04
+#define TSM_CMD_GET_TIME_RESP		0x08
+
+#define TSM_CMD_CLOCK_SET		0x10
+#define TSM_CMD_CLOCK_CORRECT		0x20
+#define TSM_CMD_DB_SET			0x30
+#define TSM_CMD_DB_GET			0x40
+#define TSM_CMD_STAT_CLEAR		0x50
+#define TSM_CMD_STAT_GET		0x60
+#define TSM_CMD_CNF_SET			0x70
+#define TSM_CMD_CNF_GET			0x80
+#define TSM_CMD_GPIO_SET		0x90
+#define TSM_CMD_GPIO_GET		0xA0
+#define	TSM_CMD_SET_SECONDS		0xB0
+/* todo */
+#define TSM_CMD_GET_GPS_TS		0xE0
+
+/* used for accessing reserved DB entry for a given port for SYNC or DELAY_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV1		0xB0
+/* used for accessing reserved DB entry for a given port for P2P PATH_DEL_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV2		0xC0
+/* used for getting time from TSM, no look-up of a DB entry with an ingress or
+ * egress time stamp
+ */
+#define TSM_CMD_DB_GET_TIME		0xD0
+#define TSM_CMD_DB_SET_TIME		0xF0
+#endif
+
+struct tsm_cfg {
+	u8 cmd;
+	u8 port;
+	u8 enable;
+	u8 gmp;
+	u32 ingress_delay;
+	u16 egress_delay;
+} __packed;
+
+struct tsm_clock_set {
+	u8 cmd;
+	u32 timestamp;
+	u32 nsec;
+	u32 sec;
+	u8 reserved[5];
+} __packed;
+
+struct tsm_clock_correct {
+	u8 cmd;
+	u8 add;
+	u32 sec;
+	u32 nsec;
+	u32 drift;
+	u32 offset;
+} __packed;
+
+struct tsm_db {
+	u8 cmd;
+	u8 index;
+	u16 seqid;
+	u8 mac[2];
+	u32 cur_sec;
+	u32 cur_nsec;
+	u32 timestamp;
+} __packed;
+
+struct tsm_get_gps {
+	u8 cmd;
+	u8 reserved[7];
+	u16 seqid;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+struct tsm_get_time {
+	u8 cmd;
+	u16 seqid;
+	u8 msg;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+#define PTP_CAN_RX_TIMESTAMP		(1 << 0)
+#define PTP_KNOW_ABOUT_LATENCY		(1 << 1)
+#define PTP_HAVE_MULT_DEVICES		(1 << 2)
+#define PTP_HAVE_MULT_PORTS		(1 << 3)
+#define PTP_KNOW_ABOUT_MULT_PORTS	(1 << 4)
+#define PTP_USE_RESERVED_FIELDS		(1 << 5)
+
+#ifdef __KERNEL__
+struct ptp_attributes {
+	int features;
+	int overrides;
+	int vid;
+};
+#endif
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_ptp_iba.c b/drivers/net/ethernet/micrel/ksz_ptp_iba.c
new file mode 100644
index 0000000..42df79c
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_ptp_iba.c
@@ -0,0 +1,1359 @@
+/**
+ * Micrel PTP common code in IBA format
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static u32 ptp_unit_index(struct ptp_info *ptp, int shift, u8 unit)
+{
+	u32 index;
+#ifndef USE_OLD_PTP_UNIT_INDEX
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	index = sw->cached.ptp_unit_index;
+	index &= ~(PTP_UNIT_M << shift);
+	index |= (u32) unit << shift;
+	sw->cached.ptp_unit_index = index;
+#else
+	index = unit;
+#endif
+	return index;
+}  /* ptp_unit_index */
+
+static void *get_time_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = *data;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_CLK_CTRL);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_RTC_SEC);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_RTC_NANOSEC);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_RTC_SUB_NANOSEC__2);
+	return info->fptr;
+}  /* get_time_pre */
+
+static int get_time_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	struct ptp_utime *t = obj;
+	int i = 0;
+	u16 subnsec = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_PTP_RTC_SEC:
+				t->sec = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_RTC_NANOSEC:
+				t->nsec = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_RTC_SUB_NANOSEC__2:
+				subnsec = (u16) iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			}
+		}
+		i++;
+	}
+
+	add_nsec(t, subnsec * 8);
+	return i;
+}  /* get_time_post */
+
+static void get_ptp_time_iba(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+
+	data[0] = sw->cached.ptp_clk_ctrl;
+	data[0] |= PTP_READ_TIME;
+	iba_req(info, data, NULL, t, get_time_pre, get_time_post);
+}  /* get_ptp_time_iba */
+
+static void *set_time_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ptp_utime *t = obj;
+
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_RTC_SUB_NANOSEC__2);
+	info->data[0] = t->nsec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RTC_NANOSEC);
+	info->data[0] = t->sec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RTC_SEC);
+	info->data[0] = data[0];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_CLK_CTRL);
+	return info->fptr;
+}  /* set_time_pre */
+
+static void set_ptp_time_iba(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+
+	data[0] = sw->cached.ptp_clk_ctrl;
+	data[0] |= PTP_LOAD_TIME;
+	iba_req(info, data, NULL, t, set_time_pre, NULL);
+}  /* set_ptp_time_iba */
+
+static void *adjust_time_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u32 nsec = data[4];
+	u32 val = nsec;
+
+	info->data[0] = data[3];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RTC_SEC);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		info->data[0] = nsec;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_PTP_RTC_NANOSEC);
+		info->data[0] = data[2];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_PTP_CLK_CTRL);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (data[5]) {
+		info->data[0] = data[5];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_PTP_CLK_CTRL);
+	}
+	return info->fptr;
+}  /* adjust_time_pre */
+
+static void adjust_ptp_time_iba(struct ptp_info *ptp, int add, u32 sec,
+	u32 nsec, int adj_hack)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[6];
+	u16 ctrl;
+	u16 adj = 0;
+
+	data[2] = sw->cached.ptp_clk_ctrl;
+	ctrl = data[2];
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_ADJ;
+	data[2] = ctrl;
+	data[3] = sec;
+	data[4] = nsec;
+	data[5] = 0;
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		data[5] = adj;
+	iba_req(info, data, NULL, NULL, adjust_time_pre, NULL);
+}  /* adjust_ptp_time_iba */
+
+static void *adjust_sync_time_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = data[0];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RATE_DURATION);
+	info->data[0] = data[1];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_SUBNANOSEC_RATE);
+	return info->fptr;
+}
+
+static void adjust_sync_time_iba(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[2];
+	u32 adjust;
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE;
+	data[0] = duration;
+	data[1] = adjust;
+	iba_req(info, data, NULL, NULL, adjust_sync_time_pre, NULL);
+}  /* adjust_sync_time_iba */
+
+static void *ptp_unit_index_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int shift = data[0];
+	u8 unit = (u8) data[1];
+	struct ptp_info *ptp = obj;
+
+	info->data[0] = ptp_unit_index(ptp, shift, unit);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_UNIT_INDEX__4);
+	return info->fptr;
+}  /* ptp_unit_index_pre */
+
+static void *rx_reset_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_ENABLE | TS_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_reset_pre */
+
+static void ptp_rx_reset_iba(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	u32 buf[8];
+	void *func[3];
+	void *data_in[2];
+	u32 *data = buf;
+	int i = 0;
+
+	if (!ctrl_ptr) {
+		data_in[i] = data;
+		data[0] = PTP_TSI_INDEX_S;
+		data[1] = tsi;
+		data += 2;
+		func[i++] = ptp_unit_index_pre;
+	}
+
+	data_in[i] = data;
+	func[i++] = rx_reset_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_reset_iba */
+
+static void *rx_off_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_INT_ENABLE | TS_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_off_pre */
+
+static void ptp_rx_off_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u16 tsi_bit = (1 << tsi);
+	u32 ts_intr = 0;
+	int rc;
+	u32 buf[13];
+	void *func[6];
+	void *data_in[5];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		ts_intr = tsi_bit;
+	}
+
+	data_in[i] = data;
+	func[i++] = rx_off_pre;
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		data_in[i] = data;
+		func[i++] = rx_reset_pre;
+
+		data_in[i] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_TS_CTRL_STAT__4;
+		data[2] = 0;
+		data += 3;
+		func[i++] = iba_w_pre;
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+	if (ts_intr) {
+		data_in[i] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_PTP_INT_STATUS__4;
+		data[2] = ts_intr;
+		data += 3;
+		func[i++] = iba_w_pre;
+	}
+
+	data_out = data;
+	data = iba_prepare_data(REG_PTP_CTRL_STAT__4, data);
+	data = iba_prepare_data(-1, data);
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+}  /* ptp_rx_off_iba */
+
+static inline void rx_intr_iba(struct ptp_info *ptp, u16 tsi_bit, u32 *ctrl)
+{
+	ptp->ts_intr |= tsi_bit;
+	*ctrl |= TS_INT_ENABLE;
+}  /* rx_intr_iba */
+
+static void *rx_on_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tsi = (u8) data[0];
+	int intr = data[1];
+	struct ptp_info *ptp = obj;
+	u32 ctrl = 0;
+
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+
+	ctrl |= TS_ENABLE;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_on_pre */
+
+static void *rx_restart_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_restart_pre */
+
+static void ptp_rx_restart_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	func[i++] = rx_restart_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_restart_iba */
+
+static void *rx_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tsi = data[0];
+	u8 gpi = data[1];
+	u8 event = data[2];
+	struct ptp_info *ptp = obj;
+	u32 ctrl;
+
+	info->data[0] = ptp_unit_index(ptp, PTP_TSI_INDEX_S, tsi);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_UNIT_INDEX__4);
+
+	/* Config pattern. */
+	ctrl = ts_event_gpi(gpi, event);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TS_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_event_pre */
+
+static void ptp_rx_event_iba(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = tsi;
+	data[1] = gpi;
+	data[2] = event;
+	data += 3;
+	func[i++] = rx_event_pre;
+
+	data_in[i] = data;
+	data[0] = tsi;
+	data[1] = intr;
+	data += 2;
+	func[i++] = rx_on_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_event_iba */
+
+static void *rx_cascade_event_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u8 first = data[0];
+	u8 total = data[1];
+	u8 gpi = data[2];
+	u8 event = data[3];
+	int intr = data[4];
+	struct ptp_info *ptp = obj;
+	int last;
+	int tsi;
+	u32 ctrl;
+	u32 tail;
+	int i;
+	int prev;
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		info->data[0] = ptp_unit_index(ptp, PTP_TSI_INDEX_S, tsi);
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_PTP_UNIT_INDEX__4);
+
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = ts_event_gpi(gpi, event);
+		ctrl |= ts_cascade(prev);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TS_CTRL_STAT__4);
+
+		/* Enable timestamp interrupt. */
+		if (intr) {
+			ctrl = 0;
+			ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+			info->data[0] = 0;
+			info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+				IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+			info->data[0] = ctrl;
+			info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1,
+				IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		}
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	info->data[0] = ptp_unit_index(ptp, PTP_TSI_INDEX_S, first);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_UNIT_INDEX__4);
+
+	ctrl = ts_event_gpi(gpi, event);
+	ctrl |= ts_cascade(last);
+	ptp->cascade_rx |= (1 << first);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TS_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_cascade_event_pre */
+
+static void ptp_rx_cascade_event_iba(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = first;
+	data[1] = total;
+	data[2] = gpi;
+	data[3] = event;
+	data[4] = intr;
+	data += 5;
+	func[i++] = rx_cascade_event_pre;
+
+	data_in[i] = data;
+	data[0] = first;
+	data[1] = intr;
+	data += 2;
+	func[i++] = rx_on_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_cascade_event_iba */
+
+static u32 ptp_get_event_cnt_iba(struct ptp_info *ptp, u8 tsi, void *ptr)
+{
+	struct ksz_iba_info *info = ptr;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	data[0] = IBA_CMD_32;
+	data[1] = REG_TS_CTRL_STAT__4;
+	data += 3;
+	func[i++] = iba_r_pre;
+
+	data_out = data;
+	data = iba_prepare_data(REG_TS_CTRL_STAT__4, data);
+	data = iba_prepare_data(-1, data);
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+	return data_out[1];
+}  /* ptp_get_event_cnt_iba */
+
+static void ptp_get_events_iba(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr)
+{
+	struct ksz_iba_info *info = ptr;
+
+	iba_burst(info, reg_ns, len, buf, 0,
+		iba_get_pre, iba_get_post_le);
+}  /* ptp_get_events_iba */
+
+static void ptp_read_event_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+
+	ptp_read_event_func(ptp, tsi, info, ptp_get_event_cnt_iba,
+		ptp_get_events_iba);
+}  /* ptp_read_event_iba */
+
+static void *tx_reset_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_ENABLE | TRIG_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* tx_reset_pre */
+
+static void *tx_off_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* tx_off_pre */
+
+static void *tx_init_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	info->data[0] = TRIG_CASCADE_ENABLE | TRIG_CASCADE_TAIL;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	info->data[0] = trig_cascade(TRIG_CASCADE_UPS_M);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	return info->fptr;
+}  /* tx_init_pre */
+
+static void ptp_tx_off_iba(struct ptp_info *ptp, u8 tso)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u16 tso_bit = (1 << tso);
+	int rc;
+	u32 buf[8 + 8];
+	void *func[4];
+	void *data_in[3];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	func[i++] = tx_off_pre;
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		data_in[i] = data;
+		func[i++] = tx_reset_pre;
+
+		ptp->cascade_gpo[ptp->outputs[tso].gpo].tso &= ~tso_bit;
+		ptp->cascade_tx &= ~tso_bit;
+	} else {
+		data_in[i] = data;
+		func[i++] = tx_init_pre;
+	}
+
+	data_out = data;
+	data = iba_prepare_data(-1, data);
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+}  /* ptp_tx_off_iba */
+
+static void *tx_on_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* tx_on_pre */
+
+static void *tx_trigger_time_iba(struct ksz_iba_info *info, u8 tso, u32 sec,
+	u32 nsec)
+{
+	info->data[0] = sec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_TARGET_SEC);
+	info->data[0] = nsec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_TARGET_NANOSEC);
+	return info->fptr;
+}  /* tx_trigger_time_iba */
+
+static void *tx_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 gpo = data[4];
+	u8 event = data[5];
+	u32 pulse = data[6];
+	u32 cycle = data[7];
+	u16 cnt = data[8];
+	u32 sec = data[9];
+	u32 nsec = data[10];
+	u32 iterate = data[11];
+	int intr = data[12];
+	int now = data[13];
+	int opt = data[14];
+	struct ptp_info *ptp = obj;
+	u32 ctrl;
+	u32 pattern = 0;
+	u16 tso_bit = (1 << tso);
+	struct ptp_output *cur = &ptp->outputs[tso];
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	if (intr)
+		ctrl |= TRIG_NOTIFY;
+	if (now)
+		ctrl |= TRIG_NOW;
+	if (opt)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+
+	/* Config pulse width. */
+	if (TRIG_REG_OUTPUT == event) {
+		pattern = pulse & TRIG_BIT_PATTERN_M;
+		cur->level = 0;
+		if (cnt) {
+			u32 reg;
+
+			reg = cnt - 1;
+			reg %= 16;
+			while (reg) {
+				pulse >>= 1;
+				reg--;
+			}
+			if (pulse & 1)
+				cur->level = 1;
+		}
+		pulse = 0;
+	} else if (event >= TRIG_NEG_PULSE) {
+		if (0 == pulse)
+			pulse = 1;
+		else if (pulse > TRIG_PULSE_WIDTH_M)
+			pulse = TRIG_PULSE_WIDTH_M;
+		info->data[0] = pulse;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_24,
+			REG_TRIG_PULSE_WIDTH__4 + 1);
+	}
+
+	/* Config cycle width. */
+	if (event >= TRIG_NEG_PERIOD) {
+		u32 data = cnt;
+		int min_cycle = pulse * PULSE_NSEC + MIN_CYCLE_NSEC;
+
+		if (cycle < min_cycle)
+			cycle = min_cycle;
+		info->data[0] = cycle;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TRIG_CYCLE_WIDTH);
+
+		/* Config trigger count. */
+		data <<= TRIG_CYCLE_CNT_S;
+		pattern |= data;
+		info->data[0] = pattern;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TRIG_CYCLE_CNT);
+	}
+
+	cur->len = 0;
+	if (event >= TRIG_NEG_PERIOD) {
+		if (cnt)
+			cur->len += cycle * cnt;
+		else
+			cur->len += 0xF0000000;
+	} else if (event >= TRIG_NEG_PULSE)
+		cur->len += pulse * PULSE_NSEC;
+	else
+		cur->len += MIN_CYCLE_NSEC;
+
+	cur->start.sec = sec;
+	cur->start.nsec = nsec;
+	cur->iterate = iterate;
+	cur->trig = cur->start;
+	cur->stop = cur->start;
+	add_nsec(&cur->stop, cur->len);
+	cur->gpo = gpo;
+
+	switch (event) {
+	case TRIG_POS_EDGE:
+	case TRIG_NEG_PULSE:
+	case TRIG_NEG_PERIOD:
+		cur->level = 1;
+		break;
+	case TRIG_REG_OUTPUT:
+		break;
+	default:
+		cur->level = 0;
+		break;
+	}
+
+	if (ptp->cascade)
+		return info->fptr;
+
+	/*
+	 * Need to reset after completion.  Otherwise, this output pattern
+	 * does not behave consistently in cascade mode.
+	 */
+	if (TRIG_NEG_EDGE == event)
+		ptp->cascade_tx |= tso_bit;
+
+	ptp->cascade_gpo[gpo].total = 0;
+	if (cur->level)
+		ptp->cascade_gpo[gpo].tso |= tso_bit;
+	else
+		ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+
+	/* Config trigger time. */
+	tx_trigger_time_iba(info, tso, sec, nsec);
+
+	return info->fptr;
+}  /* tx_event_pre */
+
+static void ptp_tx_event_iba(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+	int intr, int now, int opt)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	struct ptp_output *cur = &ptp->outputs[tso];
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	/* Hardware immediately keeps level high on new GPIO if not reset. */
+	if (cur->level && gpo != cur->gpo) {
+		data_in[i] = data;
+		func[i++] = tx_reset_pre;
+
+		ptp->cascade_gpo[cur->gpo].tso &= ~(1 << tso);
+	}
+
+	data_in[i] = data;
+	data[3] = tso;
+	data[4] = gpo;
+	data[5] = event;
+	data[6] = pulse;
+	data[7] = cycle;
+	data[8] = cnt;
+	data[9] = sec;
+	data[10] = nsec;
+	data[11] = iterate;
+	data[12] = intr;
+	data[13] = now;
+	data[14] = opt;
+	data += 15;
+	func[i++] = tx_event_pre;
+
+	if (!ptp->cascade) {
+		data_in[i] = data;
+		func[i++] = tx_on_pre;
+	}
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_tx_event_iba */
+
+static void *pps_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 gpo = data[3];
+	u32 sec = data[4];
+	struct ptp_info *ptp = obj;
+	u32 pattern;
+	u32 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 tso = ptp->pps_tso;
+	u8 event = TRIG_POS_PERIOD;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	info->data[0] = pulse;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_24,
+		REG_TRIG_PULSE_WIDTH__4 + 1);
+
+	/* Config cycle width. */
+	info->data[0] = cycle;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_WIDTH);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	info->data[0] = pattern;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_CNT);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	tx_trigger_time_iba(info, tso, sec, nsec);
+
+	return info->fptr;
+}  /* pps_event_pre */
+
+static void ptp_pps_event_iba(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u8 tso = ptp->pps_tso;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	int i = 0;
+
+	ptp_tx_off_iba(ptp, tso);
+
+	data_in[i] = data;
+	data[3] = gpo;
+	data[4] = sec;
+	data += 5;
+	func[i++] = pps_event_pre;
+
+	data_in[i] = data;
+	func[i++] = tx_on_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_pps_event */
+
+static void *ptp_10MHz_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 gpo = data[4];
+	u32 sec = data[5];
+	u32 nsec = data[6];
+	u32 pattern;
+	u32 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	if (1 == tso)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	info->data[0] = pulse;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_PULSE_WIDTH__4 + 0);
+
+	/* Config cycle width. */
+	info->data[0] = cycle;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_WIDTH);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	info->data[0] = pattern;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_CNT);
+
+	tx_trigger_time_iba(info, tso, sec, nsec);
+
+	return info->fptr;
+}  /* ptp_10MHz_pre */
+
+static void ptp_10MHz_iba(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int n;
+	u32 nsec;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	u32 *data_out;
+	int k = 0;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+
+	for (n = 0; n < 2; n++) {
+		ptp_tx_off_iba(ptp, tso);
+
+		k = 0;
+		data = buf;
+
+		data_in[k] = data;
+		data[3] = tso;
+		data[4] = gpo;
+		data[5] = sec;
+		data[6] = nsec;
+		data += 7;
+		func[k++] = ptp_10MHz_pre;
+
+		data_in[k] = data;
+		func[k++] = tx_on_pre;
+
+		data_out = data;
+		data = iba_prepare_data(REG_PTP_CTRL_STAT__4, data);
+		data = iba_prepare_data(-1, data);
+
+		func[k] = NULL;
+		assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+		rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz_iba */
+
+static void *tx_cascade_cycle_iba(struct ksz_iba_info *info, u8 tso, u32 nsec)
+{
+	info->data[0] = nsec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_ITERATE_TIME);
+	return info->fptr;
+}  /* tx_cascade_cycle_iba */
+
+static void *tx_cascade_on_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 first = data[4];
+	u8 last = data[5];
+	u16 repeat = data[6];
+	struct ptp_output *cur = obj;
+	u32 ctrl;
+
+	tx_trigger_time_iba(info, tso, cur->trig.sec, cur->trig.nsec);
+	tx_cascade_cycle_iba(info, tso, cur->iterate);
+
+	ctrl = data[2];
+	ctrl |= TRIG_CASCADE_ENABLE;
+	ctrl &= ~trig_cascade(TRIG_CASCADE_UPS_M);
+	if (tso == first)
+		ctrl |= trig_cascade(last);
+	else
+		ctrl |= trig_cascade(tso - 1);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		ctrl |= repeat - 1;
+	}
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	return info->fptr;
+}  /* tx_cascade_on_pre */
+
+static int ptp_tx_cascade_iba(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int n;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur = NULL;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	u32 *data_out;
+	int k = 0;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	if (check_cascade(ptp, first, total, &repeat, sec, nsec)) {
+		dbg_msg("cascade repeat timing is not right\n");
+		return 1;
+	}
+	tso = last;
+	for (n = 0; n < total; n++, tso--) {
+		cur = &ptp->outputs[tso];
+	data = buf;
+	k = 0;
+	data_in[k] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[k++] = ptp_unit_index_pre;
+
+	data_in[k] = data;
+	data[0] = IBA_CMD_32;
+	data[1] = REG_TRIG_CTRL__4;
+	data += 3;
+	func[k++] = iba_r_pre;
+
+	data_out = data;
+	data = iba_prepare_data(REG_TRIG_CTRL__4, data);
+	data = iba_prepare_data(-1, data);
+
+		func[k] = NULL;
+		assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+		rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+
+		data = buf;
+		k = 0;
+
+		data_in[k] = data;
+		data[2] = data_out[1];
+		data[3] = tso;
+		data[4] = first;
+		data[5] = last;
+		data[6] = repeat;
+		data += 7;
+		func[k++] = tx_cascade_on_pre;
+
+		func[k] = NULL;
+		assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+		if (tso != first)
+			rc = iba_reqs(info, data_in, NULL, cur, func, NULL);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	/* Do not reset last unit to keep level high. */
+	if (ptp->outputs[last].level) {
+		ptp->cascade_tx &= ~(1 << last);
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso |= (1 << last);
+	} else
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso &= ~(1 << last);
+
+	data_in[k] = data;
+	func[k++] = tx_on_pre;
+
+	func[k] = NULL;
+	assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, cur, func, NULL);
+	return 0;
+}  /* ptp_tx_cascade_iba */
+
+static void *start_pre_1(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_MSG_CONF1);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_MSG_CONF2);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_DOMAIN_VERSION);
+	return info->fptr;
+}  /* start_pre_1 */
+
+static int start_post_1(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_PTP_MSG_CONF1:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_MSG_CONF2:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_DOMAIN_VERSION:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* start_post_1 */
+
+static void *start_pre_2(struct ksz_iba_info *info, void *in, void *obj)
+{
+	struct ptp_info *ptp = obj;
+
+	info->data[0] = ptp->mode;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_MSG_CONF1);
+	info->data[0] = ptp->cfg;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_MSG_CONF2);
+	info->data[0] = 0xffffffff;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_INT_STATUS__4);
+	return info->fptr;
+}  /* start_pre_2 */
+
+static void ptp_start_iba(struct ptp_info *ptp, int init)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+	u16 ctrl;
+	struct timespec ts;
+	struct ptp_utime t;
+
+	ptp->ops->acquire(ptp);
+	iba_req(info, data, data, NULL, start_pre_1, start_post_1);
+	ctrl = data[0];
+	if (ctrl == ptp->mode) {
+		ptp->cfg = data[1];
+		ptp->domain = data[2] & PTP_DOMAIN_M;
+		if (!init) {
+			ptp->ops->release(ptp);
+			return;
+		}
+	} else if (!init)
+		ptp->mode = ctrl;
+	if (ptp->mode != ptp->def_mode) {
+		dbg_msg("mode changed: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->ptp_synt = false;
+	}
+	dbg_msg("ptp_start: %04x %04x\n",
+		ptp->mode, ptp->cfg);
+	iba_req(info, data, NULL, ptp, start_pre_2, NULL);
+	ptp_tx_intr_enable(ptp);
+	ptp->ops->release(ptp);
+#if 0
+test_iba_access = 0;
+	if (ptp->test_access_time)
+		ptp->test_access_time(ptp);
+test_iba_access = 0;
+#endif
+
+	ts = ktime_to_timespec(ktime_get_real());
+	t.sec = ts.tv_sec;
+	t.nsec = ts.tv_nsec;
+	ptp->ops->acquire(ptp);
+	set_ptp_time_iba(ptp, &t);
+	ptp->cur_time = t;
+	ptp->ops->release(ptp);
+
+	prepare_pps(ptp);
+	ptp->started = true;
+}  /* ptp_start_iba */
+
+
+struct ptp_reg_ops ptp_iba_ops = {
+	.get_time		= get_ptp_time_iba,
+	.set_time		= set_ptp_time_iba,
+	.adjust_time		= adjust_ptp_time_iba,
+	.adjust_sync_time	= adjust_sync_time_iba,
+
+	.rx_off			= ptp_rx_off_iba,
+	.rx_reset		= ptp_rx_reset_iba,
+	.rx_restart		= ptp_rx_restart_iba,
+	.rx_event		= ptp_rx_event_iba,
+	.rx_cascade_event	= ptp_rx_cascade_event_iba,
+	.read_event		= ptp_read_event_iba,
+
+	.tx_off			= ptp_tx_off_iba,
+	.tx_event		= ptp_tx_event_iba,
+	.pps_event		= ptp_pps_event_iba,
+	.ptp_10MHz		= ptp_10MHz_iba,
+	.tx_cascade		= ptp_tx_cascade_iba,
+
+	.start			= ptp_start_iba,
+};
diff --git a/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c b/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c
new file mode 100644
index 0000000..8479cb0
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c
@@ -0,0 +1,126 @@
+/**
+ * Micrel PTP common sysfs code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2013 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static ssize_t ptp_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ptp_info *ptp;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	ptp = &sw->ptp_hw;
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = ptp->ops->sysfs_read(ptp, proc_num, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t ptp_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ptp_info *ptp;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	ptp = &sw->ptp_hw;
+	proc_num = offset / sizeof(int);
+	ret = count;
+	ptp->ops->acquire(ptp);
+	ptp->ops->sysfs_write(ptp, proc_num, num, buf);
+	ptp->ops->release(ptp);
+	up(proc_sem);
+	return ret;
+}
+
+#define PTP_ATTR(_name, _mode, _show, _store) \
+struct device_attribute ptp_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define PTP_RD_ENTRY(name)						\
+static ssize_t show_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return ptp_show(d, attr, buf,					\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static PTP_ATTR(name, S_IRUGO, show_ptp_##name, NULL)
+
+/* generate a write-able attribute */
+#define PTP_WR_ENTRY(name)						\
+static ssize_t show_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return ptp_show(d, attr, buf,					\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static ssize_t store_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return ptp_store(d, attr, buf, count,				\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static PTP_ATTR(name, S_IRUGO | S_IWUSR, show_ptp_##name, store_ptp_##name)
+
+PTP_WR_ENTRY(features);
+PTP_WR_ENTRY(overrides);
+PTP_WR_ENTRY(vid);
+
+static struct attribute *ptp_attrs[] = {
+	&ptp_attr_features.attr,
+	&ptp_attr_overrides.attr,
+	&ptp_attr_vid.attr,
+	NULL
+};
+
+static struct attribute_group ptp_group = {
+	.name  = "ptpfs",
+	.attrs  = ptp_attrs,
+};
+
+static void exit_ptp_sysfs(struct ksz_ptp_sysfs *info, struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, &ptp_group);
+}
+
+static int init_ptp_sysfs(struct ksz_ptp_sysfs *info, struct device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->kobj, &ptp_group);
+	if (err)
+		return err;
+	return err;
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz_req.c b/drivers/net/ethernet/micrel/ksz_req.c
new file mode 100644
index 0000000..e992f4b
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_req.c
@@ -0,0 +1,46 @@
+/**
+ * Micrel driver request common code
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_req.h"
+
+
+#define PARAM_DATA_SIZE			80
+
+static int chk_ioctl_size(int len, int size, int additional, int *req_size,
+	int *result, void *param, u8 *data)
+{
+	if (len < size) {
+		printk(KERN_INFO "wrong size: %d %d\n", len, size);
+		*req_size = size + additional;
+		*result = DEV_IOC_INVALID_LEN;
+		return -1;
+	}
+	if (size >= PARAM_DATA_SIZE) {
+		printk(KERN_INFO "large size: %d\n", size);
+		*result = -EFAULT;
+		return -1;
+	}
+	if (data && (!access_ok(VERIFY_READ, param, size) ||
+			copy_from_user(data, param, size))) {
+		*result = -EFAULT;
+		return -1;
+	}
+	return 0;
+}  /* chk_ioctl_size */
+
diff --git a/drivers/net/ethernet/micrel/ksz_req.h b/drivers/net/ethernet/micrel/ksz_req.h
new file mode 100644
index 0000000..6e32dfe
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_req.h
@@ -0,0 +1,85 @@
+/**
+ * Micrel driver request common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_REQ_H
+#define KSZ_REQ_H
+
+enum {
+	DEV_IOC_OK,
+	DEV_IOC_INVALID_SIZE,
+	DEV_IOC_INVALID_CMD,
+	DEV_IOC_INVALID_LEN,
+
+	DEV_IOC_LAST
+};
+
+enum {
+	DEV_CMD_INFO,
+	DEV_CMD_GET,
+	DEV_CMD_PUT,
+
+	DEV_CMD_LAST
+};
+
+enum {
+	DEV_INFO_INIT,
+	DEV_INFO_EXIT,
+	DEV_INFO_QUIT,
+
+	DEV_INFO_LAST
+};
+
+struct ksz_request {
+	int size;
+	int cmd;
+	int subcmd;
+	int output;
+	int result;
+	union {
+		u8 data[1];
+		int num[1];
+	} param;
+};
+
+/* Some compilers in different OS cannot have zero number in array. */
+#define SIZEOF_ksz_request	(sizeof(struct ksz_request) - sizeof(int))
+
+/* Not used in the driver. */
+
+#ifndef MAX_REQUEST_SIZE
+#define MAX_REQUEST_SIZE	20
+#endif
+
+struct ksz_request_actual {
+	int size;
+	int cmd;
+	int subcmd;
+	int output;
+	int result;
+	union {
+		u8 data[MAX_REQUEST_SIZE];
+		int num[MAX_REQUEST_SIZE / sizeof(int)];
+	} param;
+};
+
+#define DEV_IOC_MAGIC			0x92
+
+#define DEV_IOC_MAX			1
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_spi_net.h b/drivers/net/ethernet/micrel/ksz_spi_net.h
new file mode 100644
index 0000000..87642e9
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_spi_net.h
@@ -0,0 +1,221 @@
+/**
+ * Micrel SPI switch common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SPI_NET_H
+#define KSZ_SPI_NET_H
+
+
+#if defined(_LINUX_I2C_H)
+/**
+ * struct i2c_hw_priv - I2C device private data structure
+ * @i2cdev:		Adapter device information.
+ * @rxd:		Buffer for receiving I2C data.
+ * @txd:		Buffer for transmitting I2C data.
+ */
+struct i2c_hw_priv {
+	struct i2c_client *i2cdev;
+
+	u8 rxd[8];
+	u8 txd[32];
+};
+#endif
+
+#if defined(__LINUX_SPI_H)
+/**
+ * struct spi_hw_priv - SPI device private data structure
+ * @spidev:		Adapter device information.
+ * @spi_msg1:		Used for SPI transfer with one message.
+ * @spi_msg2:		Used for SPI transfer with two messages.
+ * @spi_xfer1:		Used for SPI transfer with one message.
+ * @spi_xfer2:		Used for SPI transfer with two messages.
+ * @rx_1msg:		Flag to receive SPI data with single message.
+ * @rxd:		Buffer for receiving SPI data.
+ * @txd:		Buffer for transmitting SPI data.
+ */
+struct spi_hw_priv {
+	struct spi_device *spidev;
+	struct spi_message spi_msg1;
+	struct spi_message spi_msg2;
+	struct spi_transfer spi_xfer1;
+	struct spi_transfer spi_xfer2[2];
+	int rx_1msg;
+
+	u8 rxd[128];
+	u8 txd[128];
+};
+#endif
+
+/**
+ * struct sw_priv - Switch device private data structure
+ * @hw_dev:		Pointer to hardware access device structure.
+ * @dev:		Pointer to Linux base device of hardware device.
+ * @intr_mode:		Indicate which interrupt mode to use.
+ * @irq:		A copy of the hardware device interrupt.
+ * @sysfs:		Sysfs structure.
+ * @proc_sem:		Semaphore for sysfs accessing.
+ * @hwlock:
+ * @lock:
+ * @link_read:		Work queue for detecting link.
+ * @mib_read:		Work queue for reading MIB counters.
+ * @stp_monitor:	Work queue for STP monitoring.
+ * @mib_timer_info:	Timer information for reading MIB counters.
+ * @monitor_timer_info:	Timer information for monitoring.
+ * @counter:		MIB counter data.
+ * @debug_root:
+ * @debug_file:
+ * @phy_id:
+ * @intr_working:
+ * @intr_mask:
+ * @pdev:
+ * @bus:
+ * @bus_irqs:
+ * @name:
+ * @phydev:
+ * @phypriv:
+ * @sw:			Virtual switch structure.
+ */
+struct sw_priv {
+	void *hw_dev;
+	struct device *dev;
+	int intr_mode;
+	int irq;
+
+	struct ksz_sw_sysfs sysfs;
+#ifdef CONFIG_1588_PTP
+	struct ksz_ptp_sysfs ptp_sysfs;
+#endif
+	struct semaphore proc_sem;
+
+	struct mutex hwlock;
+	struct mutex lock;
+
+	struct delayed_work link_read;
+	struct work_struct mib_read;
+	struct delayed_work stp_monitor;
+	struct ksz_timer_info mib_timer_info;
+	struct ksz_timer_info monitor_timer_info;
+	struct ksz_counter_info counter[TOTAL_PORT_NUM];
+
+	struct dentry *debug_root;
+	struct dentry *debug_file;
+
+	int phy_id;
+	int intr_working;
+	uint intr_mask;
+
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	int bus_irqs[PHY_MAX_ADDR];
+	char name[40];
+	struct phy_device *phydev;
+	struct phy_priv *phypriv;
+
+	/* Switch structure size can be variable. */
+	struct ksz_sw sw;
+};
+
+/**
+ * struct dev_priv - Network device private data structure
+ * @adapter:		Adapter device information.
+ * @dev:
+ * @parent:
+ * @port:
+ * @monitor_timer_info:	Timer information for monitoring.
+ * @stats:		Network statistics.
+ * @phydev:		The PHY device associated with the device.
+ * @phy_pause:		Workqueue to pause the PHY state machine.
+ * @id:			Device ID.
+ * @mii_if:		MII interface information.
+ * @advertising:	Temporary variable to store advertised settings.
+ * @msg_enable:		The message flags controlling driver output.
+ * @media_state:	The connection status of the device.
+ * @multicast:		The all multicast state of the device.
+ * @promiscuous:	The promiscuous state of the device.
+ */
+struct dev_priv {
+	void *adapter;
+	struct net_device *dev;
+	void *parent;
+	struct ksz_port port;
+	struct ksz_timer_info monitor_timer_info;
+	struct net_device_stats stats;
+
+	struct phy_device *phydev;
+	struct work_struct phy_pause;
+
+	int id;
+
+	struct mii_if_info mii_if;
+	u32 advertising;
+
+	u32 msg_enable;
+	int media_state;
+	int multicast;
+	int promiscuous;
+	u8 phy_addr;
+	u8 state;
+	u8 multi_list_size;
+
+#ifdef MAX_MULTICAST_LIST
+	u8 multi_list[MAX_MULTICAST_LIST][ETH_ALEN];
+#endif
+};
+
+#ifndef SKIP_MICREL_SWITCH_SYSFS
+static void get_private_data(struct device *d, struct semaphore **proc_sem,
+	struct ksz_sw **sw, struct ksz_port **port)
+{
+	struct net_device *dev;
+	struct dev_priv *priv;
+	struct sw_priv *hw_priv;
+
+	if (d->bus && (
+#if defined(__LINUX_SPI_H)
+	    d->bus == &spi_bus_type
+#endif
+#if defined(__LINUX_SPI_H) && defined(_LINUX_I2C_H)
+	    ||
+#endif
+#if defined(_LINUX_I2C_H)
+	    d->bus == &i2c_bus_type
+#endif
+#if !defined(__LINUX_SPI_H) && !defined(_LINUX_I2C_H)
+	    d->bus
+#endif
+	    )) {
+		hw_priv = dev_get_drvdata(d);
+		if (port && hw_priv->phydev) {
+			struct phy_priv *phydata;
+
+			phydata = hw_priv->phydev->priv;
+			*port = &phydata->port;
+		}
+	} else {
+		dev = to_net_dev(d);
+		priv = netdev_priv(dev);
+		hw_priv = priv->parent;
+		if (port)
+			*port = &priv->port;
+	}
+	*proc_sem = &hw_priv->proc_sem;
+	*sw = &hw_priv->sw;
+}
+#endif
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz_sw_9897.c b/drivers/net/ethernet/micrel/ksz_sw_9897.c
new file mode 100644
index 0000000..931033e
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_sw_9897.c
@@ -0,0 +1,12052 @@
+/**
+ * Micrel gigabit switch common code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+/* (288 - 24 - 4) / (8 * 3) */
+#define MAX_IBA_MIB_ENTRIES		10
+
+#define READ_MIB_ENTRY_SIZE		2
+
+/* (288 - 24 - 4) / (8 * 6) */
+#define MAX_IBA_MAC_ENTRIES		5
+
+/* (288 - 24 - 4) / (8 * 6) */
+#define MAX_IBA_VLAN_ENTRIES		5
+
+#define READ_VLAN_ENTRY_SIZE		3
+#define WRITE_VLAN_ENTRY_SIZE		4
+
+enum {
+	PROC_SW_INFO,
+
+	PROC_SET_SW_DUPLEX,
+	PROC_SET_SW_SPEED,
+	PROC_SET_SW_FORCE,
+	PROC_SET_SW_FLOW_CTRL,
+
+	PROC_SET_SW_FEATURES,
+	PROC_SET_SW_OVERRIDES,
+	PROC_SET_SW_MIB,
+
+	PROC_SET_SW_REG,
+	PROC_SET_SW_VID,
+
+	PROC_DYNAMIC,
+	PROC_STATIC,
+	PROC_VLAN,
+
+	PROC_SET_AGING,
+	PROC_SET_FAST_AGING,
+	PROC_SET_LINK_AGING,
+
+	PROC_SET_BROADCAST_STORM,
+	PROC_SET_MULTICAST_STORM,
+	PROC_SET_DIFFSERV,
+	PROC_SET_802_1P,
+
+	PROC_ENABLE_VLAN,
+	PROC_SET_REPLACE_NULL_VID,
+	PROC_SET_DROP_INVALID_VID,
+	PROC_SET_MAC_ADDR,
+	PROC_SET_MIRROR_MODE,
+
+	PROC_SET_IGMP_SNOOP,
+	PROC_SET_IPV6_MLD_SNOOP,
+	PROC_SET_IPV6_MLD_OPTION,
+
+	PROC_SET_AGGR_BACKOFF,
+	PROC_SET_NO_EXC_DROP,
+
+	PROC_SET_JUMBO_PACKET,
+	PROC_SET_LEGAL_PACKET,
+	PROC_SET_LENGTH_CHECK,
+
+	PROC_SET_BACK_PRESSURE_MODE,
+	PROC_SET_SWITCH_FLOW_CTRL,
+	PROC_SET_SWITCH_HALF_DUPLEX,
+	PROC_SET_SWITCH_10_MBIT,
+
+	PROC_SET_FAIR_FLOW_CTRL,
+	PROC_SET_VLAN_BOUNDARY,
+	PROC_SET_DOUBLE_TAG,
+	PROC_SET_ISP_TAG,
+	PROC_SET_HSR_TAG,
+	PROC_SET_MTU,
+	PROC_SET_FORWARD_UNKNOWN_UNICAST,
+	PROC_SET_UNKNOWN_UNICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST,
+	PROC_SET_UNKNOWN_MULTICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_VID,
+	PROC_SET_UNKNOWN_VID_PORTS,
+
+	PROC_SET_PASS_PAUSE,
+	PROC_ENABLE_PME,
+	PROC_ENABLE_PME_POLARITY,
+
+	PROC_GET_HOST_PORT,
+	PROC_GET_PORTS,
+	PROC_GET_DEV_START,
+	PROC_GET_VLAN_START,
+	PROC_GET_STP,
+
+	PROC_SET_STATIC_FID,
+	PROC_SET_STATIC_USE_FID,
+	PROC_SET_STATIC_OVERRIDE,
+	PROC_SET_STATIC_VALID,
+	PROC_SET_STATIC_MSTP,
+	PROC_SET_STATIC_PRIO,
+	PROC_SET_STATIC_SRC,
+	PROC_SET_STATIC_DST,
+	PROC_SET_STATIC_PORTS,
+	PROC_SET_STATIC_MAC_ADDR,
+	PROC_SET_STATIC_TYPE,
+	PROC_SET_STATIC_INDEX,
+	PROC_SET_STATIC_INFO,
+
+	PROC_SET_VLAN_VALID,
+	PROC_SET_VLAN_PORTS,
+	PROC_SET_VLAN_UNTAG,
+	PROC_SET_VLAN_FID,
+	PROC_SET_VLAN_MSTP,
+	PROC_SET_VLAN_PRIO,
+	PROC_SET_VLAN_OPTION,
+	PROC_SET_VLAN_VID,
+	PROC_SET_VLAN_INFO,
+
+	PROC_SET_NO_COLOR,
+	PROC_SET_COLOR_RED,
+	PROC_SET_COLOR_YELLOW,
+	PROC_SET_COLOR_GREEN,
+
+};
+
+enum {
+	PROC_SET_PORT_MIB,
+
+	PROC_SET_DEF_VID,
+	PROC_SET_MEMBER,
+
+	PROC_ENABLE_BROADCAST_STORM,
+	PROC_ENABLE_DIFFSERV,
+	PROC_ENABLE_802_1P,
+	PROC_ENABLE_VLAN_PRIO,
+	PROC_ENABLE_MAC_PRIO,
+	PROC_ENABLE_ACL_PRIO,
+	PROC_SET_HIGHEST_PRIO,
+	PROC_SET_OR_PRIO,
+
+	PROC_SET_PORT_BASED,
+
+	PROC_SET_DIS_NON_VID,
+	PROC_SET_INGRESS,
+	PROC_SET_DROP_NON_VLAN,
+	PROC_SET_DROP_TAG,
+	PROC_SET_REPLACE_VID,
+	PROC_SET_REPLACE_PRIO,
+	PROC_SET_SRC_ADDR_FILTER,
+	PROC_SET_VLAN_LOOKUP_0,
+
+	PROC_SET_MSTP,
+	PROC_SET_RX,
+	PROC_SET_TX,
+	PROC_SET_LEARN,
+
+	PROC_ENABLE_PRIO_QUEUE,
+
+	PROC_ENABLE_PRIO_RATE,
+	PROC_SET_LIMIT,
+	PROC_SET_LIMIT_PORT_BASED,
+	PROC_SET_LIMIT_PACKET_BASED,
+	PROC_SET_LIMIT_FLOW_CTRL,
+	PROC_SET_LIMIT_CNT_IFG,
+	PROC_SET_LIMIT_CNT_PRE,
+	PROC_SET_RX_P0_RATE,
+	PROC_SET_RX_P1_RATE,
+	PROC_SET_RX_P2_RATE,
+	PROC_SET_RX_P3_RATE,
+	PROC_SET_RX_P4_RATE,
+	PROC_SET_RX_P5_RATE,
+	PROC_SET_RX_P6_RATE,
+	PROC_SET_RX_P7_RATE,
+	PROC_SET_TX_Q0_RATE,
+	PROC_SET_TX_Q1_RATE,
+	PROC_SET_TX_Q2_RATE,
+	PROC_SET_TX_Q3_RATE,
+
+	PROC_SET_COLOR_MAP,
+	PROC_SET_TC_MAP,
+
+	PROC_SET_MIRROR_PORT,
+	PROC_SET_MIRROR_RX,
+	PROC_SET_MIRROR_TX,
+
+	PROC_SET_BACK_PRESSURE,
+	PROC_SET_FORCE_FLOW_CTRL,
+	PROC_SET_PASS_ALL,
+	PROC_SET_TAIL_TAG,
+
+	PROC_SET_CUSTOM_VID,
+	PROC_SET_SR_1_VID,
+	PROC_SET_SR_2_VID,
+	PROC_SET_SR_1_TYPE,
+	PROC_SET_SR_2_TYPE,
+	PROC_SET_PME_CTRL,
+	PROC_SET_PME_STATUS,
+
+	PROC_SET_AUTHEN_MODE,
+	PROC_SET_ACL,
+	PROC_SET_ACL_FIRST_RULE,
+	PROC_SET_ACL_RULESET,
+	PROC_SET_ACL_MODE,
+	PROC_SET_ACL_ENABLE,
+	PROC_SET_ACL_SRC,
+	PROC_SET_ACL_EQUAL,
+	PROC_SET_ACL_MAC_ADDR,
+	PROC_SET_ACL_TYPE,
+	PROC_SET_ACL_CNT,
+	PROC_SET_ACL_MSEC,
+	PROC_SET_ACL_INTR_MODE,
+	PROC_SET_ACL_IP_ADDR,
+	PROC_SET_ACL_IP_MASK,
+	PROC_SET_ACL_PROTOCOL,
+	PROC_SET_ACL_SEQNUM,
+	PROC_SET_ACL_PORT_MODE,
+	PROC_SET_ACL_MAX_PORT,
+	PROC_SET_ACL_MIN_PORT,
+	PROC_SET_ACL_TCP_FLAG_ENABLE,
+	PROC_SET_ACL_TCP_FLAG,
+	PROC_SET_ACL_TCP_FLAG_MASK,
+	PROC_SET_ACL_PRIO_MODE,
+	PROC_SET_ACL_PRIO,
+	PROC_SET_ACL_VLAN_PRIO_REPLACE,
+	PROC_SET_ACL_VLAN_PRIO,
+	PROC_SET_ACL_MAP_MODE,
+	PROC_SET_ACL_PORTS,
+	PROC_SET_ACL_INDEX,
+	PROC_SET_ACL_ACTION_INDEX,
+	PROC_SET_ACL_ACTION,
+	PROC_SET_ACL_INFO,
+	PROC_GET_ACL_TABLE,
+
+	PROC_SET_P_INDEX,
+	PROC_SET_Q_INDEX,
+
+	PROC_SET_POLICE_PACKET_TYPE,
+	PROC_SET_NON_DSCP_COLOR,
+	PROC_ENABLE_POLICE_DROP_ALL,
+	PROC_ENABLE_PORT_BASED_POLICING,
+	PROC_ENABLE_COLOR_MARK,
+	PROC_ENABLE_COLOR_REMAP,
+	PROC_ENABLE_DROP_SRP,
+	PROC_ENABLE_COLOR_AWARE,
+	PROC_ENABLE_POLICE,
+
+	PROC_SET_Q_CIR,
+	PROC_SET_Q_PIR,
+	PROC_SET_Q_CBS,
+	PROC_SET_Q_PBS,
+
+	PROC_SET_WRED_MAX_THRESHOLD,
+	PROC_SET_WRED_MIN_THRESHOLD,
+	PROC_SET_WRED_MULTIPLIER,
+	PROC_GET_WRED_AVG_SIZE,
+
+	PROC_SET_WRED_Q_MAX_THRESHOLD,
+	PROC_SET_WRED_Q_MIN_THRESHOLD,
+	PROC_SET_WRED_Q_MULTIPLIER,
+	PROC_GET_WRED_Q_AVG_SIZE,
+
+	PROC_SET_WRED_RANDOM_DROP,
+	PROC_SET_WRED_DROP_GYR,
+	PROC_SET_WRED_DROP_YR,
+	PROC_SET_WRED_DROP_R,
+	PROC_SET_WRED_DROP_ALL,
+	PROC_GET_WRED_PMON,
+
+	PROC_SET_SCHEDULE,
+	PROC_SET_SHAPING,
+#ifdef MTI_PREEMPT_ENABLE
+	PROC_SET_PREEMPT,
+#endif
+	PROC_SET_TX_RATIO,
+	PROC_SET_CREDIT_HI_WATER_MARK,
+	PROC_SET_CREDIT_LO_WATER_MARK,
+	PROC_SET_CREDIT_INCREMENT,
+	PROC_SET_SRP,
+
+	PROC_SET_QM_DROP,
+	PROC_SET_QM_BURST_SIZE,
+	PROC_SET_QM_RESV_SPACE,
+	PROC_SET_QM_HI_WATER_MARK,
+	PROC_SET_QM_LO_WATER_MARK,
+	PROC_GET_QM_TX_USED,
+	PROC_GET_QM_TX_AVAIL,
+	PROC_GET_QM_TX_CALCULATED,
+
+	PROC_SET_MMD_ID,
+	PROC_SET_MMD_REG,
+	PROC_SET_MMD_VAL,
+
+	PROC_GET_RX_FLOW_CTRL,
+	PROC_GET_TX_FLOW_CTRL,
+
+	PROC_GET_PORT_DUPLEX,
+	PROC_GET_PORT_SPEED,
+	PROC_SET_LINK_MD,
+
+};
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_acquire(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->lock);
+	sw->reg->lock(sw);
+	mutex_unlock(&sw->lock);
+}  /* sw_acquire */
+
+static void sw_release(struct ksz_sw *sw)
+{
+	sw->reg->unlock(sw);
+}  /* sw_release */
+
+/* -------------------------------------------------------------------------- */
+
+/* Common routines used by both SPI and IBA accesses. */
+
+/**
+ * get_mac_table_info - Get MAC table information
+ * @mac:	Buffer to store the MAC table entry.
+ * @data:	Buffer holding MAC table information.
+ *
+ * This helper routine retrieves information from MAC table entry data.
+ */
+static void get_mac_table_info(struct ksz_mac_table *mac, u32 data[])
+{
+	mac->valid = !!(data[0] & ALU_V_STATIC_VALID);
+	mac->src = !!(data[0] & ALU_V_SRC_FILTER);
+	mac->dst = !!(data[0] & ALU_V_DST_FILTER);
+	mac->prio = (data[0] >> ALU_V_PRIO_AGE_CNT_S) &
+		ALU_V_PRIO_AGE_CNT_M;
+	mac->mstp = data[0] & ALU_V_MSTP_M;
+	mac->override = !!(data[1] & ALU_V_OVERRIDE);
+	if (mac->ignore_use_fid)
+		mac->use_fid = 1;
+	else
+		mac->use_fid = !!(data[1] & ALU_V_USE_FID);
+	mac->ports = data[1] & ALU_V_PORT_MAP;
+	mac->fid = data[2] >> ALU_V_FID_S;
+	mac->addr[1] = (u8) data[2];
+	mac->addr[0] = (u8)(data[2] >> 8);
+	mac->addr[5] = (u8) data[3];
+	mac->addr[4] = (u8)(data[3] >> 8);
+	mac->addr[3] = (u8)(data[3] >> 16);
+	mac->addr[2] = (u8)(data[3] >> 24);
+}  /* get_mac_table_info */
+
+/**
+ * set_mac_table_info - Set MAC table information
+ * @mac:	The MAC table entry.
+ * @data:	Buffer to hold MAC table information.
+ *
+ * This helper routine puts information to MAC table entry.
+ */
+static void set_mac_table_info(struct ksz_mac_table *mac, u32 data[])
+{
+	data[0] = (u32)(mac->prio & ALU_V_PRIO_AGE_CNT_M) <<
+		ALU_V_PRIO_AGE_CNT_S;
+	data[0] |= mac->mstp & ALU_V_MSTP_M;
+	if (mac->src)
+		data[0] |= ALU_V_SRC_FILTER;
+	if (mac->dst)
+		data[0] |= ALU_V_DST_FILTER;
+	if (mac->valid)
+		data[0] |= ALU_V_STATIC_VALID;
+	data[1] = mac->ports & ALU_V_PORT_MAP;
+	if (mac->override)
+		data[1] |= ALU_V_OVERRIDE;
+	if (!mac->ignore_use_fid && mac->use_fid)
+		data[1] |= ALU_V_USE_FID;
+	data[2] = (u32) mac->fid << ALU_V_FID_S;
+	data[2] |= ((u32) mac->addr[0] << 8) | mac->addr[1];
+	data[3] = ((u32) mac->addr[2] << 24) |
+		((u32) mac->addr[3] << 16) |
+		((u32) mac->addr[4] << 8) | mac->addr[5];
+}  /* set_mac_table_info */
+
+/**
+ * wait_for_dyn_mac_table - Wait for dynamic MAC table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper routines waits for dynamic MAC table to be ready for access.
+ */
+static void wait_for_dyn_mac_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	do {
+#if 0
+		sw->ops->acquire(sw);
+#endif
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+#if 0
+		sw->ops->release(sw);
+#endif
+	} while ((ctrl & ALU_START) || ALU_SEARCH == (ctrl & ALU_ACTION));
+}  /* wait_for_dyn_mac_table */
+
+/**
+ * wait_for_sta_mac_table - Wait for static MAC table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper routines waits for static MAC table to be ready for access.
+ */
+static void wait_for_sta_mac_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	do {
+#if 0
+		sw->ops->acquire(sw);
+#endif
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+#if 0
+		sw->ops->release(sw);
+#endif
+	} while (ctrl & ALU_STAT_START);
+}  /* wait_for_sta_mac_table */
+
+/**
+ * get_vlan_table_info - Get VLAN table information
+ * @vlan:	Buffer to store the VLAN table entry.
+ * @data:	Buffer holding VLAN table information.
+ *
+ * This helper routine retrieves information from VLAN table entry data.
+ */
+static void get_vlan_table_info(struct ksz_vlan_table *vlan, u32 data[])
+{
+	vlan->valid = !!(data[0] & VLAN_VALID);
+	vlan->fid = (data[0] & VLAN_FID_M);
+	vlan->mstp = (data[0] >> VLAN_MSTP_S) & VLAN_MSTP_M;
+	vlan->prio = (data[0] >> VLAN_PRIO_S) & VLAN_PRIO_M;
+	vlan->option = !!(data[0] & VLAN_FORWARD_OPTION);
+	vlan->untag = data[1];
+	vlan->ports = data[2];
+}  /* get_vlan_table_info */
+
+/**
+ * set_vlan_table_info - set VLAN table information
+ * @vlan:	The VLAN table entry.
+ * @data:	Buffer to hold VLAN table information.
+ *
+ * This helper routine puts information to VLAN table entry.
+ */
+static void set_vlan_table_info(struct ksz_vlan_table *vlan, u32 data[])
+{
+	data[0] = vlan->fid & VLAN_FID_M;
+	data[0] |= (u32)(vlan->mstp & VLAN_MSTP_M) << VLAN_MSTP_S;
+	data[0] |= (u32)(vlan->prio & VLAN_PRIO_M) << VLAN_PRIO_S;
+	if (vlan->option)
+		data[0] |= VLAN_FORWARD_OPTION;
+	if (vlan->valid)
+		data[0] |= VLAN_VALID;
+	data[1] = vlan->untag;
+	data[2] = vlan->ports;
+}  /* set_vlan_table_info */
+
+/**
+ * wait_for_vlan_table - Wait for VLAN table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper routines waits for VLAN table to be ready for access.
+ */
+static void wait_for_vlan_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	do {
+#if 0
+		sw->ops->acquire(sw);
+#endif
+		ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+#if 0
+		sw->ops->release(sw);
+#endif
+	} while (ctrl & VLAN_START);
+}  /* wait_for_vlan_table */
+
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_9897.c"
+#endif
+#ifdef KSZ_IBA
+static void sw_set_spi(struct ksz_sw *sw, struct ksz_iba_info *iba);
+
+#include "ksz_iba.c"
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Switch functions */
+
+/**
+ * sw_r_mac_table - read from MAC table
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This helper function reads an entry of the MAC table of the switch.
+ */
+static void sw_r_mac_table(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	sw_r(sw, REG_SW_ALU_VAL_A, data, 4 * 4);
+	data[0] = be32_to_cpu(data[0]);
+	data[1] = be32_to_cpu(data[1]);
+	data[2] = be32_to_cpu(data[2]);
+	data[3] = be32_to_cpu(data[3]);
+	get_mac_table_info(mac, data);
+}  /* sw_r_mac_table */
+
+/**
+ * sw_w_mac_table - write to MAC table
+ * @sw:		The switch instance.
+ * @mac:	The MAC table entry.
+ *
+ * This helper function writes an entry of the MAC table of the switch.
+ */
+static void sw_w_mac_table(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	set_mac_table_info(mac, data);
+	data[0] = cpu_to_be32(data[0]);
+	data[1] = cpu_to_be32(data[1]);
+	data[2] = cpu_to_be32(data[2]);
+	data[3] = cpu_to_be32(data[3]);
+	sw_w(sw, REG_SW_ALU_VAL_A, data, 4 * 4);
+}  /* sw_w_mac_table */
+
+/**
+ * sw_s_dyn_mac_table - prepare dynmaic MAC table for access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ *
+ * This helper function prepares dynmaic MAC table for access.
+ */
+static u32 sw_s_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & ALU_DIRECT_INDEX_M;
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_1, data);
+		ctrl |= ALU_DIRECT;
+	} else {
+		data = (u32) src_fid << ALU_FID_INDEX_S;
+		data |= ((u32) src_addr[0] << 8) | src_addr[1];
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_0, data);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_1, data);
+	}
+	return ctrl;
+}  /* sw_s_dyn_mac_table */
+
+/**
+ * sw_r_dyn_mac_hw - read from dynamic MAC table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This routine reads an entry of the dynamic MAC table using default access.
+ */
+static void sw_r_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_dyn_mac_table(sw, addr, src_addr, src_fid);
+	ctrl |= ALU_READ;
+	ctrl |= ALU_START;
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	do {
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	} while (ctrl & ALU_START);
+if (!mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+#if 0
+	mac->ignore_use_fid = 1;
+#endif
+	sw_r_mac_table(sw, mac);
+
+	/* Hash read. */
+	if (!addr && entry)
+		*entry = (sw->reg->r16(sw, REG_SW_LUE_INDEX_0__2) &
+			ENTRY_INDEX_M) + 1;
+}  /* sw_r_dyn_mac_hw */
+
+/**
+ * sw_r_dyn_mac_table - read from dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This routine reads an entry of the dynamic MAC table of the switch.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_r_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	if (entry)
+		*entry = 0;
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_dyn_mac_table(sw);
+	mac->ignore_use_fid = 1;
+	sw->reg->r_dyn_mac_hw(sw, addr, src_addr, src_fid, mac, entry);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_r_dyn_mac_table */
+
+/**
+ * sw_w_dyn_mac_hw - write to dynamic MAC table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the dynamic MAC table using default access.
+ */
+static void sw_w_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_dyn_mac_table(sw, addr, src_addr, src_fid);
+	ctrl |= ALU_WRITE;
+	ctrl |= ALU_START;
+#if 0
+	mac->ignore_use_fid = 1;
+#endif
+if (!mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+	sw_w_mac_table(sw, mac);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+}  /* sw_w_dyn_mac_hw */
+
+/**
+ * sw_w_dyn_mac_table - write to dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the dynamic MAC table of the switch.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_w_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_dyn_mac_table(sw);
+	mac->ignore_use_fid = 1;
+	sw->reg->w_dyn_mac_hw(sw, addr, src_addr, src_fid, mac);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_w_dyn_mac_table */
+
+/**
+ * sw_start_dyn_mac_hw - start dynamic MAC table search using default access
+ * @sw:		The switch instance.
+ *
+ * This routine starts dynamic MAC table search using default access.
+ */
+static void sw_start_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = ALU_SEARCH;
+	ctrl |= ALU_START;
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+}  /* sw_start_dyn_mac_hw */
+
+/**
+ * sw_start_dyn_mac_table - start dynamic MAC table search
+ * @sw:		The switch instance.
+ *
+ * This routine starts dynamic MAC table search.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_start_dyn_mac_table(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_dyn_mac_table(sw);
+	sw->reg->start_dyn_mac_hw(sw);
+	sw->ops->release(sw);
+}  /* sw_start_dyn_mac_table */
+
+/**
+ * sw_g_dyn_mac_hw - retrieve dynamic MAC table result using default access
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This routine retrieves dynamic MAC table result using default access.
+ */
+static void sw_g_dyn_mac_hw(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+#if 0
+	mac->ignore_use_fid = 1;
+#endif
+if (!mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+	sw_r_mac_table(sw, mac);
+}  /* sw_g_dyn_mac_hw */
+
+/**
+ * sw_g_dyn_mac_table - retrieve dynamic MAC table result
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves an entry of the dynamic MAC table search result.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_g_dyn_mac_table(struct ksz_sw *sw, u16 *addr,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+	int rc = 0;
+int timeout = 10;
+u32 data = 0;
+
+	sw->ops->acquire(sw);
+	do {
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+if (--timeout <= 0) {
+printk(" %s to\n", __func__);
+break;
+}
+/**
+ * THa  2015/04/21
+ * The long delay before the chip select is dropped causes the chip to think
+ * the register is written twice, so the valid bit will not be set when read.
+ */
+#if 1
+	if (!*addr && 1 == (ctrl >> ALU_VALID_CNT_S) && !(ctrl & ALU_VALID))
+		ctrl |= ALU_VALID;
+#endif
+if ((ctrl & ALU_START) && !(ctrl & ALU_VALID))
+data = ctrl;
+	} while (!(ctrl & ALU_VALID) && (ctrl & ALU_START));
+	if (ctrl & ALU_VALID) {
+		ctrl >>= ALU_VALID_CNT_S;
+		ctrl &= ALU_VALID_CNT_M;
+		mac->ignore_use_fid = 1;
+		sw->reg->g_dyn_mac_hw(sw, mac);
+if (ctrl != *addr + 1)
+printk(" ?get: %x %x\n", ctrl, *addr);
+		if (ctrl != *addr + 1)
+			*addr = ctrl - 1;
+	} else
+		rc = -1;
+if (timeout < 9)
+printk(" %s to: %d %x\n", __func__, timeout, data);
+	sw->ops->release(sw);
+	return rc;
+}  /* sw_g_dyn_mac_table */
+
+/**
+ * sw_stop_dyn_mac_hw - stop dynamic MAC table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function stops dynamic MAC table search using default access.
+ *
+ * Return the last MAC table control.
+ */
+static u32 sw_stop_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	return ctrl;
+}  /* sw_stop_dyn_mac_hw */
+
+/**
+ * sw_stop_dyn_mac_table - stop dynamic MAC table search
+ * @sw:		The switch instance.
+ * @addr:	The address of the last table entry.
+ *
+ * This routine stops dynamic MAC table search.
+ */
+static void sw_stop_dyn_mac_table(struct ksz_sw *sw, u16 addr)
+{
+	u32 ctrl;
+
+	sw->ops->acquire(sw);
+	ctrl = sw->reg->stop_dyn_mac_hw(sw);
+	ctrl >>= ALU_VALID_CNT_S;
+	ctrl &= ALU_VALID_CNT_M;
+if (ctrl != addr)
+dbg_msg(" ?stop: %x %x\n", ctrl, addr);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_stop_dyn_mac_table */
+
+/**
+ * sw_d_dyn_mac_table - dump dynamic MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps dynamic MAC table contents.
+ */
+static void sw_d_dyn_mac_table(struct ksz_sw *sw)
+{
+	u16 i;
+	struct ksz_mac_table mac;
+
+	sw_start_dyn_mac_table(sw);
+	i = 0;
+	do {
+		if (sw_g_dyn_mac_table(sw, &i, &mac))
+			break;
+		printk(KERN_INFO
+			"%4x: %02X:%02X:%02X:%02X:%02X:%02X "
+			"%04x m:%u t:%u s:%u d:%u o:%u %u:%02x [%u]\n",
+			i, mac.addr[0], mac.addr[1], mac.addr[2],
+			mac.addr[3], mac.addr[4], mac.addr[5],
+			mac.ports, mac.mstp, mac.prio, mac.src, mac.dst,
+			mac.override, mac.use_fid, mac.fid, mac.valid);
+		yield();
+		i++;
+	} while (1);
+	sw_stop_dyn_mac_table(sw, i);
+}  /* sw_d_dyn_mac_table */
+
+static u8 mcast_reserved_map[RESERVED_MCAST_TABLE_ENTRIES] = {
+	0, 1, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	4, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+};
+
+static u32 get_mac_table_ctrl(u16 addr, int mcast)
+{
+	u32 ctrl;
+
+	ctrl = addr;
+	if (mcast)
+		ctrl &= ALU_RESV_MCAST_INDEX_M;
+	else
+		ctrl &= ALU_STAT_INDEX_M;
+	ctrl <<= ALU_STAT_INDEX_S;
+	if (mcast)
+		ctrl |= ALU_RESV_MCAST_ADDR;
+	ctrl |= ALU_STAT_START;
+	return ctrl;
+}  /* get_mac_table_ctrl */
+
+/**
+ * sw_r_sta_mac_hw - read from static MAC table using default access
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for read operation.
+ * @num:	Number of entries to read.
+ * @mac:	Buffer to hold the MAC table entries.
+ *
+ * This routine reads from static MAC table using default access.
+ */
+static void sw_r_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 status;
+	int cnt = 0;
+
+	do {
+		sw->reg->w32(sw, REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		do {
+			status = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+		} while (status & ALU_STAT_START);
+if (mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+#if 0
+		mac->ignore_use_fid = 0;
+#endif
+		sw_r_mac_table(sw, mac);
+		++cnt;
+		++ctrl;
+		++mac;
+	} while (cnt < num);
+}  /* sw_r_sta_mac_hw */
+
+/**
+ * sw_r_sta_mac_table - read from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This routine reads an entry of the static MAC table of the switch.
+ */
+static void sw_r_sta_mac_table(struct ksz_sw *sw, u16 addr, int mcast,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_sta_mac_table(sw);
+	ctrl = get_mac_table_ctrl(addr, mcast);
+	ctrl |= ALU_STAT_READ;
+	mac->ignore_use_fid = 0;
+	sw->reg->r_sta_mac_hw(sw, &ctrl, 1, mac);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	if (mcast) {
+		mac->addr[0] = 0x01;
+		mac->addr[1] = 0x80;
+		mac->addr[2] = 0xC2;
+		mac->addr[3] = 0x00;
+		mac->addr[4] = 0x00;
+		mac->addr[5] = addr;
+		mac->valid = 1;
+	}
+	mac->dirty = 0;
+}  /* sw_r_sta_mac_table */
+
+/**
+ * sw_r_m_sta_mac_table - read many from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The addresses of the table entries.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	Buffer to store the MAC table entries.
+ *
+ * This routines reads several entries of the static MAC table of the switch.
+ */
+static void sw_r_m_sta_mac_table(struct ksz_sw *sw, u16 addr[], int mcast,
+	int num, struct ksz_mac_table *mac)
+{
+	u32 ctrl[MAX_IBA_MAC_ENTRIES];
+	u16 max_addr;
+	int i;
+	int j;
+	int cnt = MAX_IBA_MAC_ENTRIES;
+
+	if (mcast)
+		max_addr = RESERVED_MCAST_TABLE_ENTRIES;
+	else
+		max_addr = STATIC_MAC_TABLE_ENTRIES;
+	memset(mac, 0, sizeof(struct ksz_mac_table) * num);
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_sta_mac_table(sw);
+	for (i = 0; i < num; i += MAX_IBA_MAC_ENTRIES) {
+#if 1
+		if (addr[i] >= max_addr)
+			break;
+#endif
+		if (cnt > max_addr - addr[i])
+			cnt = max_addr - addr[i];
+		if (cnt > num - i)
+			cnt = num - i;
+		for (j = 0; j < cnt; j++) {
+			ctrl[j] = get_mac_table_ctrl(addr[i + j], mcast);
+			ctrl[j] |= ALU_STAT_READ;
+			mac[j].ignore_use_fid = 0;
+		}
+		sw->reg->r_sta_mac_hw(sw, ctrl, cnt, mac);
+		for (j = 0; j < cnt; j++) {
+			if (mcast) {
+				mac->addr[0] = 0x01;
+				mac->addr[1] = 0x80;
+				mac->addr[2] = 0xC2;
+				mac->addr[3] = 0x00;
+				mac->addr[4] = 0x00;
+				mac->addr[5] = addr[i + j];
+				mac->valid = 1;
+			}
+			++mac;
+		}
+	}
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_r_m_sta_mac_table */
+
+/**
+ * sw_w_sta_mac_hw - write to static MAC table using default access
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for write operation.
+ * @num:	Number of entries to write.
+ * @mac:	MAC table entries.
+ *
+ * This routine writes to static MAC table using default access.
+ */
+static void sw_w_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 status;
+	int cnt;
+
+	for (cnt = 0; cnt < num; cnt++, ctrl++, mac++) {
+if (mac->ignore_use_fid)
+dbg_msg("  !!! %s\n", __func__);
+#if 0
+		mac->ignore_use_fid = 0;
+#endif
+		sw_w_mac_table(sw, mac);
+		sw->reg->w32(sw, REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		do {
+			status = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+		} while (status & ALU_STAT_START);
+	}
+}  /* sw_w_sta_mac_hw */
+
+/**
+ * sw_w_sta_mac_table - write to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the static MAC table of the switch.
+ */
+static void sw_w_sta_mac_table(struct ksz_sw *sw, u16 addr, int mcast,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_sta_mac_table(sw);
+	ctrl = get_mac_table_ctrl(addr, mcast);
+	mac->ignore_use_fid = 0;
+	sw->reg->w_sta_mac_hw(sw, &ctrl, 1, mac);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_w_sta_mac_table */
+
+/**
+ * sw_w_m_sta_mac_table - write many to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The addresses of the table entries.
+ * @mcast:	Multicast reserved table indication.
+ * num:		Numbe of entries.
+ * @mac:	The MAC table entries.
+ *
+ * This routine writes several entries of the static MAC table of the switch.
+ */
+static void sw_w_m_sta_mac_table(struct ksz_sw *sw, u16 addr[], int mcast,
+	int num, struct ksz_mac_table *mac)
+{
+	u32 ctrl[MAX_IBA_MAC_ENTRIES];
+	int i;
+	int j;
+	int cnt = MAX_IBA_MAC_ENTRIES;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	wait_for_sta_mac_table(sw);
+	for (i = 0; i < num; i += MAX_IBA_MAC_ENTRIES) {
+		if (cnt > num - i)
+			cnt = num - i;
+		for (j = 0; j < cnt; j++) {
+			ctrl[j] = get_mac_table_ctrl(addr[i + j], mcast);
+			mac[j].ignore_use_fid = 0;
+		}
+		sw->reg->w_sta_mac_hw(sw, ctrl, cnt, mac);
+		mac += MAX_IBA_MAC_ENTRIES;
+	}
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_w_m_sta_mac_table */
+
+/**
+ * sw_d_sta_mac_table - dump static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps static MAC table contents.
+ */
+static void sw_d_sta_mac_table(struct ksz_sw *sw)
+{
+	int i;
+	int j;
+	u16 addr[8];
+	struct ksz_mac_table *mac;
+	struct ksz_mac_table table[8];
+	int first_static = true;
+
+	i = 0;
+	do {
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++)
+			addr[j] = i + j;
+		sw_r_m_sta_mac_table(sw, addr, 1, MAX_IBA_MAC_ENTRIES,
+			table);
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++) {
+			mac = &table[j];
+			if (!mac->valid)
+				continue;
+			printk(KERN_INFO
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X "
+				"%04x m:%u p:%u s:%u d:%u o:%u %u:%02x <%u>\n",
+				i + j,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				mac->ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid,
+				mcast_reserved_map[i + j]);
+		}
+		yield();
+		i += MAX_IBA_MAC_ENTRIES;
+	} while (i < RESERVED_MCAST_TABLE_ENTRIES);
+	i = 0;
+	do {
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++)
+			addr[j] = i + j;
+		sw_r_m_sta_mac_table(sw, addr, 0, MAX_IBA_MAC_ENTRIES,
+			table);
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++) {
+			mac = &table[j];
+			if (!mac->valid)
+				continue;
+			if (first_static) {
+				first_static = false;
+				printk(KERN_INFO "\n");
+			}
+			printk(KERN_INFO
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X "
+				"%04x m:%u p:%u s:%u d:%u o:%u %u:%02x\n",
+				i + j,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				mac->ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid);
+		}
+		yield();
+		i += MAX_IBA_MAC_ENTRIES;
+	} while (i < STATIC_MAC_TABLE_ENTRIES);
+}  /* sw_d_sta_mac_table */
+
+static int get_mcast_reserved_addr(u8 group)
+{
+	int i;
+
+	for (i = 0; i < RESERVED_MCAST_TABLE_ENTRIES; i++)
+		if (group == mcast_reserved_map[i])
+			return i;
+	return -1;
+}  /* get_mcast_reserved_addr */
+
+static void sw_d_mac_table(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *mac;
+	struct ksz_alu_table *alu;
+	int i;
+	int first_static = true;
+
+	i = STATIC_MAC_TABLE_ENTRIES;
+	do {
+		mac = &sw->info->mac_table[i];
+		if (mac->valid) {
+			if (first_static) {
+				first_static = false;
+				printk(KERN_INFO "\n");
+			}
+			alu = &sw->info->alu_table[i];
+			printk(KERN_INFO
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X "
+				"%04x m:%u p:%u s:%u d:%u o:%u %u:%02x  "
+				"%02x:%02x\n",
+				i,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				mac->ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid,
+				alu->forward, alu->owner);
+		}
+		i++;
+		if (SWITCH_MAC_TABLE_ENTRIES == i)
+			first_static = true;
+	} while (i < MULTI_MAC_TABLE_ENTRIES);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_r_vlan_hw - read from VLAN table using default access
+ * @sw:		The switch instance.
+ * @data:	Buffer to hold the VLAN data.
+ * @num:	Number of entries to read.
+ *
+ * This routine reads from VLAN table using default access.
+ */
+static void sw_r_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	u8 ctrl;
+	int cnt = 0;
+	u16 addr = data[READ_VLAN_ENTRY_SIZE];
+
+	do {
+		sw->reg->w16(sw, REG_SW_VLAN_ENTRY_INDEX__2, addr);
+		ctrl = VLAN_READ;
+		ctrl |= VLAN_START;
+		sw->reg->w8(sw, REG_SW_VLAN_CTRL, ctrl);
+		do {
+			ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+		} while (ctrl & VLAN_START);
+		data[0] = sw->reg->r32(sw, REG_SW_VLAN_ENTRY__4);
+		data[1] = 0;
+		data[2] = 0;
+		if (data[0] & VLAN_VALID) {
+			sw_r(sw, REG_SW_VLAN_ENTRY_UNTAG__4, &data[1], 4 * 2);
+			data[1] = be32_to_cpu(data[1]);
+			data[2] = be32_to_cpu(data[2]);
+		}
+		++cnt;
+		++addr;
+		data += READ_VLAN_ENTRY_SIZE;
+	} while (cnt < num);
+}  /* sw_r_vlan_hw */
+
+/**
+ * sw_r_vlan_table - read from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vlan:	Buffer to store the VLAN table entry.
+ *
+ * This function reads an entry of the VLAN table of the switch.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_vlan_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[READ_VLAN_ENTRY_SIZE + 1];
+	int rc = -1;
+
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	wait_for_vlan_table(sw);
+	data[READ_VLAN_ENTRY_SIZE] = addr;
+	sw->reg->r_vlan_hw(sw, data, 1);
+	get_vlan_table_info(vlan, data);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+	if (vlan->valid)
+		rc = 0;
+	vlan->vid = addr;
+	vlan->dirty = 0;
+	return rc;
+}  /* sw_r_vlan_table */
+
+/**
+ * sw_r_m_vlan_table - read many from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The starting address of the table entries.
+ * @num:	Number of entries.
+ * @vlan:	Buffer to store the VLAN table entries.
+ *
+ * This routine reads several entries of the VLAN table of the switch.
+ */
+static void sw_r_m_vlan_table(struct ksz_sw *sw, u16 addr, int num,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[READ_VLAN_ENTRY_SIZE * MAX_IBA_VLAN_ENTRIES + 1];
+	int i;
+	int j;
+	int cnt = MAX_IBA_VLAN_ENTRIES;
+
+	memset(vlan, 0, sizeof(struct ksz_vlan_table) * num);
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	wait_for_vlan_table(sw);
+	for (i = 0; i < num; i += MAX_IBA_VLAN_ENTRIES,
+	     addr += MAX_IBA_VLAN_ENTRIES) {
+		if (addr >= 4096)
+			break;
+		if (cnt > 4096 - addr)
+			cnt = 4096 - addr;
+		if (cnt > num - i)
+			cnt = num - i;
+		data[READ_VLAN_ENTRY_SIZE] = addr;
+		sw->reg->r_vlan_hw(sw, data, cnt);
+		for (j = 0; j < cnt; j++) {
+			get_vlan_table_info(vlan, &data[j *
+				READ_VLAN_ENTRY_SIZE]);
+			vlan->vid = addr + j;
+			vlan++;
+		}
+	}
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+}  /* sw_r_m_vlan_table */
+
+/**
+ * sw_w_vlan_hw - write to VLAN table using default access
+ * @sw:		The switch instance.
+ * @data:	VLAN data to write.
+ * @num:	Number of entries to write.
+ *
+ * This routine writes to VLAN table using default access.
+ */
+static void sw_w_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	u8 ctrl;
+	int cnt;
+
+	for (cnt = 0; cnt < num; cnt++) {
+		data[0] = cpu_to_be32(data[0]);
+		data[1] = cpu_to_be32(data[1]);
+		data[2] = cpu_to_be32(data[2]);
+		data[3] = cpu_to_be16(data[3]);
+#if 0
+		sw_w(sw, REG_SW_VLAN_ENTRY__4, data, 4 * 3 + 2);
+#else
+		sw_w(sw, REG_SW_VLAN_ENTRY__4, data, 4 * 3 + 4);
+#endif
+		ctrl = VLAN_WRITE;
+		ctrl |= VLAN_START;
+		sw->reg->w8(sw, REG_SW_VLAN_CTRL, ctrl);
+		do {
+			ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+		} while (ctrl & VLAN_START);
+		data += WRITE_VLAN_ENTRY_SIZE;
+	}
+}  /* sw_w_vlan_hw */
+
+/**
+ * sw_w_vlan_table - write to VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vlan:	The VLAN table entry.
+ *
+ * This routine writes an entry of the VLAN table of the switch.
+ */
+static void sw_w_vlan_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[4];
+
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	wait_for_vlan_table(sw);
+	set_vlan_table_info(vlan, data);
+	data[3] = addr;
+	sw->reg->w_vlan_hw(sw, data, 1);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+	vlan->dirty = 0;
+}  /* sw_w_vlan_table */
+
+/**
+ * sw_d_vlan_table - dump VLAN table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps the VLAN table of the switch.
+ */
+static void sw_d_vlan_table(struct ksz_sw *sw)
+{
+	u16 i;
+	int j;
+	struct ksz_vlan_table *vlan;
+	struct ksz_vlan_table table[8];
+
+	i = 0;
+	do {
+		sw_r_m_vlan_table(sw, i, MAX_IBA_VLAN_ENTRIES, table);
+		for (j = 0; j < MAX_IBA_VLAN_ENTRIES; j++) {
+			vlan = &table[j];
+			if (!vlan->valid)
+				continue;
+			printk(KERN_INFO
+				"%3x: 0x%03x m:%x p:%x o:%u %04x %04x\n",
+				vlan->vid,
+				vlan->fid, vlan->mstp, vlan->prio,
+				vlan->option, vlan->untag, vlan->ports);
+		}
+		yield();
+		i += MAX_IBA_VLAN_ENTRIES;
+	} while (i < 4096);
+}  /* sw_d_vlan_table */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Port functions
+ */
+
+/**
+ * port_chk - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk(struct ksz_sw *sw, int port, int offset, SW_D bits)
+{
+	u32 addr;
+	SW_D data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk */
+
+/**
+ * port_cfg - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg(struct ksz_sw *sw, int port, int offset, SW_D bits,
+	int set)
+{
+	u32 addr;
+	SW_D data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* port_cfg */
+
+/**
+ * port_chk32 - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk32(struct ksz_sw *sw, int port, int offset, u32 bits)
+{
+	u32 addr;
+	u32 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r32(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk32 */
+
+/**
+ * port_cfg32 - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg32(struct ksz_sw *sw, int port, int offset, u32 bits,
+	int set)
+{
+	u32 addr;
+	u32 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r32(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	sw->reg->w32(sw, addr, data);
+}  /* port_cfg32 */
+
+#if 0
+/**
+ * port_chk_shift - check port bit
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the register.
+ * @shift:	Number of bits to shift.
+ *
+ * This function checks whether the specified port is set in the register or
+ * not.
+ *
+ * Return 0 if the port is not set.
+ */
+static int port_chk_shift(struct ksz_sw *sw, int port, u32 addr, int shift)
+{
+	SW_D data;
+	SW_D bit = 1 << port;
+
+	data = SW_R(sw, addr);
+	data >>= shift;
+	return (data & bit) == bit;
+}
+
+/**
+ * port_cfg_shift - set port bit
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the register.
+ * @shift:	Number of bits to shift.
+ * @set:	The flag indicating whether the port is to be set or not.
+ *
+ * This routine sets or resets the specified port in the register.
+ */
+static void port_cfg_shift(struct ksz_sw *sw, int port, u32 addr, int shift,
+	int set)
+{
+	SW_D data;
+	SW_D bits = 1 << port;
+
+	data = SW_R(sw, addr);
+	bits <<= shift;
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}
+#endif
+
+/**
+ * port_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_r8(struct ksz_sw *sw, int port, int offset, u8 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r8(sw, addr);
+}  /* port_r8 */
+
+/**
+ * port_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_w8(struct ksz_sw *sw, int port, int offset, u8 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w8(sw, addr, data);
+}  /* port_w8 */
+
+/**
+ * port_r16 - read word from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a word from the port register.
+ */
+static void port_r16(struct ksz_sw *sw, int port, int offset, u16 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r16(sw, addr);
+}  /* port_r16 */
+
+/**
+ * port_w16 - write word to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a word to the port register.
+ */
+static void port_w16(struct ksz_sw *sw, int port, int offset, u16 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w16(sw, addr, data);
+}  /* port_w16 */
+
+/**
+ * port_r32 - read dword from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a dword from the port register.
+ */
+static void port_r32(struct ksz_sw *sw, int port, int offset, u32 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r32(sw, addr);
+}  /* port_r32 */
+
+/**
+ * port_w32 - write dword to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a dword to the port register.
+ */
+static void port_w32(struct ksz_sw *sw, int port, int offset, u32 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w32(sw, addr, data);
+}  /* port_w32 */
+
+static void port_get(struct ksz_sw *sw, int port, int offset, void *buf,
+	size_t cnt)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->r(sw, addr, buf, cnt);
+}
+
+#if 0
+static void port_set(struct ksz_sw *sw, int port, int offset, void *buf,
+	size_t cnt)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w(sw, addr, buf, cnt);
+}
+#endif
+
+/**
+ * port_r_s - read bits with shift from port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This function reads bits from the port register.
+ */
+static u8 port_r_s(struct ksz_sw *sw, int p, u32 reg, u8 mask, u8 shift)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}  /* port_r_s */
+
+/**
+ * port_w_s - write bits with shift to port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This routine writes bits to the port register.
+ */
+static void port_w_s(struct ksz_sw *sw, int p, u32 reg, u8 mask, u8 shift,
+	u8 val)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w8(sw, p, reg, data);
+}  /* port_w_s */
+
+static u32 port_r_s_32(struct ksz_sw *sw, int p, u32 reg, u32 mask,
+	u32 shift)
+{
+	u32 data;
+
+	port_r32(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}
+
+static void port_w_s_32(struct ksz_sw *sw, int p, u32 reg, u32 mask,
+	u32 shift, u32 val)
+{
+	u32 data;
+
+	port_r32(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w32(sw, p, reg, data);
+}
+
+/**
+ * sw_chk - check switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the switch register are
+ * set or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int sw_chk(struct ksz_sw *sw, u32 addr, SW_D bits)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* sw_chk */
+
+/**
+ * sw_cfg - set switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This function sets or resets the specified bits of the switch register.
+ */
+static void sw_cfg(struct ksz_sw *sw, u32 addr, SW_D bits, int set)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* sw_cfg */
+
+static SW_D sw_r_shift(struct ksz_sw *sw, u32 addr, u32 mask,
+	u32 shift)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	data >>= shift;
+	data &= mask;
+	return data;
+}
+
+static void sw_w_shift(struct ksz_sw *sw, u32 addr, u32 mask, u32 shift,
+	SW_D val)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	SW_W(sw, addr, data);
+}
+
+#ifdef PORT_OUT_RATE_ADDR
+/**
+ * port_out_rate_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_out_rate_r8(struct ksz_sw *sw, int port, int offset, u8 *data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}
+
+/**
+ * port_out_rate_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_out_rate_w8(struct ksz_sw *sw, int port, int offset, u8 data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* ACL */
+
+static inline void port_cfg_acl(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_ACL_ENABLE, set);
+}
+
+static inline int port_chk_acl(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_ACL_ENABLE);
+}
+
+static inline u8 port_get_authen_mode(struct ksz_sw *sw, int p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_AUTHEN_MODE, 0);
+}
+
+static void port_set_authen_mode(struct ksz_sw *sw, int p, u8 mode)
+{
+	port_w_s(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_AUTHEN_MODE, 0, mode);
+}
+
+/**
+ * get_acl_action_info - Get ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL action field information.
+ */
+static void get_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	acl->prio_mode = (data[10] >> ACL_PRIO_MODE_S) & ACL_PRIO_MODE_M;
+	acl->prio = (data[10] >> ACL_PRIO_S) & ACL_PRIO_M;
+	acl->vlan_prio_replace = !!(data[10] & ACL_VLAN_PRIO_REPLACE);
+	acl->vlan_prio = data[11] >> ACL_VLAN_PRIO_S;
+	acl->vlan_prio |= (data[10] & ACL_VLAN_PRIO_HI_M) << 1;
+	acl->map_mode = (data[11] >> ACL_MAP_MODE_S) & ACL_MAP_MODE_M;
+
+	/* byte 12 is not used at all. */
+	acl->ports = data[13];
+}  /* get_acl_action_info */
+
+/**
+ * get_acl_table_info - Get ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL table information.
+ */
+static void get_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+	int cnt = 0;
+
+	acl->first_rule = data[0] & ACL_FIRST_RULE_M;
+	acl->mode = (data[1] >> ACL_MODE_S) & ACL_MODE_M;
+	acl->enable = (data[1] >> ACL_ENABLE_S) & ACL_ENABLE_M;
+	acl->src = !!(data[1] & ACL_SRC);
+	acl->equal = !!(data[1] & ACL_EQUAL);
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			acl->mac[i] = data[2 + i];
+		ptr_16 = (u16 *) &data[8];
+		acl->eth_type = be16_to_cpu(*ptr_16);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			cnt = 1;
+			ptr_16 = (u16 *) &data[10];
+			acl->cnt = (be16_to_cpu(*ptr_16) >> ACL_CNT_S) &
+				ACL_CNT_M;
+			acl->msec =
+				!!(data[ACL_INTR_CNT_START] & ACL_MSEC_UNIT);
+			acl->intr_mode =
+				!!(data[ACL_INTR_CNT_START] & ACL_INTR_MODE);
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_addr[i] = data[j];
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_mask[i] = data[j];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			acl->seqnum = be32_to_cpu(*ptr_32);
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			acl->max_port = be16_to_cpu(*ptr_16);
+			++ptr_16;
+			acl->min_port = be16_to_cpu(*ptr_16);
+			acl->port_mode = (data[6] >> ACL_PORT_MODE_S) &
+				ACL_PORT_MODE_M;
+		}
+		acl->protocol = (data[6] & 1) << 7;
+		acl->protocol |= (data[7] >> 1);
+		acl->tcp_flag_enable = !!(data[7] & ACL_TCP_FLAG_ENABLE);
+		acl->tcp_flag_mask = data[8];
+		acl->tcp_flag = data[9];
+		break;
+	default:
+		break;
+	}
+	if (!cnt)
+		get_acl_action_info(acl, data);
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	acl->ruleset = be16_to_cpu(*ptr_16);
+}  /* get_acl_table_info */
+
+/**
+ * set_acl_action_info - Set ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL action field information.
+ */
+static void set_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[10] = (acl->prio_mode & ACL_PRIO_MODE_M) << ACL_PRIO_MODE_S;
+	data[10] |= (acl->prio & ACL_PRIO_M) << ACL_PRIO_S;
+	if (acl->vlan_prio_replace)
+		data[10] |= ACL_VLAN_PRIO_REPLACE;
+	data[10] |= (acl->vlan_prio >> 1);
+	data[11] = acl->vlan_prio << ACL_VLAN_PRIO_S;
+	data[11] |= (acl->map_mode & ACL_MAP_MODE_M) << ACL_MAP_MODE_S;
+
+	/* byte 12 is not used at all. */
+	data[13] = acl->ports;
+}  /* set_acl_action_info */
+
+/**
+ * set_acl_table_info - Set ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL table information.
+ */
+static void set_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+
+	memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[0] = acl->first_rule & ACL_FIRST_RULE_M;
+	data[1] = (acl->mode & ACL_MODE_M) << ACL_MODE_S;
+	data[1] |= (acl->enable & ACL_ENABLE_M) << ACL_ENABLE_S;
+	if (acl->src)
+		data[1] |= ACL_SRC;
+	if (acl->equal)
+		data[1] |= ACL_EQUAL;
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			data[2 + i] = acl->mac[i];
+		ptr_16 = (u16 *) &data[8];
+		*ptr_16 = cpu_to_be16(acl->eth_type);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			ptr_16 = (u16 *) &data[10];
+			*ptr_16 = cpu_to_be16((acl->cnt & ACL_CNT_M) <<
+				ACL_CNT_S);
+			data[12] = 0;
+			data[ACL_INTR_CNT_START] = 0;
+			if (acl->msec)
+				data[ACL_INTR_CNT_START] |= ACL_MSEC_UNIT;
+			if (acl->intr_mode)
+				data[ACL_INTR_CNT_START] |= ACL_INTR_MODE;
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_addr[i];
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_mask[i];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			*ptr_32 = cpu_to_be32(acl->seqnum);
+			data[6] = 0;
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			*ptr_16 = cpu_to_be16(acl->max_port);
+			++ptr_16;
+			*ptr_16 = cpu_to_be16(acl->min_port);
+			data[6] = (acl->port_mode & ACL_PORT_MODE_M) <<
+				ACL_PORT_MODE_S;
+		}
+		data[6] |= (acl->protocol >> 7);
+		data[7] = (acl->protocol << 1);
+		if (acl->tcp_flag_enable)
+			data[7] |= ACL_TCP_FLAG_ENABLE;
+		data[8] = acl->tcp_flag_mask;
+		data[9] = acl->tcp_flag;
+		break;
+	default:
+		break;
+	}
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	*ptr_16 = cpu_to_be16(acl->ruleset);
+}  /* set_acl_table_info */
+
+/**
+ * wait_for_acl_table - Wait for ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This helper routine waits for ACL table to be ready for access.
+ */
+static void wait_for_acl_table(struct ksz_sw *sw, int port)
+{
+#ifndef NO_ACL
+	u8 ctrl;
+int timeout = 100;
+
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+if (--timeout < 1) {
+dbg_msg(" %s %d %x to\n", __func__, port, ctrl);
+break;
+}
+	} while (!(ctrl & (PORT_ACL_WRITE_DONE | PORT_ACL_READ_DONE)));
+#endif
+}  /* wait_for_acl_table */
+
+/**
+ * sw_r_acl_hw - read from ACL table using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This routine reads from ACL table of the port using default access.
+ */
+static void sw_r_acl_hw(struct ksz_sw *sw, int port, u16 addr, u8 data[])
+{
+#ifndef NO_ACL
+	u8 ctrl = (addr & PORT_ACL_INDEX_M);
+int timeout = 100;
+
+	port_w(sw, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+if (--timeout < 1) {
+dbg_msg(" %s %d %x to\n", __func__, port, addr);
+break;
+}
+	} while (!(ctrl & PORT_ACL_READ_DONE));
+	sw_r(sw, PORT_CTRL_ADDR(port, REG_PORT_ACL_0), data, ACL_TABLE_LEN);
+#else
+	memset(data, 0, 16);
+#endif
+}  /* sw_r_acl_hw */
+
+/**
+ * sw_r_acl_table - read from ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	Buffer to store the ACL entry.
+ *
+ * This function reads an entry of the ACL table of the port.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_acl_table(struct ksz_sw *sw, int port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	int rc = -1;
+
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	sw->reg->r_acl_hw(sw, port, addr, data);
+	get_acl_table_info(acl, data);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+	sw->ops->release(sw);
+	if (acl->mode)
+		rc = 0;
+	acl->changed = 0;
+	return rc;
+}  /* sw_r_acl_table */
+
+/**
+ * sw_w_acl_hw - write to ACL table using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This routine writes to ACL table of the port using default access.
+ */
+static void sw_w_acl_hw(struct ksz_sw *sw, int port, u16 addr, u8 data[])
+{
+#ifndef NO_ACL
+	u8 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+#if 1
+int timeout = 100;
+#endif
+
+	sw_w(sw, PORT_CTRL_ADDR(port, REG_PORT_ACL_0), data, ACL_TABLE_LEN);
+	port_w(sw, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+#if 1
+if (--timeout < 1) {
+dbg_msg(" %s %d %x to\n", __func__, port, addr);
+break;
+}
+#endif
+	} while (!(ctrl & PORT_ACL_WRITE_DONE));
+#endif
+}  /* sw_w_acl_hw */
+
+/**
+ * sw_w_acl_action - write to ACL action field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the action field of an entry of the ACL table.
+ */
+static void sw_w_acl_action(struct ksz_sw *sw, int port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	set_acl_action_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_ACTION_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(&acl->data[ACL_ACTION_START], &data[ACL_ACTION_START],
+		ACL_ACTION_LEN);
+	sw->ops->release(sw);
+	acl->action_changed = 0;
+}  /* sw_w_acl_action */
+
+/**
+ * sw_w_acl_table - write to ACL matching and process fields
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the matching and process fields of an entry of the
+ * ACL table of the port.
+ */
+static void sw_w_acl_table(struct ksz_sw *sw, int port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	u16 byte_enable = ACL_MATCH_ENABLE;
+
+	if (ACL_MODE_LAYER_2 == acl->mode && ACL_ENABLE_2_COUNT == acl->enable)
+		byte_enable = ACL_BYTE_ENABLE;
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	set_acl_table_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, byte_enable);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(acl->data, data, ACL_ACTION_START);
+	memcpy(&acl->data[ACL_RULESET_START], &data[ACL_RULESET_START], 2);
+	sw->ops->release(sw);
+	acl->changed = 0;
+}  /* sw_w_acl_table */
+
+/**
+ * acl_action_info - format ACL action field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL action field information.
+ */
+static int acl_action_info(struct ksz_acl_table *acl, u16 index, char *buf,
+	int len)
+{
+	char prio = 'p';
+	char vlan = 'v';
+
+	if (acl->prio_mode != ACL_PRIO_MODE_DISABLE)
+		prio = 'P';
+	if (acl->vlan_prio_replace)
+		vlan = 'V';
+	len += sprintf(buf + len,
+		"%x: %c:%u=%u %c:%u=%u %u=%04x [%u]\n",
+		index,
+		prio, acl->prio_mode, acl->prio,
+		vlan, acl->vlan_prio_replace, acl->vlan_prio,
+		acl->map_mode, acl->ports,
+		acl->action_changed ? 8 : 1);
+	return len;
+}  /* acl_action_info */
+
+/**
+ * acl_info - format ACL matching and process field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL matching and process field information.
+ */
+static int acl_info(struct ksz_acl_table *acl, u16 index, char *buf, int len)
+{
+	char enable = 'e';
+	char equal = 'q';
+	char src = 's';
+	char cnt = 'c';
+	char protocol = 'x';
+	char flag = 'f';
+	char seqnum = 's';
+	char msec[4];
+
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		enable = 'E';
+		*msec = 0;
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+			cnt = 'C';
+			if (acl->intr_mode)
+				*msec = 0;
+			else if (acl->msec)
+				strcpy(msec, "ms ");
+			else
+				strcpy(msec, "us ");
+		} else {
+			equal = 'Q';
+			if (ACL_ENABLE_2_TYPE != acl->enable)
+				src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %02X:%02X:%02X:%02X:%02X:%02X-%04x "
+			"%c:%u.%u %s"
+			"%c:%u %c:%u %c:%u "
+			"%x:%04x [%u]\n",
+			index,
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5],
+			acl->eth_type,
+			cnt, acl->intr_mode, acl->cnt, msec,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->first_rule, acl->ruleset,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_3:
+		if (ACL_ENABLE_3_IP == acl->enable ||
+		    ACL_ENABLE_3_SRC_DST_COMP == acl->enable)
+			enable = 'E';
+		if (ACL_ENABLE_3_IP == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %u.%u.%u.%u:%u.%u.%u.%u "
+			"%c:%u %c:%u %c:%u "
+			"%x:%04x [%u]\n",
+			index,
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3],
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3],
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->first_rule, acl->ruleset,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_4:
+		enable = 'E';
+		if (ACL_ENABLE_4_PROTOCOL == acl->enable) {
+			protocol = 'X';
+			equal = 'Q';
+		} else if (ACL_ENABLE_4_TCP_SEQN_COMP == acl->enable) {
+			seqnum = 'S';
+			equal = 'Q';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else if (ACL_ENABLE_4_TCP_PORT_COMP == acl->enable) {
+			src = 'S';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else
+			src = 'S';
+		len += sprintf(buf + len,
+			"%x: %u=%4x-%4x 0%c%x %c:%08x %c:%u=%x:%x "
+			"%c:%u %c:%u %c:%u "
+			"%x:%04x [%u]\n",
+			index,
+			acl->port_mode, acl->min_port, acl->max_port,
+			protocol, acl->protocol, seqnum, acl->seqnum,
+			flag, acl->tcp_flag_enable,
+			acl->tcp_flag, acl->tcp_flag_mask,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->first_rule, acl->ruleset,
+			acl->changed ? 8 : acl->mode);
+		break;
+	default:
+		len += sprintf(buf + len,
+			"%x: "
+			"%c:%u %c:%u %c:%u "
+			"%x:%04x [%u]\n",
+			index,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->first_rule, acl->ruleset,
+			acl->changed ? 8 : acl->mode);
+		break;
+	}
+	return len;
+}  /* acl_info */
+
+/**
+ * sw_d_acl_table - dump ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine dumps ACL table of the port.
+ */
+static void sw_d_acl_table(struct ksz_sw *sw, int port)
+{
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+	char buf[100];
+	int len;
+	int min = 0;
+
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		acl->action_selected = false;
+		sw_r_acl_table(sw, port, i, acl);
+	}
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->mode)
+			continue;
+		if (!min)
+			printk(KERN_INFO "rules:\n");
+		cfg->acl_info[acl->first_rule].action_selected = true;
+		len = acl_info(acl, i, buf, 0);
+		printk(KERN_INFO "%s", buf);
+		min = 1;
+	}
+	if (min)
+		printk(KERN_INFO "\n");
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (ACL_PRIO_MODE_DISABLE == acl->prio_mode &&
+		    ACL_MAP_MODE_DISABLE == acl->map_mode &&
+		    !acl->vlan_prio_replace && !acl->action_selected)
+			continue;
+		if (ACL_MODE_LAYER_2 == acl->mode &&
+		    ACL_ENABLE_2_COUNT == acl->enable)
+			continue;
+		if (!min)
+			printk(KERN_INFO "actions:\n");
+		len = acl_action_info(acl, i, buf, 0);
+		printk(KERN_INFO "%s", buf);
+		min = 1;
+	}
+}  /* sw_d_acl_table */
+
+static void sw_reset_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	int port;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		cfg = &sw->info->port_cfg[port];
+		memset(cfg->acl_info, 0, sizeof(struct ksz_acl_table) *
+			ACL_TABLE_ENTRIES);
+		cfg->acl_index = cfg->acl_act_index = 0;
+	}
+}  /* sw_reset_acl */
+
+static void sw_reset_acl_hw(struct ksz_sw *sw)
+{
+#ifndef NO_ACL
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	int port;
+
+	sw_reset_acl(sw);
+	sw->ops->release(sw);
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		cfg = &sw->info->port_cfg[port];
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			acl->data[0] = 0xff;
+			acl->mode = 0;
+			sw_w_acl_action(sw, port, i, acl);
+			sw_w_acl_table(sw, port, i, acl);
+		}
+	}
+	sw->ops->acquire(sw);
+#endif
+}  /* sw_reset_acl_hw */
+
+static void sw_init_acl(struct ksz_sw *sw)
+{
+#ifndef NO_ACL
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	int port;
+#if 0
+	u8 data[20];
+
+	memset(data, 0, 20);
+	data[11] = 0x60;
+#endif
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port_cfg_acl(sw, port, 1);
+#if 0
+		port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, 0xffff);
+/*
+ * THa  2015/11/29
+ * The DLR beacon drop 2-entry rule occasionally leaks the beacon when running
+ * in Gigabit and heavy traffic unless the port mapping action in first entry
+ * is set in AND or REPLACE with empty ports.
+ */
+		wait_for_acl_table(sw, port);
+		sw->reg->w_acl_hw(sw, port, 0, data);
+#endif
+		cfg = &sw->info->port_cfg[port];
+		sw->ops->release(sw);
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			sw_r_acl_table(sw, port, i, acl);
+		}
+		sw->ops->acquire(sw);
+	}
+#endif
+}  /* sw_init_acl */
+
+/* -------------------------------------------------------------------------- */
+
+/* 36-bit counts. */
+#define MIB_RX_HI_PRIO			0x00
+#define MIB_RX_UNDERSIZE		0x01
+#define MIB_RX_FRAGMENT			0x02
+#define MIB_RX_OVERSIZE			0x03
+#define MIB_RX_JABBER			0x04
+#define MIB_RX_SYMBOL_ERR		0x05
+#define MIB_RX_CRC_ERR			0x06
+#define MIB_RX_ALIGNMENT_ERR		0x07
+#define MIB_RX_CTRL_8808		0x08
+#define MIB_RX_PAUSE			0x09
+#define MIB_RX_BROADCAST		0x0A
+#define MIB_RX_MULTICAST		0x0B
+#define MIB_RX_UNICAST			0x0C
+#define MIB_RX_OCTET_64			0x0D
+#define MIB_RX_OCTET_65_127		0x0E
+#define MIB_RX_OCTET_128_255		0x0F
+#define MIB_RX_OCTET_256_511		0x10
+#define MIB_RX_OCTET_512_1023		0x11
+#define MIB_RX_OCTET_1024_1522		0x12
+#define MIB_RX_OCTET_1523_2000		0x13
+#define MIB_RX_OCTET_2001		0x14
+#define MIB_TX_HI_PRIO			0x15
+#define MIB_TX_LATE_COLLISION		0x16
+#define MIB_TX_PAUSE			0x17
+#define MIB_TX_BROADCAST		0x18
+#define MIB_TX_MULTICAST		0x19
+#define MIB_TX_UNICAST			0x1A
+#define MIB_TX_DEFERRED			0x1B
+#define MIB_TX_TOTAL_COLLISION		0x1C
+#define MIB_TX_EXCESS_COLLISION		0x1D
+#define MIB_TX_SINGLE_COLLISION		0x1E
+#define MIB_TX_MULTI_COLLISION		0x1F
+
+#define MIB_RX_TOTAL			0x20
+#define MIB_TX_TOTAL			0x21
+
+#define MIB_RX_DROPS			0x22
+#define MIB_TX_DROPS			0x23
+
+/* Actual locations. */
+#define MIB_9897_RX_BYTE_CNT		0x80
+#define MIB_9897_TX_BYTE_CNT		0x81
+
+#define MIB_9897_RX_DROPPED_PACKET	0x82
+#define MIB_9897_TX_DROPPED_PACKET	0x83
+
+static struct {
+	char string[20];
+} mib_names[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx_hi        " },
+	{ "rx_undersize" },
+	{ "rx_fragments" },
+	{ "rx_oversize" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_err" },
+	{ "rx_crc_err" },
+	{ "rx_align_err" },
+	{ "rx_mac_ctrl" },
+	{ "rx_pause" },
+	{ "rx_bcast" },
+	{ "rx_mcast" },
+	{ "rx_ucast" },
+	{ "rx_64_or_less" },
+	{ "rx_65_127" },
+	{ "rx_128_255" },
+	{ "rx_256_511" },
+	{ "rx_512_1023" },
+	{ "rx_1024_1522" },
+	{ "rx_1523_2000" },
+	{ "rx_2001     " },
+
+	{ "tx_hi        " },
+	{ "tx_late_col" },
+	{ "tx_pause" },
+	{ "tx_bcast" },
+	{ "tx_mcast" },
+	{ "tx_ucast" },
+	{ "tx_deferred" },
+	{ "tx_total_col" },
+	{ "tx_exc_col" },
+	{ "tx_single_col" },
+	{ "tx_mult_col" },
+
+	{ "rx_total" },
+	{ "tx_total" },
+
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+/*
+ * Some counters do not need to be read too often because they are less likely
+ * to increase much.
+ */
+static u8 mib_read_max[SWITCH_COUNTER_NUM] = {
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	1,
+	1,
+	1,
+	1,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+
+	1,
+	4,
+	1,
+	1,
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+};
+
+static u8 mib_start[TOTAL_SWITCH_COUNTER_NUM];
+static u8 mib_index[TOTAL_SWITCH_COUNTER_NUM];
+
+static u8 sw_fill_mib_index(struct ksz_sw *sw, u8 index, u8 interval)
+{
+	int i;
+
+	for (i = 0; i < SWITCH_COUNTER_NUM; i++) {
+		if (interval == mib_read_max[i])
+			mib_start[index++] = i;
+	}
+	return index;
+}  /* sw_fill_mib_index */
+
+static void sw_setup_mib(struct ksz_sw *sw)
+{
+	int i;
+	int j;
+	u8 index = 0;
+
+	for (i = 0; i < 3; i++) {
+		j = 2 - i;
+		sw->mib_interval_start[j] = index;
+		index = sw_fill_mib_index(sw, index, 1 << j);
+	}
+	memcpy(mib_index, mib_start, SWITCH_COUNTER_NUM);
+	j = MIB_9897_RX_BYTE_CNT;
+	for (i = 0; i < 4; i++, j++, index++) {
+		mib_start[index] = SWITCH_COUNTER_NUM + i;
+		mib_index[index] = j;
+	}
+}  /* sw_setup_mib */
+
+static void get_mib_cnt_info(u64 *cnt, u32 data[])
+{
+	u64 num;
+
+	if (data[0] & MIB_COUNTER_OVERFLOW) {
+		num = 1;
+		num <<= 32 + 4;
+		*cnt += num;
+	}
+	num = (data[0] & MIB_COUNTER_DATA_HI_M);
+	num <<= 32;
+	num |= data[1];
+	*cnt += num;
+}  /* get_mib_cnt_info */
+
+/**
+ * sw_r_mib_cnt_hw - read MIB counters using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The addresses of the counters.
+ * @num:	Number of entries to read.
+ * @data:	Buffer to store the counters.
+ *
+ * This routine reads MIB counters of the port using default access.
+ */
+static void sw_r_mib_cnt_hw(struct ksz_sw *sw, int port, u32 addr[], int num,
+	u32 data[])
+{
+	int i;
+	u32 ctrl_addr;
+
+	for (i = 0; i < num; i++) {
+		ctrl_addr = (addr[i] & MIB_COUNTER_INDEX_M);
+		ctrl_addr <<= MIB_COUNTER_INDEX_S;
+		ctrl_addr |= MIB_COUNTER_READ;
+
+#if 1
+		/*
+		 * First KSZ956X revision chip has a bug that writing 32-bit
+		 * data to the register does not trigger the read operation.
+		 */
+		port_w16(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl_addr >> 16);
+#else
+		port_w32(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl_addr);
+#endif
+		/*
+		 * Need to check the valid bit, but SPI access is slow enough
+		 * to have that bit always set when reading.
+		 */
+		do {
+			port_get(sw, port, REG_PORT_MIB_CTRL_STAT__4, data, 8);
+
+			data[0] = be32_to_cpu(data[0]);
+#ifndef NO_PHY_READ
+			if (!(data[0] & MIB_COUNTER_VALID))
+				dbg_msg(" !valid: %08x\n", data[0]);
+#endif
+			data[1] = be32_to_cpu(data[1]);
+		} while (!(data[0] & MIB_COUNTER_VALID));
+		data += READ_MIB_ENTRY_SIZE;
+	}
+}  /* sw_r_mib_cnt_hw */
+
+/**
+ * port_r_cnt - read MIB counters periodically
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to read the counters of the port periodically to avoid
+ * counter overflow.  The hardware should be acquired first before calling this
+ * routine.
+ *
+ * Return non-zero when not all counters not read.
+ */
+static int port_r_cnt(struct ksz_sw *sw, int port)
+{
+	struct ksz_port_mib *mib = &sw->port_mib[port];
+	u32 index[MAX_IBA_MIB_ENTRIES];
+	u32 data[MAX_IBA_MIB_ENTRIES * READ_MIB_ENTRY_SIZE];
+	u8 start;
+	int cnt;
+	int i;
+
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+return 0;
+}
+#endif
+	/* First read in this interval. */
+	if (!mib->cnt_ptr) {
+		u8 interval;
+
+		++mib->interval;
+		switch (mib->interval) {
+		case 2:
+			interval = 1;
+			break;
+		case 4:
+			interval = 2;
+			mib->interval = 0;
+			break;
+		default:
+			interval = 0;
+		}
+
+		/* Determine the starting index in this interval. */
+		mib->cnt_ptr = sw->mib_interval_start[interval];
+	}
+	while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM) {
+		cnt = MAX_IBA_MIB_ENTRIES;
+		if (cnt > TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr)
+			cnt = TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr;
+		for (i = 0; i < cnt; i++)
+			index[i] = mib_index[mib->cnt_ptr + i];
+		sw->ops->acquire(sw);
+		sw->reg->r_mib_cnt_hw(sw, port, index, cnt, data);
+		sw->ops->release(sw);
+		for (i = 0; i < cnt; i++, mib->cnt_ptr++) {
+			start = mib_start[mib->cnt_ptr];
+			get_mib_cnt_info(&mib->counter[start],
+				&data[i * READ_MIB_ENTRY_SIZE]);
+		}
+		if (exit_mib_read(sw))
+			return mib->cnt_ptr;
+	}
+	mib->cnt_ptr = 0;
+	return 0;
+}  /* port_r_cnt */
+
+/**
+ * port_init_cnt - initialize MIB counter values
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to initialize all counters to zero if the hardware
+ * cannot do it after reset.
+ */
+static inline void port_init_cnt(struct ksz_sw *sw, int port)
+{
+	struct ksz_port_mib *mib = &sw->port_mib[port];
+	u32 index[MAX_IBA_MIB_ENTRIES];
+	u32 data[MAX_IBA_MIB_ENTRIES * READ_MIB_ENTRY_SIZE];
+	int cnt;
+	int i;
+
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+return;
+}
+#endif
+	sw->ops->acquire(sw);
+	mib->cnt_ptr = 0;
+	mib->interval = 0;
+	do {
+		cnt = MAX_IBA_MIB_ENTRIES;
+		if (cnt > TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr)
+			cnt = TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr;
+		for (i = 0; i < cnt; i++, mib->cnt_ptr++)
+			index[i] = mib_index[mib->cnt_ptr];
+		sw->reg->r_mib_cnt_hw(sw, port, index, cnt, data);
+	} while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM);
+	memset((void *) mib->counter, 0, sizeof(u64) *
+		TOTAL_SWITCH_COUNTER_NUM);
+	mib->cnt_ptr = 0;
+	sw->ops->release(sw);
+}  /* port_init_cnt */
+
+/* -------------------------------------------------------------------------- */
+
+/* Bandwidth */
+
+static inline void port_cfg_broad_storm(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM, set);
+}
+
+static inline int port_chk_broad_storm(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM);
+}
+
+/* Driver set switch broadcast storm protection at 10% rate. */
+#define BROADCAST_STORM_PROTECTION_RATE	10
+
+/* 148,800 frames * 67 ms / 100 */
+#define BROADCAST_STORM_VALUE		9969
+
+/**
+ * sw_cfg_broad_storm - configure broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ */
+static void sw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	u16 data;
+	u32 value = ((u32) BROADCAST_STORM_VALUE * (u32) percent / 100);
+
+	if (value > BROADCAST_STORM_RATE)
+		value = BROADCAST_STORM_RATE;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	data &= ~BROADCAST_STORM_RATE;
+	data |= value;
+	sw->reg->w16(sw, S_REPLACE_VID_CTRL, data);
+}  /* sw_cfg_broad_storm */
+
+/**
+ * sw_get_board_storm - get broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Buffer to store the broadcast storm threshold percentage.
+ *
+ * This routine retrieves the broadcast storm threshold of the switch.
+ */
+static void sw_get_broad_storm(struct ksz_sw *sw, u8 *percent)
+{
+	int num;
+	u16 data;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	num = (data & BROADCAST_STORM_RATE);
+	num = (num * 100 + BROADCAST_STORM_VALUE / 2) / BROADCAST_STORM_VALUE;
+	*percent = (u8) num;
+}  /* sw_get_broad_storm */
+
+/**
+ * sw_dis_broad_storm - disable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the broadcast storm limit function of the switch.
+ */
+static void sw_dis_broad_storm(struct ksz_sw *sw, int port)
+{
+	port_cfg_broad_storm(sw, port, 0);
+}  /* sw_dis_broad_storm */
+
+/**
+ * sw_ena_broad_storm - enable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the broadcast storm limit function of the switch.
+ */
+static void sw_ena_broad_storm(struct ksz_sw *sw, int port)
+{
+	sw_cfg_broad_storm(sw, sw->info->broad_per);
+	port_cfg_broad_storm(sw, port, 1);
+}  /* sw_ena_broad_storm */
+
+/**
+ * sw_init_broad_storm - initialize broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the broadcast storm limit function of the switch.
+ */
+static void sw_init_broad_storm(struct ksz_sw *sw)
+{
+	u8 percent;
+
+	sw_get_broad_storm(sw, &percent);
+	sw->info->broad_per = percent;
+}  /* sw_init_broad_storm */
+
+/**
+ * hw_cfg_broad_storm - configure broadcast storm
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	if (percent > 100)
+		percent = 100;
+
+	sw_cfg_broad_storm(sw, percent);
+	sw_init_broad_storm(sw);
+}  /* hw_cfg_broad_storm */
+
+/**
+ * sw_setup_broad_storm - setup broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine setup the broadcast storm limit function of the switch.
+ */
+static void sw_setup_broad_storm(struct ksz_sw *sw)
+{
+	int port;
+
+	/* Enable switch broadcast storm protection at 10% percent rate. */
+	hw_cfg_broad_storm(sw, BROADCAST_STORM_PROTECTION_RATE);
+	for (port = 0; port < sw->mib_port_cnt; port++)
+		sw_ena_broad_storm(sw, port);
+	sw_cfg(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE, 1);
+}  /* sw_setup_broad_storm */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Limit */
+
+/**
+ * hw_cfg_rate_limit - configure port rate limit modes
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mask:	The mask value.
+ * @shift:	The shift position.
+ * @mode:	The rate limit mode.
+ *
+ * This helper routine configures the rate limit modes of the port.
+ */
+static void hw_cfg_rate_limit(struct ksz_sw *sw, int port, u8 mask, u8 shift,
+	u8 mode)
+{
+	u8 data;
+	u8 saved;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	saved = data;
+	data &= ~(mask << shift);
+	data |= mode << shift;
+	if (data != saved)
+		port_w8(sw, port, P_RATE_LIMIT_CTRL, data);
+	sw->info->port_cfg[port].rate_limit = data;
+}  /* hw_cfg_rate_limit */
+
+static void hw_cfg_in_port_based(struct ksz_sw *sw, int port, int set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_PORT_BASED_S, set != 0);
+}
+
+static void hw_cfg_in_packet_based(struct ksz_sw *sw, int port, int set)
+{
+	sw->info->port_cfg[port].packet_based = set;
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_PACKET_BASED_S, set != 0);
+}
+
+static void hw_cfg_in_flow_ctrl(struct ksz_sw *sw, int port, int set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_FLOW_CTRL_S, set != 0);
+}
+
+/**
+ * hw_cfg_cnt_ifg - configure port rate limit count IFG control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count IFG control of the port.
+ */
+static void hw_cfg_cnt_ifg(struct ksz_sw *sw, int port, int set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_IFG_S, set != 0);
+}  /* hw_cfg_cnt_ifg */
+
+/**
+ * hw_cfg_cnt_pre - configure port rate limit count preamble control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count preamble control of the port.
+ */
+static void hw_cfg_cnt_pre(struct ksz_sw *sw, int port, int set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_PREAMBLE_S, set != 0);
+}  /* hw_cfg_cnt_pre */
+
+/**
+ * hw_cfg_rx_limit - configure port rate limit mode
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mode:	The rate limit mode.
+ *
+ * This routine configures the rate limit mode of the port.
+ */
+static void hw_cfg_rx_limit(struct ksz_sw *sw, int port, u8 mode)
+{
+	if (mode > PORT_IN_LIMIT_MODE_M)
+		return;
+
+	hw_cfg_rate_limit(sw, port, PORT_IN_LIMIT_MODE_M,
+		PORT_IN_LIMIT_MODE_S, mode);
+}  /* hw_cfg_rx_limit */
+
+/**
+ * hw_get_rate_limit - get port rate limit control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine retrieves the rate limit of the port.
+ */
+static void hw_get_rate_limit(struct ksz_sw *sw, int port)
+{
+	u8 data;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	sw->info->port_cfg[port].rate_limit = data;
+	sw->info->port_cfg[port].packet_based = (data >>
+		PORT_IN_PACKET_BASED_S) & 1;
+}  /* hw_get_rate_limit */
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_rate_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1000 * val;
+	else
+		i = 64 * (val - 100);
+	return i;
+}
+
+static int get_rate_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1000) {
+		i = (rate + 500) / 1000;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate)
+		i = 0;
+	else {
+		i = (rate + 32) / 64;
+		if (0 == i)
+			i = 1;
+		else if (i > 15)
+			i = 15;
+		i += 100;
+	}
+	return i;
+}
+
+static uint get_packet_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1920 * val;
+	else if (101 == val)
+		i = 64;
+	else
+		i = 128 * (val - 101);
+	return i;
+}
+
+static int get_packet_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1920) {
+		i = (rate + 960) / 1920;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate)
+		i = 0;
+	else if (rate <= 64)
+		i = 101;
+	else {
+		i = (rate + 64) / 128;
+		if (0 == i)
+			i = 1;
+		else if (i > 14)
+			i = 14;
+		i += 101;
+	}
+	return i;
+}
+
+/**
+ * port_cfg_rate - configure port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to set the value.
+ * @rate:	The rate limit in number of Kbps.
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port.
+ */
+static void port_cfg_rate(struct ksz_sw *sw, int port, int prio, int offset,
+	uint rate, int packet)
+{
+	u8 factor;
+
+	if (packet)
+		factor = (u8) get_packet_to_val(rate);
+	else
+		factor = (u8) get_rate_to_val(rate);
+
+	port_w8(sw, port, offset + prio, factor);
+}  /* port_cfg_rate */
+
+/**
+ * port_get_rate - get port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to get the value.
+ * @rate:	Buffer to store the data rate in number of Kbps.
+ * @packet:	Buffer to store the data rate in number of packets.
+ *
+ * This helper routine retrieves the priority rate of the port.
+ */
+static void port_get_rate(struct ksz_sw *sw, int port, int prio, int offset,
+	uint *rate, uint *packet)
+{
+	u8 data;
+
+	port_r8(sw, port, offset + prio, &data);
+	*rate = get_rate_from_val(data);
+	*packet = get_packet_from_val(data);
+}  /* port_get_rate */
+
+/**
+ * hw_cfg_prio_rate - configure port priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ * @offset:	The receive or transmit rate offset.
+ * @result:	Buffer to store the data rate in number of Kbps.
+ * @r_packet:	Buffer to store the data rate in number of packets.
+ *
+ * This helper routine configures the priority rate of the port and retrieves
+ * the actual rate number.
+ */
+static void hw_cfg_prio_rate(struct ksz_sw *sw, int port, int prio, uint rate,
+	int packet, int offset, uint *result_rate, uint *r_packet)
+{
+	port_cfg_rate(sw, port, prio, offset, rate, packet);
+	port_get_rate(sw, port, prio, offset, result_rate, r_packet);
+}  /* hw_cfg_prio_rate */
+
+/**
+ * hw_cfg_rx_prio_rate - configure port receive priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This routine configures the receive priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_rx_prio_rate(struct ksz_sw *sw, int port, int prio,
+	uint rate)
+{
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		sw->info->port_cfg[port].packet_based,
+		REG_PORT_IN_RATE_0,
+		&sw->info->port_cfg[port].rx_rate[prio],
+		&sw->info->port_cfg[port].rx_packet[prio]);
+}  /* hw_cfg_rx_prio_rate */
+
+/**
+ * hw_cfg_tx_prio_rate - configure port transmit priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This routine configures the transmit priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tx_prio_rate(struct ksz_sw *sw, int port, int prio,
+	uint rate)
+{
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		sw->info->port_cfg[port].packet_based,
+		REG_PORT_OUT_RATE_0,
+		&sw->info->port_cfg[port].tx_rate[prio],
+		&sw->info->port_cfg[port].tx_packet[prio]);
+}  /* hw_cfg_tx_prio_rate */
+
+/**
+ * sw_chk_prio_rate - check switch priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_prio_rate(struct ksz_sw *sw, int port)
+{
+	u32 addr;
+	u32 rate_addr;
+	u32 in_rate0;
+	u32 in_rate1;
+	u32 out_rate;
+
+	addr = PORT_CTRL_ADDR(port, 0);
+	rate_addr = addr + REG_PORT_IN_RATE_0;
+	in_rate0 = sw->reg->r32(sw, rate_addr);
+	in_rate1 = sw->reg->r32(sw, rate_addr + 4);
+	rate_addr = addr + REG_PORT_OUT_RATE_0;
+	out_rate = sw->reg->r32(sw, rate_addr);
+	return (in_rate0 | in_rate1 | out_rate) != 0;
+}  /* sw_chk_prio_rate */
+
+/**
+ * sw_dis_prio_rate - disable switch priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the priority rate function of the switch.
+ */
+static void sw_dis_prio_rate(struct ksz_sw *sw, int port)
+{
+	u32 addr;
+	u32 rate_addr;
+
+	addr = PORT_CTRL_ADDR(port, 0);
+	rate_addr = addr + REG_PORT_IN_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+	sw->reg->w32(sw, rate_addr + 4, 0);
+	rate_addr = addr + REG_PORT_OUT_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_prio_rate */
+
+/**
+ * sw_ena_prio_rate - enable switch priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the priority rate function of the switch.
+ */
+static void sw_ena_prio_rate(struct ksz_sw *sw, int port)
+{
+	int prio;
+
+	for (prio = 0; prio < PRIO_QUEUES; prio++) {
+		hw_cfg_rx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].rx_rate[prio]);
+		hw_cfg_tx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].tx_rate[prio]);
+	}
+	for (; prio < RX_PRIO_QUEUES; prio++)
+		hw_cfg_rx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].rx_rate[prio]);
+}  /* sw_ena_prio_rate */
+
+/**
+ * sw_init_prio_rate - initialize switch prioirty rate
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the priority rate function of the switch.
+ */
+static void sw_init_prio_rate(struct ksz_sw *sw)
+{
+	int offset;
+	int port;
+	int prio;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		hw_get_rate_limit(sw, port);
+		for (prio = 0; prio < RX_PRIO_QUEUES; prio++) {
+			offset = REG_PORT_IN_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&sw->info->port_cfg[port].rx_rate[prio],
+				&sw->info->port_cfg[port].rx_packet[prio]);
+		}
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			offset = REG_PORT_OUT_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&sw->info->port_cfg[port].tx_rate[prio],
+				&sw->info->port_cfg[port].tx_packet[prio]);
+		}
+	}
+}  /* sw_init_prio_rate */
+
+/* -------------------------------------------------------------------------- */
+
+/* Communication */
+
+static inline void port_cfg_back_pressure(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MAC_CTRL_1, PORT_BACK_PRESSURE, set);
+}
+
+static inline void port_cfg_force_flow_ctrl(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0,
+		PORT_FORCE_TX_FLOW_CTRL | PORT_FORCE_RX_FLOW_CTRL, set);
+}
+
+static inline void port_cfg_tail_tag(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0, PORT_TAIL_TAG_ENABLE, set);
+}
+
+static inline int port_chk_back_pressure(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MAC_CTRL_1, PORT_BACK_PRESSURE);
+}
+
+static inline int port_chk_force_flow_ctrl(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0,
+		PORT_FORCE_TX_FLOW_CTRL | PORT_FORCE_RX_FLOW_CTRL);
+}
+
+static inline int port_chk_tail_tag(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0, PORT_TAIL_TAG_ENABLE);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Spanning Tree */
+
+static inline void port_cfg_mstp(struct ksz_sw *sw, int p, u8 mstp)
+{
+	port_w(sw, p, REG_PORT_LUE_MSTP_INDEX, mstp);
+}
+
+static inline void port_cfg_dis_learn(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE, set);
+}
+
+static inline void port_cfg_rx(struct ksz_sw *sw, int p, int set)
+{
+	SW_D mstp;
+
+	port_r(sw, p, REG_PORT_LUE_MSTP_INDEX, &mstp);
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE, set);
+	if (mstp)
+		return;
+	if (set)
+		sw->rx_ports |= (1 << p);
+	else
+		sw->rx_ports &= ~(1 << p);
+}
+
+static inline void port_cfg_tx(struct ksz_sw *sw, int p, int set)
+{
+	SW_D mstp;
+
+	port_r(sw, p, REG_PORT_LUE_MSTP_INDEX, &mstp);
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE, set);
+	if (mstp)
+		return;
+	if (set)
+		sw->tx_ports |= (1 << p);
+	else
+		sw->tx_ports &= ~(1 << p);
+}
+
+static inline u8 port_chk_mstp(struct ksz_sw *sw, int p)
+{
+	SW_D mstp;
+
+	port_r(sw, p, REG_PORT_LUE_MSTP_INDEX, &mstp);
+	return mstp;
+}
+
+static inline int port_chk_dis_learn(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE);
+}
+
+static inline int port_chk_rx(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE);
+}
+
+static inline int port_chk_tx(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE);
+}
+
+static inline void sw_cfg_fast_aging(struct ksz_sw *sw, int set)
+{
+	sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING, set);
+}
+
+static void sw_flush_dyn_mac_table(struct ksz_sw *sw, int port)
+{
+	int cnt;
+	int first;
+	int index;
+	int learn_disable[TOTAL_PORT_NUM];
+	SW_D data;
+
+	data = SW_R(sw, REG_SW_LUE_CTRL_2);
+	data &= ~(SW_FLUSH_OPTION_M << SW_FLUSH_OPTION_S);
+	data |= (SW_FLUSH_OPTION_DYN_MAC << SW_FLUSH_OPTION_S);
+	SW_W(sw, REG_SW_LUE_CTRL_2, data);
+	if (port < sw->mib_port_cnt) {
+		first = port;
+		cnt = port + 1;
+		for (index = first; index < cnt; index++) {
+			learn_disable[index] = port_chk_dis_learn(sw, index);
+			if (!learn_disable[index])
+				port_cfg_dis_learn(sw, index, 1);
+		}
+		sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_DYN_MAC_TABLE, 1);
+		for (index = first; index < cnt; index++) {
+			if (!learn_disable[index])
+				port_cfg_dis_learn(sw, index, 0);
+		}
+	} else {
+		first = 0;
+		cnt = sw->mib_port_cnt;
+		sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_STP_TABLE, 1);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* VLAN */
+
+static inline void port_cfg_drop_non_vlan(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_NON_VLAN, set);
+}
+
+/**
+ * port_cfg_drop_tag - configure 802.1q tagged packet drop
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static inline void port_cfg_drop_tag(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_TAG, set);
+}
+
+static inline int port_chk_drop_non_vlan(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_NON_VLAN);
+}
+
+static inline int port_chk_drop_tag(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_TAG);
+}
+
+/**
+ * port_cfg_dis_non_vid - configure discard non VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Discard Non VID packets of the switch port.
+ * If enabled, the device will discard packets whose VLAN id does not match
+ * ingress port-based default VLAN id.
+ */
+static inline void port_cfg_dis_non_vid(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_LUE_CTRL, PORT_DISCARD_NON_VID, set);
+}  /* port_cfg_dis_non_vid */
+
+/**
+ * port_cfg_in_filter - configure ingress VLAN filtering
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Ingress VLAN filtering of the switch port.
+ * If enabled, the device will discard packets whose VLAN id membership	in the
+ * VLAN table receive ports does not include the ingress port that received
+ * this packet.
+ */
+static inline void port_cfg_in_filter(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_LUE_CTRL, PORT_INGRESS_FILTER, set);
+}  /* port_cfg_in_filter */
+
+static inline int port_chk_dis_non_vid(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_LUE_CTRL, PORT_DISCARD_NON_VID);
+}
+
+static inline int port_chk_in_filter(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_LUE_CTRL, PORT_INGRESS_FILTER);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Mirroring */
+
+static inline void port_cfg_mirror_sniffer(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER, set);
+}
+
+static inline void port_cfg_mirror_rx(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX, set);
+}
+
+static inline void port_cfg_mirror_tx(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX, set);
+}
+
+static inline void sw_cfg_mirror_rx_tx(struct ksz_sw *sw, int set)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX, set);
+}
+
+static inline int port_chk_mirror_sniffer(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER);
+}
+
+static inline int port_chk_mirror_rx(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX);
+}
+
+static inline int port_chk_mirror_tx(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX);
+}
+
+static inline int sw_chk_mirror_rx_tx(struct ksz_sw *sw)
+{
+	return sw_chk(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX);
+}
+
+static void sw_setup_mirror(struct ksz_sw *sw)
+{
+	int port;
+
+	/*
+	 * The mirror sniffer port requires it to be in the port membership
+	 * of the receive and transmit ports.
+	 * For example, port 3 is the mirror port of traffic between ports 1
+	 * and 2.  Port 3 needs only to turn sniffer on; its port membership
+	 * can be 0.  Ordinarily the port membership of ports 1 and 2 is 3 for
+	 * just commnunicating with eath other.  It has to be set to 7 to pass
+	 * the frames to port 3.  Only one of the ports needs to turn on
+	 * receive and transmit mirroring.
+	 * The mirror receive and transmit mode requires at least two ports to
+	 * turn on receive and transmit mirroring.
+	 */
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port_cfg_mirror_sniffer(sw, port, 0);
+		port_cfg_mirror_rx(sw, port, 0);
+		port_cfg_mirror_tx(sw, port, 0);
+	}
+	sw_cfg_mirror_rx_tx(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Priority */
+
+static inline void port_cfg_diffserv(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_802_1p(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_vlan_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_VLAN_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_mac_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_MAC_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_acl_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_ACL_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_highest_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_HIGHEST_PRIO, set);
+}
+
+static inline void port_cfg_or_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_OR_PRIO, set);
+}
+
+static inline void port_cfg_replace_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_USER_PRIO_CEILING, set);
+}
+
+static inline void port_cfg_prio(struct ksz_sw *sw, int p, int set)
+{
+	port_w_s(sw, p,
+		REG_PORT_CTRL_0, PORT_QUEUE_SPLIT_ENABLE, 0, set);
+}
+
+static inline int port_chk_diffserv(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_PRIO_ENABLE);
+}
+
+static inline int port_chk_802_1p(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_PRIO_ENABLE);
+}
+
+static inline int port_chk_vlan_prio(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_VLAN_PRIO_ENABLE);
+}
+
+static inline int port_chk_mac_prio(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_MAC_PRIO_ENABLE);
+}
+
+static inline int port_chk_acl_prio(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_ACL_PRIO_ENABLE);
+}
+
+static inline int port_chk_highest_prio(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_HIGHEST_PRIO);
+}
+
+static inline int port_chk_or_prio(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_OR_PRIO);
+}
+
+static inline int port_chk_replace_prio(struct ksz_sw *sw, int p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_USER_PRIO_CEILING);
+}
+
+static inline int port_chk_prio(struct ksz_sw *sw, int p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_CTRL_0, PORT_QUEUE_SPLIT_ENABLE, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Policing */
+
+static void port_cfg_index(struct ksz_sw *sw, int port, int p, int q)
+{
+	u32 data;
+
+	data = (p & MRI_INDEX_P_M) << MRI_INDEX_P_S;
+	data |= (q & MRI_INDEX_Q_M) << MRI_INDEX_Q_S;
+	port_w32(sw, port, REG_PORT_MRI_INDEX__4, data);
+}
+
+static inline void port_cfg_police(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_ENABLE, set);
+}
+
+static inline void port_cfg_color_aware(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_COLOR_NOT_AWARE, !set);
+}
+
+static inline void port_cfg_drop_srp(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_SRP, set);
+}
+
+static inline void port_cfg_color_mark(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_MARK_ENABLE, set);
+}
+
+static inline void port_cfg_color_remap(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_REMAP_ENABLE, set);
+}
+
+static inline void port_cfg_port_based_policing(struct ksz_sw *sw, int p,
+	int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, PORT_BASED_POLICING, set);
+}
+
+static inline void port_cfg_police_drop_all(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_ALL, set);
+}
+
+static inline void port_set_police_packet_type(struct ksz_sw *sw, int p,
+	u32 type)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_PACKET_TYPE_M,
+			POLICE_PACKET_TYPE_S, type);
+}
+
+static inline void port_set_non_dscp_color(struct ksz_sw *sw, int p, u32 color)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, SW_COLOR_M,
+			NON_DSCP_COLOR_S, color);
+}
+
+static inline int port_chk_police(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_ENABLE);
+}
+
+static inline int port_chk_color_aware(struct ksz_sw *sw, int p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_COLOR_NOT_AWARE);
+}
+
+static inline int port_chk_drop_srp(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_SRP);
+}
+
+static inline int port_chk_color_mark(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_MARK_ENABLE);
+}
+
+static inline int port_chk_color_remap(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_REMAP_ENABLE);
+}
+
+static inline int port_chk_port_based_policing(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, PORT_BASED_POLICING);
+}
+
+static inline int port_chk_police_drop_all(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_ALL);
+}
+
+static inline u32 port_get_police_packet_type(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_PACKET_TYPE_M,
+			POLICE_PACKET_TYPE_S);
+}
+
+static inline u32 port_get_non_dscp_color(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, SW_COLOR_M,
+			NON_DSCP_COLOR_S);
+}
+
+static inline u16 port_get_cir(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_CIR_S);
+}
+
+static inline u16 port_get_pir(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_PIR_S);
+}
+
+static inline void port_set_cir(struct ksz_sw *sw, int p, u16 rate)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_CIR_S, rate);
+}
+
+static inline void port_set_pir(struct ksz_sw *sw, int p, u16 rate)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_PIR_S, rate);
+}
+
+static inline u16 port_get_cbs(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_CBS_S);
+}
+
+static inline u16 port_get_pbs(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_PBS_S);
+}
+
+static inline void port_set_cbs(struct ksz_sw *sw, int p, u16 size)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_CBS_S, size);
+}
+
+static inline void port_set_pbs(struct ksz_sw *sw, int p, u16 size)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_PBS_S, size);
+}
+
+static inline u16 port_get_wred_max(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_min(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_multiplier(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S);
+}
+
+static inline u16 port_get_wred_avg_size(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_AVG_QUEUE_SIZE_S);
+}
+
+static inline void port_set_wred_max(struct ksz_sw *sw, int p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_min(struct ksz_sw *sw, int p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_multiplier(struct ksz_sw *sw, int p, u16 val)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S, val);
+}
+
+static inline u16 port_get_wred_q_max(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_q_min(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_q_multiplier(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S);
+}
+
+static inline u16 port_get_wred_q_avg_size(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_AVG_QUEUE_SIZE_S);
+}
+
+static inline void port_set_wred_q_max(struct ksz_sw *sw, int p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_q_min(struct ksz_sw *sw, int p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_q_multiplier(struct ksz_sw *sw, int p, u16 val)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S, val);
+}
+
+static inline void port_cfg_wred_random_drop(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_RANDOM_DROP_ENABLE, set);
+}
+
+static inline void port_cfg_wred_drop_gyr(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_GYR_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_yr(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_YR_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_r(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_R_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_all(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_ALL, set);
+}
+
+static inline int port_chk_wred_random_drop(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_RANDOM_DROP_ENABLE);
+}
+
+static inline int port_chk_wred_drop_gyr(struct ksz_sw *sw, int p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_GYR_DISABLE);
+}
+
+static inline int port_chk_wred_drop_yr(struct ksz_sw *sw, int p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_YR_DISABLE);
+}
+
+static inline int port_chk_wred_drop_r(struct ksz_sw *sw, int p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_R_DISABLE);
+}
+
+static inline int port_chk_wred_drop_all(struct ksz_sw *sw, int p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_ALL);
+}
+
+static u32 port_get_wred_pmon(struct ksz_sw *sw, int p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_PMON_M, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Control */
+
+#ifdef MTI_PREEMPT_ENABLE
+static inline void port_cfg_preempt(struct ksz_sw *sw, int p, int q, int set)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_cfg(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_PREEMPT_ENABLE, set);
+}
+
+static inline int port_chk_preempt(struct ksz_sw *sw, int p, int q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_chk(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_PREEMPT_ENABLE);
+}
+#endif
+
+static inline u8 port_get_schedule_mode(struct ksz_sw *sw, int p, int q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_r_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SCHEDULE_MODE_M,
+			MTI_SCHEDULE_MODE_S);
+}
+
+static inline u8 port_get_shaping(struct ksz_sw *sw, int p, int q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_r_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SHAPING_M,
+			MTI_SHAPING_S);
+}
+
+static inline u8 port_get_tx_ratio(struct ksz_sw *sw, int p, int q)
+{
+	u8 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r(sw, p, REG_PORT_MTI_QUEUE_CTRL_1, &data);
+	return data;
+}
+
+/**
+ * port_set_schedule_mode - configure port rate control
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @q:		The priority queue.
+ * @mode:	The schedule mode to specify strict priority or WRR.
+ *
+ * This routine configures the priority queue rate control of the port.
+ */
+static inline void port_set_schedule_mode(struct ksz_sw *sw, int p, int q,
+	u8 mode)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SCHEDULE_MODE_M,
+			MTI_SCHEDULE_MODE_S, mode);
+}  /* port_set_schedule_mode */
+
+static inline void port_set_shaping(struct ksz_sw *sw, int p, int q,
+	u8 shaping)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SHAPING_M,
+			MTI_SHAPING_S, shaping);
+}
+
+/**
+ * port_set_tx_ratio - configure port rate ratio
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @q:		The priority queue.
+ * @ratio:	The rate ratio.
+ *
+ * This routine configures the priority queue rate ratio of the port.
+ */
+static inline void port_set_tx_ratio(struct ksz_sw *sw, int p, int q, u8 ratio)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w(sw, p, REG_PORT_MTI_QUEUE_CTRL_1, ratio & MTI_TX_RATIO_M);
+}  /* port_set_tx_ratio */
+
+static u16 port_get_hi_water_mark(struct ksz_sw *sw, int p, int q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_2__2, &data);
+	return data;
+}
+
+static u16 port_get_lo_water_mark(struct ksz_sw *sw, int p, int q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_3__2, &data);
+	return data;
+}
+
+static u16 port_get_increment(struct ksz_sw *sw, int p, int q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_4__2, &data);
+	return data;
+}
+
+static u8 port_get_srp(struct ksz_sw *sw, int p)
+{
+	u8 data;
+
+	port_r(sw, p, REG_PORT_CTRL_1, &data);
+	return data & PORT_SRP_ENABLE;
+}
+
+static void port_set_hi_water_mark(struct ksz_sw *sw, int p, int q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_2__2, val);
+}
+
+static void port_set_lo_water_mark(struct ksz_sw *sw, int p, int q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_3__2, val);
+}
+
+static void port_set_increment(struct ksz_sw *sw, int p, int q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_4__2, val);
+}
+
+static void port_set_srp(struct ksz_sw *sw, int p, u8 srp)
+{
+	port_w(sw, p, REG_PORT_CTRL_1, srp & PORT_SRP_ENABLE);
+}
+
+#ifdef KSZ_MRP
+#include "ksz_mrp.c"
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Queue Management */
+
+static inline u8 port_get_qm_drop(struct ksz_sw *sw, int p)
+{
+	return (u8) port_r_s_32(sw, p, REG_PORT_QM_CTRL__4,
+		PORT_QM_DROP_PRIO_M, 0);
+}
+
+static u8 port_get_qm_burst_size(struct ksz_sw *sw, int p)
+{
+	return (u8) port_r_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		3, PORT_QM_BURST_SIZE_S);
+}
+
+static u16 port_get_qm_resv_space(struct ksz_sw *sw, int p)
+{
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PORT_QM_MIN_RESV_SPACE_M, 0);
+}
+
+static u16 port_get_qm_hi_water_mark(struct ksz_sw *sw, int p, int q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_HI_WATER_MARK_S);
+}
+
+static u16 port_get_qm_lo_water_mark(struct ksz_sw *sw, int p, int q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_LO_WATER_MARK_S);
+}
+
+static u16 port_get_qm_tx_used(struct ksz_sw *sw, int p, int q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_0__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_USED_S);
+}
+
+static u16 port_get_qm_tx_avail(struct ksz_sw *sw, int p, int q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_1__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_AVAIL_S);
+}
+
+static u16 port_get_qm_tx_calculated(struct ksz_sw *sw, int p, int q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_1__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_CALCULATED_S);
+}
+
+static inline void port_set_qm_drop(struct ksz_sw *sw, int p, u8 drop)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_CTRL__4,
+		PORT_QM_DROP_PRIO_M, 0, drop);
+}
+
+static inline void port_set_qm_burst_size(struct ksz_sw *sw, int p, u8 burst)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		3, PORT_QM_BURST_SIZE_S, burst);
+}
+
+static inline void port_set_qm_resv_space(struct ksz_sw *sw, int p, u16 space)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PORT_QM_MIN_RESV_SPACE_M, 0, space);
+}
+
+static void port_set_qm_hi_water_mark(struct ksz_sw *sw, int p, int q, u16 val)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	port_w_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_HI_WATER_MARK_S, val);
+}
+
+static void port_set_qm_lo_water_mark(struct ksz_sw *sw, int p, int q, u16 val)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	port_w_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_LO_WATER_MARK_S, val);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_tos_prio - program switch TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the TOS priority into the switch registers.
+ */
+static void sw_set_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	SW_W(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE, prio);
+}  /* sw_set_tos_prio */
+
+/**
+ * sw_dis_diffserv - disable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the DiffServ priority function of the switch.
+ */
+static void sw_dis_diffserv(struct ksz_sw *sw, int port)
+{
+	port_cfg_diffserv(sw, port, 0);
+}  /* sw_dis_diffserv */
+
+/**
+ * sw_ena_diffserv - enable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the DiffServ priority function of the switch.
+ */
+static void sw_ena_diffserv(struct ksz_sw *sw, int port)
+{
+	port_cfg_diffserv(sw, port, 1);
+}  /* sw_ena_diffserv */
+
+/**
+ * hw_cfg_tos_prio - configure TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+	SW_D regmask = KS_PRIO_M;
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	if (prio >= 0x10)
+		regmask = (KS_PRIO_M << KS_PRIO_S) | KS_PRIO_M;
+	shift = (tos & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tos /= KS_PRIO_IN_REG;
+
+	sw->info->diffserv[tos] &= ~mask;
+	sw->info->diffserv[tos] |= prio;
+
+	sw_set_tos_prio(sw, tos, sw->info->diffserv[tos]);
+}  /* hw_cfg_tos_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_802_1p_prio - program switch 802.1p priority
+ * @sw:		The switch instance.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the 802.1p priority into the switch register.
+ */
+static void sw_set_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	SW_W(sw, S_802_1P_PRIO_CTRL + tag / SW_SIZE, prio);
+}  /* sw_set_802_1p_prio */
+
+/**
+ * sw_dis_802_1p - disable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the 802.1p priority function of the switch.
+ */
+static void sw_dis_802_1p(struct ksz_sw *sw, int port)
+{
+	port_cfg_802_1p(sw, port, 0);
+}  /* sw_dis_802_1p */
+
+/**
+ * sw_ena_802_1p - enable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the 802.1p priority function of the switch.
+ */
+static void sw_ena_802_1p(struct ksz_sw *sw, int port)
+{
+	port_cfg_802_1p(sw, port, 1);
+}  /* sw_ena_802_1p */
+
+/**
+ * hw_cfg_802_1p_prio - configure 802.1p priority
+ * @sw:		The switch instance.
+ * @tag:	The 802.1p tag priority value, ranging from 0 to 7.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the 802.1p priority in the hardware.
+ * 802.1p Tag priority value 0 ~ 7 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+	SW_D regmask = KS_PRIO_M;
+
+	if (tag >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	if (prio >= 0x10)
+		regmask = (KS_PRIO_M << KS_PRIO_S) | KS_PRIO_M;
+	shift = (tag & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tag /= KS_PRIO_IN_REG;
+
+	sw->info->p_802_1p[tag] &= ~mask;
+	sw->info->p_802_1p[tag] |= prio;
+
+	sw_set_802_1p_prio(sw, tag, sw->info->p_802_1p[tag]);
+}  /* hw_cfg_802_1p_prio */
+
+/**
+ * sw_cfg_replace_null_vid - enable switch null VID replacement
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the VID to be replaced with port default VID if it is
+ * empty.
+ */
+static void sw_cfg_replace_null_vid(struct ksz_sw *sw, int p, int set)
+{
+	port_cfg32(sw, p, REG_PORT_MTI_QUEUE_CTRL_0__4, MTI_PVID_REPLACE, set);
+}  /* sw_cfg_replace_null_vid */
+
+/**
+ * sw_cfg_replace_prio - enable switch 802.1p priority re-mapping
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the 802.1p priority re-mapping function of the switch.
+ * That allows 802.1p priority field to be replaced with the port's default
+ * tag's priority value if the ingress packet's 802.1p priority has a higher
+ * priority than port's default tag's priority.
+ */
+static void sw_cfg_replace_prio(struct ksz_sw *sw, int port, int set)
+{
+	port_cfg_replace_prio(sw, port, set);
+}  /* sw_cfg_replace_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_cfg_port_based - configure switch port based priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority to set.
+ *
+ * This routine configures the port based priority of the switch.
+ */
+static void sw_cfg_port_based(struct ksz_sw *sw, int port, u8 prio)
+{
+	SW_D data;
+
+	if (prio > PORT_BASED_PRIO_M)
+		prio = PORT_BASED_PRIO_M;
+
+	port_r(sw, port, REG_PORT_MRI_MAC_CTRL, &data);
+	data &= ~(PORT_BASED_PRIO_M << PORT_BASED_PRIO_S);
+	data |= prio << PORT_BASED_PRIO_S;
+	port_w(sw, port, REG_PORT_MRI_MAC_CTRL, data);
+
+	sw->info->port_cfg[port].port_prio = prio;
+}  /* sw_cfg_port_based */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_multi_queue - enable transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the transmit multiple queues selection of the switch
+ * port.  The port transmit queue is split into two or four priority queues.
+ */
+static void sw_set_multi_queue(struct ksz_sw *sw, int port, int queue)
+{
+	port_cfg_prio(sw, port, queue);
+}  /* sw_set_multi_queue */
+
+/**
+ * sw_init_prio - initialize switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the switch QoS priority functions.
+ */
+static void sw_init_prio(struct ksz_sw *sw)
+{
+	int port;
+	int tos;
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+
+	for (tos = 0; tos < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->p_802_1p[tos] =
+			SW_R(sw, S_802_1P_PRIO_CTRL + tos * SW_SIZE);
+
+	for (tos = 0; tos < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->diffserv[tos] =
+			SW_R(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE);
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port_r(sw, port, REG_PORT_MRI_MAC_CTRL, &data);
+		data &= PORT_BASED_PRIO_M;
+		sw->info->port_cfg[port].port_prio = data;
+		port_cfg = &sw->info->port_cfg[port];
+		for (tos = 0; tos < PRIO_802_1P_ENTRIES / 8; tos++)
+			port_r32(sw, port, REG_PORT_MRI_TC_MAP__4 - tos * 4,
+				&port_cfg->tc_map[tos]);
+
+		for (tos = 0; tos < DIFFSERV_ENTRIES / 16; tos++)
+			port_r32(sw, port, REG_PORT_POLICE_COLOR_3__4 -
+				tos * 4, &port_cfg->color_map[tos]);
+	}
+
+#if 0
+#ifdef USE_DIFF_PORT_PRIORITY
+/*
+ * THa  2012/02/01
+ * Port 3 sometimes cannot send frames out through the port where 100%
+ * wire-rate 64 bytes traffic also goes through.  Solution is to assign
+ * different port priorities.  It does not matter port 3 has higher priority,
+ * just different from the port where heavy traffic comes in.
+ */
+	for (port = 0; port < TOTAL_PORT_NUM; port++)
+		sw->info->port_cfg[port].port_prio = port;
+#endif
+#ifdef USE_DIFF_PORT_PRIORITY
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		int prio;
+
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			sw->info->port_cfg[port].rx_rate[prio] = 95000;
+			sw->info->port_cfg[port].tx_rate[prio] = 0;
+		}
+		sw_ena_prio_rate(sw, port);
+	}
+#endif
+#endif
+}  /* sw_init_prio */
+
+static void port_set_color_map(struct ksz_sw *sw, int port, u8 tos, u32 prio)
+{
+	port_w32(sw, port, REG_PORT_POLICE_COLOR_3__4 - tos * 4, prio);
+}  /* port_set_color_map */
+
+static void port_set_tc_map(struct ksz_sw *sw, int port, u8 tos, u32 prio)
+{
+	port_w32(sw, port, REG_PORT_MRI_TC_MAP__4 + tos * 4, prio);
+}  /* port_set_color_map */
+
+/**
+ * port_cfg_color_map - configure port police color map
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void port_cfg_color_map(struct ksz_sw *sw, int port, u8 tos, u32 prio)
+{
+	int shift;
+	u32 data = prio;
+	u32 mask = POLICE_COLOR_MAP_M;
+	struct ksz_port_cfg *port_cfg = &sw->info->port_cfg[port];
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10000)
+		mask = 0xffffffff;
+	else if (prio >= 0x100)
+		mask = 0xffff;
+	else if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > POLICE_COLOR_MAP_M)
+		mask = 0xf;
+	shift = (tos & (16 - 1)) * POLICE_COLOR_MAP_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tos /= 16;
+
+	port_cfg->color_map[tos] &= ~mask;
+	port_cfg->color_map[tos] |= prio;
+
+	port_set_color_map(sw, port, tos, port_cfg->color_map[tos]);
+}  /* port_cfg_color_map */
+
+/**
+ * port_cfg_tc_map - configure port traffic class map
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void port_cfg_tc_map(struct ksz_sw *sw, int port, u8 tos, u32 prio)
+{
+	int shift;
+	u32 data = prio;
+	u32 mask = PORT_TC_MAP_M;
+	u32 regmask = PORT_TC_MAP_M;
+	struct ksz_port_cfg *port_cfg = &sw->info->port_cfg[port];
+
+	if (tos >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10000)
+		mask = 0xffffffff;
+	else if (prio >= 0x100)
+		mask = 0xffff;
+	else if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > PORT_TC_MAP_M)
+		mask = 0xf;
+	if (prio >= 0x10000)
+		regmask = 0x33333333;
+	else if (prio >= 0x100)
+		regmask = 0x3333;
+	else if (prio >= 0x10)
+		regmask = 0x33;
+	shift = (tos & (8 - 1)) * PORT_TC_MAP_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tos /= 8;
+
+	port_cfg->tc_map[tos] &= ~mask;
+	port_cfg->tc_map[tos] |= prio;
+
+	port_set_tc_map(sw, port, tos, port_cfg->tc_map[tos]);
+}  /* port_cfg_tc_map */
+
+/**
+ * sw_setup_prio - setup switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine setup the switch QoS priority functions.
+ */
+static void sw_setup_prio(struct ksz_sw *sw)
+{
+	int port;
+
+	/* All QoS functions disabled. */
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		sw_set_multi_queue(sw, port, 2);
+		sw_dis_diffserv(sw, port);
+		sw_cfg_replace_prio(sw, port, 0);
+		sw_cfg_replace_null_vid(sw, port, 0);
+		sw_cfg_port_based(sw, port, sw->info->port_cfg[port].port_prio);
+
+		sw_ena_802_1p(sw, port);
+	}
+}  /* sw_setup_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_cfg_def_vid - configure port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void port_cfg_def_vid(struct ksz_sw *sw, int port, u16 vid)
+{
+	port_w16(sw, port, REG_PORT_DEFAULT_VID, vid);
+}  /* port_cfg_def_vid */
+
+/**
+ * port_get_def_vid - get port default VID.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	Buffer to store the VID.
+ *
+ * This routine retrieves the default VID of the port.
+ */
+static void port_get_def_vid(struct ksz_sw *sw, int port, u16 *vid)
+{
+	port_r16(sw, port, REG_PORT_DEFAULT_VID, vid);
+}  /* port_get_def_vid */
+
+/**
+ * sw_cfg_def_vid - configure switch port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void sw_cfg_def_vid(struct ksz_sw *sw, int port, u16 vid)
+{
+	sw->info->port_cfg[port].vid = vid;
+	port_cfg_def_vid(sw, port, vid);
+}  /* sw_cfg_def_vid */
+
+/**
+ * sw_cfg_port_base_vlan - configure port-based VLAN membership
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @member:	The port-based VLAN membership.
+ *
+ * This routine configures the port-based VLAN membership of the port.
+ */
+static void sw_cfg_port_base_vlan(struct ksz_sw *sw, int port, u16 member)
+{
+#if 0
+/*
+ * THa  2014/10/08
+ * There are problems in using the VLAN table method for PTP.
+ */
+	struct ksz_vlan_table entry;
+	int p;
+
+	memset(&entry, 0, sizeof(struct ksz_vlan_table));
+	p = 100 + port;
+	sw->reg->unlock(sw);
+	entry.fid = 0;
+	entry.untag = sw->PORT_MASK;
+	entry.ports = member;
+	entry.valid = 1;
+	sw_w_vlan_table(sw, p, &entry);
+	sw->reg->lock(sw);
+
+	sw_cfg_def_vid(sw, port, p);
+#else
+/*
+ * THa  2014/10/08
+ * In KSZ9566 the bit for the last host port is 0x40 instead of 0x20.
+ */
+	if (6 == sw->mib_port_cnt && (member & 0x20))
+		member |= 0x40;
+	port_w32(sw, port, REG_PORT_VLAN_MEMBERSHIP__4, member);
+#endif
+
+	sw->info->port_cfg[port].member = member;
+}  /* sw_cfg_port_base_vlan */
+
+static void sw_cfg_default_vlan(struct ksz_sw *sw, int reset)
+{
+	struct ksz_vlan_table vlan;
+
+	if (sw->overrides & VLAN_SET)
+		return;
+	sw->ops->release(sw);
+	vlan.vid = 1;
+	sw_r_vlan_table(sw, vlan.vid, &vlan);
+	if (reset)
+		vlan.untag = 0;
+	else
+		vlan.untag = vlan.ports;
+	vlan.valid = vlan.ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+	sw->ops->acquire(sw);
+}  /* sw_cfg_default_vlan */
+
+/**
+ * sw_dis_vlan - disable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine disables the VLAN function of the switch.
+ */
+static void sw_dis_vlan(struct ksz_sw *sw)
+{
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE, 0);
+	sw_cfg_default_vlan(sw, true);
+}  /* sw_dis_vlan */
+
+/**
+ * sw_ena_vlan - enable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine enables the VLAN function of the switch.
+ */
+static void sw_ena_vlan(struct ksz_sw *sw)
+{
+	sw_cfg_default_vlan(sw, false);
+
+	/* Enable 802.1q VLAN mode. */
+	sw_cfg(sw, REG_SW_QM_CTRL, UNICAST_VLAN_BOUNDARY, 1);
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE, 1);
+}  /* sw_ena_vlan */
+
+/**
+ * sw_init_vlan - initialize switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the VLAN function of the switch.
+ */
+static void sw_init_vlan(struct ksz_sw *sw)
+{
+	int port;
+	u32 data;
+	struct ksz_sw_info *info = sw->info;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port_get_def_vid(sw, port, &info->port_cfg[port].vid);
+		port_r32(sw, port, REG_PORT_VLAN_MEMBERSHIP__4, &data);
+		info->port_cfg[port].member = (u16) data;
+	}
+}  /* sw_init_vlan */
+
+/**
+ * sw_get_addr - get the switch MAC address.
+ * @sw:		The switch instance.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the switch.
+ */
+static inline void sw_get_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	sw->reg->r(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_get_addr */
+
+/**
+ * sw_set_addr - configure switch MAC address
+ * @sw:		The switch instance.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the switch.
+ */
+static void sw_set_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	sw->reg->w(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+#ifdef KSZ_IBA
+	prepare_iba(&sw->info->iba, mac_addr, sw->info->iba.src);
+#endif
+}  /* sw_set_addr */
+
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+/**
+ * sw_init_phy_addr - initialize switch PHY address
+ * @sw:		The switch instance.
+ *
+ * This function initializes the PHY address of the switch.
+ */
+static void sw_init_phy_addr(struct ksz_sw *sw)
+{
+	u8 addr;
+
+	addr = sw->reg->r8(sw, REG_SWITCH_CTRL_13);
+	addr >>= SWITCH_PORT_PHY_ADDR_SHIFT;
+	addr &= SWITCH_PORT_PHY_ADDR_MASK;
+	sw->info->phy_addr = addr;
+}
+
+/**
+ * sw_set_phy_addr - configure switch PHY address
+ * @sw:		The switch instance.
+ * @addr:	The PHY address.
+ *
+ * This function configures the PHY address of the switch.
+ */
+static void sw_set_phy_addr(struct ksz_sw *sw, u8 addr)
+{
+	sw->info->phy_addr = addr;
+	addr &= SWITCH_PORT_PHY_ADDR_MASK;
+	addr <<= SWITCH_PORT_PHY_ADDR_SHIFT;
+	sw->reg->w8(sw, REG_SWITCH_CTRL_13, addr);
+}
+#endif
+
+static void sw_setup_reserved_multicast(struct ksz_sw *sw)
+{
+	struct ksz_mac_table table[8];
+	u16 addr[8];
+	u32 ctrl[8];
+	int i;
+
+	memset(table, 0, sizeof(struct ksz_mac_table) * 8);
+	for (i = 0; i < 8; i++)
+		addr[i] = get_mcast_reserved_addr(i);
+
+	/* Register access is locked before this call. */
+	sw->ops->release(sw);
+	sw_r_m_sta_mac_table(sw, addr, 1, 8, table);
+	if (table[0].ports != sw->HOST_MASK) {
+		for (i = 0; i < 8; i++)
+			ctrl[i] = get_mac_table_ctrl(addr[i], true);
+		table[0].ports = sw->HOST_MASK;
+		table[1].ports = 0;
+		table[2].ports = sw->HOST_MASK;
+		table[3].ports = sw->PORT_MASK;
+		table[4].ports = sw->PORT_MASK & ~sw->HOST_MASK;
+		table[5].ports = sw->PORT_MASK & ~sw->HOST_MASK;
+		table[6].ports = sw->HOST_MASK;
+		table[7].ports = sw->PORT_MASK & ~sw->HOST_MASK;
+		sw_w_m_sta_mac_table(sw, addr, true, 8, table);
+	}
+	sw->ops->acquire(sw);
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_RESV_MCAST_ENABLE, 1);
+}  /* sw_setup_reserved_multicast */
+
+static int sw_get_gbit(struct ksz_sw *sw, u8 data)
+{
+	int gbit;
+
+	if (sw->features & NEW_CAP)
+		gbit = !(data & PORT_MII_NOT_1GBIT);
+	else
+		gbit = data & PORT_MII_1000MBIT_S1;
+	return gbit;
+}  /* sw_get_gbit */
+
+static void sw_set_gbit(struct ksz_sw *sw, int gbit, u8 *data)
+{
+	if (sw->features & NEW_CAP) {
+		if (gbit)
+			*data &= ~PORT_MII_NOT_1GBIT;
+		else
+			*data |= PORT_MII_NOT_1GBIT;
+	} else {
+		if (gbit)
+			*data |= PORT_MII_1000MBIT_S1;
+		else
+			*data &= ~PORT_MII_1000MBIT_S1;
+	}
+}  /* sw_set_gbit */
+
+static int sw_get_xmii(struct ksz_sw *sw, u8 data)
+{
+	int mode;
+
+	if (sw->features & NEW_CAP) {
+		switch (data & PORT_MII_SEL_M) {
+		case PORT_MII_SEL:
+			mode = 0;
+			break;
+		case PORT_RMII_SEL:
+			mode = 1;
+			break;
+		case PORT_GMII_SEL:
+			mode = 2;
+			break;
+		default:
+			mode = 3;
+		}
+	} else {
+		switch (data & PORT_MII_SEL_M) {
+		case PORT_MII_SEL_S1:
+			mode = 0;
+			break;
+		case PORT_RMII_SEL_S1:
+			mode = 1;
+			break;
+		case PORT_GMII_SEL_S1:
+			mode = 2;
+			break;
+		default:
+			mode = 3;
+		}
+	}
+	return mode;
+}  /* sw_get_xmii */
+
+static void sw_set_xmii(struct ksz_sw *sw, int mode, u8 *data)
+{
+	u8 xmii;
+
+	if (sw->features & NEW_CAP) {
+		switch (mode) {
+		case 0:
+			xmii = PORT_MII_SEL;
+			break;
+		case 1:
+			xmii = PORT_RMII_SEL;
+			break;
+		case 2:
+			xmii = PORT_GMII_SEL;
+			break;
+		default:
+			xmii = PORT_RGMII_SEL;
+		}
+	} else {
+		switch (mode) {
+		case 0:
+			xmii = PORT_MII_SEL_S1;
+			break;
+		case 1:
+			xmii = PORT_RMII_SEL_S1;
+			break;
+		case 2:
+			xmii = PORT_GMII_SEL_S1;
+			break;
+		default:
+			xmii = PORT_RGMII_SEL_S1;
+		}
+	}
+	*data &= ~PORT_MII_SEL_M;
+	*data |= xmii;
+}  /* sw_set_xmii */
+
+/**
+ * sw_set_global_ctrl - set switch global control
+ * @sw:		The switch instance.
+ *
+ * This routine sets the global control of the switch function.
+ */
+static void sw_set_global_ctrl(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	if (sw->HOST_PORT >= sw->phy_port_cnt) {
+		struct phy_device *phydev = sw->phydev;
+		struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+		int gbit;
+		int mode;
+
+		/* Allow slower speed to be used for testing purpose. */
+#ifdef USE_10_MBIT_MODE
+		phydev->speed = SPEED_10;
+		phydev->dev_flags |= 1;
+#endif
+#ifdef USE_HALF_DUPLEX
+		phydev->duplex = DUPLEX_HALF;
+		phydev->dev_flags |= 1;
+#endif
+#if defined(USE_MII_PHY) || defined(USE_RGMII_PHY)
+		phydev->dev_flags |= 2;
+#endif
+		if ((sw->features & NO_GLOBAL_RESET) ||
+		    (phydev->dev_flags & 1)) {
+			port_r(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_0, &data);
+			data |= PORT_MII_100MBIT;
+			data |= PORT_MII_FULL_DUPLEX;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_10 == phydev->speed)
+					data &= ~PORT_MII_100MBIT;
+				if (DUPLEX_HALF == phydev->duplex)
+					data &= ~PORT_MII_FULL_DUPLEX;
+			}
+			if ((data & PORT_MII_100MBIT) &&
+			    phydev->speed < SPEED_100)
+				phydev->speed = SPEED_100;
+			if ((data & PORT_MII_FULL_DUPLEX) &&
+			    phydev->duplex < DUPLEX_FULL)
+				phydev->duplex = DUPLEX_FULL;
+			port_w(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_0, data);
+		}
+
+		port_r(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_1, &data);
+		data &= ~PORT_MII_MAC_MODE;
+		if (phydev->dev_flags & 2)
+			data |= PORT_MII_MAC_MODE;
+		switch (phydev->interface) {
+		case PHY_INTERFACE_MODE_MII:
+			sw_set_gbit(sw, false, &data);
+			mode = 0;
+			if (phydev->speed > SPEED_100)
+				phydev->speed = SPEED_100;
+			break;
+		case PHY_INTERFACE_MODE_RMII:
+			sw_set_gbit(sw, false, &data);
+			mode = 1;
+			break;
+		case PHY_INTERFACE_MODE_GMII:
+			sw_set_gbit(sw, true, &data);
+			mode = 2;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_1000 != phydev->speed)
+					sw_set_gbit(sw, false, &data);
+			}
+			gbit = sw_get_gbit(sw, data);
+			if (gbit && phydev->speed < SPEED_1000)
+				phydev->speed = SPEED_1000;
+			break;
+		case PHY_INTERFACE_MODE_SGMII:
+			mode = 3;
+			break;
+		default:
+			data &= ~PORT_RGMII_ID_IG_ENABLE;
+			data &= ~PORT_RGMII_ID_EG_ENABLE;
+			if (PHY_INTERFACE_MODE_RGMII_ID == phydev->interface ||
+			    PHY_INTERFACE_MODE_RGMII_RXID == phydev->interface)
+				data |= PORT_RGMII_ID_IG_ENABLE;
+			if (PHY_INTERFACE_MODE_RGMII_ID == phydev->interface ||
+			    PHY_INTERFACE_MODE_RGMII_TXID == phydev->interface)
+				data |= PORT_RGMII_ID_EG_ENABLE;
+			sw_set_gbit(sw, true, &data);
+			mode = 3;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_1000 != phydev->speed)
+					sw_set_gbit(sw, false, &data);
+			}
+			gbit = sw_get_gbit(sw, data);
+			if (gbit && phydev->speed < SPEED_1000)
+				phydev->speed = SPEED_1000;
+			break;
+		}
+		info->tx_rate = phydev->speed * TX_RATE_UNIT;
+		info->duplex = phydev->duplex + 1;
+#ifdef USE_RGMII_PHY
+		data |= PORT_RGMII_ID_EG_ENABLE;
+		mode = 3;
+#endif
+		sw_set_xmii(sw, mode, &data);
+		port_w(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_1, data);
+	}
+
+	data = SW_R(sw, REG_SW_MAC_CTRL_0);
+
+	/* Enable aggressive back off algorithm in half duplex mode. */
+	data |= SW_AGGR_BACKOFF;
+	SW_W(sw, REG_SW_MAC_CTRL_0, data);
+
+	data = SW_R(sw, REG_SW_MAC_CTRL_1);
+
+	/* Enable no excessive collision drop. */
+	data |= NO_EXC_COLLISION_DROP;
+	SW_W(sw, REG_SW_MAC_CTRL_1, data);
+
+	data = SW_R(sw, S_LINK_AGING_CTRL);
+
+	data |= SW_AGING_ENABLE;
+	if (sw->overrides & FAST_AGING)
+		data |= SW_FAST_AGING;
+	else
+		data &= ~SW_FAST_AGING;
+
+	/* Enable automatic fast aging when link changed detected. */
+	data |= SW_LINK_AUTO_AGING;
+
+#if 1
+/*
+ * THa  2014/10/08
+ * The host port also gets filtered if lookup is used!
+ */
+	if (sw->features & NEW_CAP)
+		data |= SW_SRC_ADDR_FILTER;
+#endif
+	SW_W(sw, S_LINK_AGING_CTRL, data);
+
+	data = SW_R(sw, REG_SW_QM_CTRL);
+
+	/* Make sure unicast VLAN boundary is set as default. */
+	if (sw->dev_count > 1)
+		data |= UNICAST_VLAN_BOUNDARY;
+	SW_W(sw, REG_SW_QM_CTRL, data);
+
+#if 1
+/*
+ * THa  2015/12/06
+ * The HSR register 0x640 needs to be set, even though the value read is 3.
+ * The NODE_UNICAST bit in register 0x644 needs to be turned off for multicast
+ * address to work.
+ * If HSR_LEARN_UCAST_DISABLE bit in register 0x645 is turned on, multicast
+ * address also does not work, even though HSR_LEARN_MCAST_DISABLE bit in
+ * register 0x644 can be used to control that.
+ */
+	if (sw->features & HSR_HW) {
+		sw->reg->w32(sw, REG_HSR_PORT_MAP__4, 3);
+		data = SW_R(sw, REG_HSR_ALU_CTRL_0__1);
+		data &= ~HSR_NODE_UNICAST;
+		SW_W(sw, REG_HSR_ALU_CTRL_0__1, data);
+	}
+#endif
+}  /* sw_set_global_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_set_stp_state - configure port spanning tree state
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @state:	The spanning tree state.
+ *
+ * This routine configures the spanning tree state of the port.
+ */
+static void port_set_stp_state(struct ksz_sw *sw, int port, int state)
+{
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+	int member = -1;
+
+	port_cfg = &sw->info->port_cfg[port];
+	port_r(sw, port, P_STP_CTRL, &data);
+	switch (state) {
+	case STP_STATE_DISABLED:
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT)
+			member = 0;
+		break;
+	case STP_STATE_LISTENING:
+/*
+ * No need to turn on transmit because of port direct mode.
+ * Turning on receive is required if static MAC table is not setup.
+ */
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | (1 << port);
+		break;
+	case STP_STATE_LEARNING:
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_FORWARDING:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_BLOCKED:
+/*
+ * Need to setup static MAC table with override to keep receiving BPDU
+ * messages.  See sw_setup_stp routine.
+ */
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | (1 << port);
+		break;
+	case STP_STATE_SIMPLE:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | (1 << port);
+		break;
+	}
+	port_w(sw, port, P_STP_CTRL, data);
+	port_cfg->stp_state = state;
+	if (data & PORT_RX_ENABLE)
+		sw->rx_ports |= (1 << port);
+	else
+		sw->rx_ports &= ~(1 << port);
+	if (data & PORT_TX_ENABLE)
+		sw->tx_ports |= (1 << port);
+	else
+		sw->tx_ports &= ~(1 << port);
+
+	/* Port membership may share register with STP state. */
+	if (member >= 0)
+		sw_cfg_port_base_vlan(sw, port, (u8) member);
+}  /* port_set_stp_state */
+
+#define STP_ENTRY			0
+#define BROADCAST_ENTRY			1
+#define BRIDGE_ADDR_ENTRY		2
+#define IPV6_ADDR_ENTRY			3
+
+/**
+ * sw_clr_sta_mac_table - clear static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine clears the static MAC table.
+ */
+static void sw_clr_sta_mac_table(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	data = SW_R(sw, REG_SW_LUE_CTRL_2);
+	data &= ~(SW_FLUSH_OPTION_M << SW_FLUSH_OPTION_S);
+	data |= (SW_FLUSH_OPTION_STA_MAC << SW_FLUSH_OPTION_S);
+	SW_W(sw, REG_SW_LUE_CTRL_2, data);
+	sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_STP_TABLE, 1);
+}  /* sw_clr_sta_mac_table */
+
+/**
+ * sw_setup_stp - setup switch spanning tree support
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the spanning tree support of the switch.
+ */
+static void sw_setup_stp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct ksz_sw_info *info = sw->info;
+
+	entry = &info->mac_table[STP_ENTRY];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x80;
+	entry->addr[2] = 0xC2;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x00;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 1;
+	entry->valid = 1;
+	alu = &info->alu_table[STP_ENTRY];
+	alu->forward = FWD_STP_DEV | FWD_HOST | FWD_HOST_OVERRIDE;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+	sw->ops->release(sw);
+	sw_w_sta_mac_table(sw, alu->index, alu->type, entry);
+	sw->ops->acquire(sw);
+
+	entry = &info->mac_table[BROADCAST_ENTRY];
+	memset(entry->addr, 0xFF, ETH_ALEN);
+	entry->ports = sw->HOST_MASK;
+	entry->override = 0;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[BROADCAST_ENTRY];
+	alu->forward = FWD_MAIN_DEV | FWD_STP_DEV | FWD_HOST;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = BROADCAST_ENTRY - 1;
+	alu->type = 0;
+
+	entry = &info->mac_table[BRIDGE_ADDR_ENTRY];
+	memcpy(entry->addr, info->mac_addr, ETH_ALEN);
+	entry->ports = sw->HOST_MASK;
+	entry->override = 0;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[BRIDGE_ADDR_ENTRY];
+	alu->forward = FWD_MAIN_DEV | FWD_HOST;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = BRIDGE_ADDR_ENTRY - 1;
+	alu->type = 0;
+
+	entry = &info->mac_table[IPV6_ADDR_ENTRY];
+	memcpy(entry->addr, info->mac_addr, ETH_ALEN);
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0xFF;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 0;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[IPV6_ADDR_ENTRY];
+	alu->forward = FWD_MAIN_DEV | FWD_STP_DEV | FWD_HOST;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = IPV6_ADDR_ENTRY - 1;
+	alu->type = 0;
+}  /* sw_setup_stp */
+
+#ifdef CONFIG_KSZ_STP
+/**
+ * sw_block_addr - block certain packets from the host port
+ * @sw:		The switch instance.
+ *
+ * This routine blocks certain packets from reaching to the host port.
+ */
+static void sw_block_addr(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw_info *info = sw->info;
+
+	for (i = BROADCAST_ENTRY; i <= IPV6_ADDR_ENTRY; i++) {
+		entry = &info->mac_table[i];
+		entry->valid = 0;
+		alu = &info->alu_table[i];
+		sw_w_sta_mac_table(sw, alu->index, alu->type, entry);
+	}
+}  /* sw_block_addr */
+
+static void sw_block_multi(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw_info *info = sw->info;
+
+	for (i = STATIC_MAC_TABLE_ENTRIES; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &info->mac_table[i];
+		if (entry->ports) {
+			entry->valid = 0;
+			alu = &info->alu_table[i];
+			if (2 == alu->type)
+				sw_w_dyn_mac_table(sw, alu->index,
+					entry->addr, entry->fid, entry);
+			else if (alu->type < 2)
+				sw_w_sta_mac_table(sw, alu->index, alu->type,
+					entry);
+		}
+	}
+}  /* sw_block_multi */
+
+#ifdef CONFIG_1588_PTP
+static void sw_setup_ptp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	u8 forward;
+	struct ksz_sw_info *info = sw->info;
+
+	i = info->multi_sys;
+	forward = FWD_MAIN_DEV;
+	forward |= FWD_VLAN_DEV;
+
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x01;
+	entry->addr[5] = 0x81;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x6B;
+	entry->ports = sw->PORT_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x01;
+	entry->addr[5] = 0x81;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x6B;
+	entry->ports = sw->PORT_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x1B;
+	entry->addr[2] = 0x19;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x00;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x80;
+	entry->addr[2] = 0xC2;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x0E;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0xE;
+	alu->type = 1;
+
+	info->multi_sys = i;
+}
+#endif
+
+static void sw_setup_multi(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	u8 forward;
+	struct ksz_sw_info *info = sw->info;
+
+	i = MULTI_MAC_TABLE_ENTRIES;
+	forward = FWD_STP_DEV;
+	forward |= FWD_MAIN_DEV;
+
+	/* Used for V2 IGMP messages. */
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x01;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x01;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x02;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x02;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+
+	/* Used for V3 IGMP messages. */
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x16;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x16;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+
+	info->multi_sys = i;
+}
+
+static void bridge_change(struct ksz_sw *sw)
+{
+	int port;
+	u8 member;
+	struct ksz_sw_info *info = sw->info;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (port == sw->HOST_PORT)
+			continue;
+		if (STP_STATE_FORWARDING == info->port_cfg[port].stp_state)
+			member = sw->HOST_MASK | info->member;
+		else if (STP_STATE_DISABLED == info->port_cfg[port].stp_state)
+			member = 0;
+		else
+			member = sw->HOST_MASK | (1 << port);
+		if (member != info->port_cfg[port].member)
+			sw_cfg_port_base_vlan(sw, port, member);
+	}
+}  /* bridge_change */
+
+/**
+ * sw_pass_addr - allow certain packets to the host port
+ * @sw:		The switch instance.
+ *
+ * This routine allows certain packets to reach the host port.
+ */
+static void sw_pass_addr(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw_info *info = sw->info;
+
+	for (i = BROADCAST_ENTRY; i <= IPV6_ADDR_ENTRY; i++) {
+		entry = &info->mac_table[i];
+		switch (i) {
+		case BROADCAST_ENTRY:
+			memset(entry->addr, 0xFF, ETH_ALEN);
+			break;
+		case BRIDGE_ADDR_ENTRY:
+			memcpy(entry->addr, info->br_addr, ETH_ALEN);
+			break;
+		case IPV6_ADDR_ENTRY:
+			memcpy(entry->addr, info->br_addr, ETH_ALEN);
+			entry->addr[0] = 0x33;
+			entry->addr[1] = 0x33;
+			entry->addr[2] = 0xFF;
+			break;
+		}
+		entry->ports = sw->HOST_MASK;
+		entry->valid = 1;
+		alu = &info->alu_table[i];
+		sw_w_sta_mac_table(sw, alu->index, alu->type, entry);
+	}
+}  /* sw_pass_addr */
+
+static void sw_pass_multi(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw_info *info = sw->info;
+
+	for (i = STATIC_MAC_TABLE_ENTRIES; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &info->mac_table[i];
+		if (entry->ports) {
+			entry->valid = 1;
+			alu = &info->alu_table[i];
+			if (2 == alu->type)
+				sw_w_dyn_mac_table(sw, alu->index,
+					entry->addr, entry->fid, entry);
+			else if (alu->type < 2)
+				sw_w_sta_mac_table(sw, alu->index, alu->type,
+					entry);
+		}
+	}
+}  /* sw_pass_multi */
+
+static void monitor_ports(struct ksz_sw *sw)
+{
+	int port;
+	struct net_device *bridge_dev = NULL;
+	struct ksz_sw_info *info = sw->info;
+	u8 member = info->member;
+	u8 stp = info->stp;
+	u8 prev_stp = info->stp;
+	u8 stp_down = 0;
+	u8 state;
+	u8 forwarding[SWITCH_PORT_NUM];
+
+	memset(forwarding, 0, SWITCH_PORT_NUM);
+	sw->ops->acquire(sw);
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		int index = port;
+		struct net_device *dev;
+
+		if (port == sw->HOST_PORT)
+			continue;
+		if (port > sw->HOST_PORT)
+			--index;
+		dev = sw->netdev[index + sw->dev_offset];
+		state = sw->net_ops->get_port_state(dev, &bridge_dev);
+		if (state != STP_STATE_SIMPLE) {
+			stp |= (1 << port);
+			if (STP_STATE_DISABLED == state)
+				stp_down |= (1 << port);
+		} else {
+			stp &= ~(1 << port);
+			state = sw->net_ops->get_state(dev);
+		}
+		if (stp != info->stp) {
+			info->stp = stp;
+
+			/* Device just removed from bridge. */
+			if (!(stp & (1 << port))) {
+				if (netif_running(dev))
+					state = STP_STATE_SIMPLE;
+			}
+		}
+		sw->net_ops->set_state(dev, state);
+
+		if (info->port_cfg[port].stp_state != state) {
+			if (STP_STATE_FORWARDING ==
+					info->port_cfg[port].stp_state)
+				member &= ~(1 << port);
+			if (STP_STATE_FORWARDING == state)
+				member |= (1 << port);
+
+			/* Try to set forwarding after the other states. */
+			if (STP_STATE_FORWARDING == state)
+				forwarding[port] = true;
+			else
+				port_set_stp_state(sw, port, state);
+			if (STP_STATE_LEARNING == state ||
+			    STP_STATE_BLOCKED == state)
+				sw_flush_dyn_mac_table(sw, port);
+		}
+	}
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (port == sw->HOST_PORT)
+			continue;
+		if (forwarding[port])
+			port_set_stp_state(sw, port, STP_STATE_FORWARDING);
+	}
+	sw->ops->release(sw);
+	if (prev_stp != info->stp && !info->stp)
+		memset(info->br_addr, 0, ETH_ALEN);
+	if (stp_down != info->stp_down || prev_stp != info->stp) {
+		struct ksz_mac_table *entry = &sw->info->mac_table[0];
+		struct ksz_alu_table *alu = &sw->info->alu_table[0];
+		int valid = entry->valid;
+
+		if (stp_down == info->stp) {
+
+			/* Turn off STP only when it is already setup. */
+			if (prev_stp == info->stp)
+				entry->valid = 0;
+		} else if (info->stp_down == info->stp ||
+				(!prev_stp && info->stp))
+			entry->valid = 1;
+		if (valid != entry->valid) {
+
+			/* Cannot really disable the entry. */
+			if (entry->valid)
+				entry->override = 1;
+			else
+				entry->override = 0;
+			sw_w_sta_mac_table(sw, alu->index, alu->type, entry);
+
+			/* No ports in forwarding state. */
+			if (!entry->valid) {
+				sw->ops->acquire(sw);
+				port_set_stp_state(sw, SWITCH_PORT_NUM,
+					STP_STATE_SIMPLE);
+				sw->ops->release(sw);
+				sw_block_addr(sw);
+				sw_block_multi(sw);
+			}
+		}
+
+		/* Update disabled ports when STP is settled down. */
+		if (prev_stp == info->stp)
+			info->stp_down = stp_down;
+	}
+
+	if (member != info->member) {
+		int cnt = 0;
+
+		for (port = 0; port < sw->mib_port_cnt; port++) {
+			if (port == sw->HOST_PORT)
+				continue;
+			if (member & (1 << port))
+				cnt++;
+		}
+		info->fwd_ports = cnt;
+
+		/* Have first member. */
+		if (!info->member) {
+
+			/* Force to program bridge address. */
+			info->br_addr[0] = 0xFF;
+		}
+		info->member = member;
+		sw->ops->acquire(sw);
+		bridge_change(sw);
+		sw->ops->release(sw);
+	}
+
+	/* At least one port in forwarding state. */
+	if (info->member && bridge_dev && memcmp(bridge_dev->dev_addr,
+			info->br_addr, ETH_ALEN)) {
+		memcpy(info->br_addr, bridge_dev->dev_addr, ETH_ALEN);
+		sw_pass_addr(sw);
+		sw_pass_multi(sw);
+	}
+}  /* monitor_ports */
+#endif
+
+#ifdef KSZ_DLR
+#include "ksz_dlr.c"
+#endif
+
+/*
+ * Link detection routines
+ */
+
+static inline void dbp_link(struct ksz_port *port, struct ksz_sw *sw,
+	int change)
+{
+	struct ksz_port_info *info;
+	int i;
+	int p;
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+
+		/* Port does not have PHY. */
+		if (p >= sw->phy_port_cnt) {
+			break;
+		}
+
+		info = &sw->port_info[p];
+		if (media_connected == info->state) {
+			if (change & (1 << i)) {
+				printk(KERN_INFO "link %d-%d: %d, %d\n",
+					sw->id, i + port->first_port,
+					info->tx_rate / TX_RATE_UNIT,
+					info->duplex);
+			}
+		} else {
+			if (change & (1 << i))
+				printk(KERN_INFO "link %d-%d disconnected\n",
+					sw->id, i + port->first_port);
+		}
+	}
+}
+
+static u16 port_advertised_flow_ctrl(struct ksz_port *port, u16 ctrl)
+{
+	ctrl &= ~PORT_AUTO_NEG_PAUSE;
+	switch (port->flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		ctrl |= PORT_AUTO_NEG_SYM_PAUSE;
+		break;
+	case PHY_TX_ONLY:
+		ctrl |= PORT_AUTO_NEG_ASYM_PAUSE;
+		break;
+	case PHY_RX_ONLY:
+		ctrl |= PORT_AUTO_NEG_PAUSE;
+		break;
+	default:
+		break;
+	}
+	return ctrl;
+}  /* port_advertised_flow_ctrl */
+
+static u8 sw_determine_flow_ctrl(struct ksz_sw *sw, struct ksz_port *port,
+	u16 local, u16 remote)
+{
+	int rx;
+	int tx;
+	u8 flow = 0;
+
+	if (sw->overrides & PAUSE_FLOW_CTRL)
+		return flow;
+
+	rx = tx = 0;
+	if (port->force_link)
+		rx = tx = 1;
+	if (remote & PORT_REMOTE_SYM_PAUSE) {
+		if (local & PORT_AUTO_NEG_SYM_PAUSE)
+			rx = tx = 1;
+		else if ((remote & PORT_AUTO_NEG_ASYM_PAUSE) &&
+			 (local & PORT_AUTO_NEG_PAUSE) ==
+			 PORT_AUTO_NEG_ASYM_PAUSE)
+			tx = 1;
+	} else if (remote & PORT_AUTO_NEG_ASYM_PAUSE) {
+		if ((local & PORT_AUTO_NEG_PAUSE) == PORT_AUTO_NEG_PAUSE)
+			rx = 1;
+	}
+	if (rx)
+		flow |= 0x01;
+	if (tx)
+		flow |= 0x02;
+#ifdef DEBUG
+	printk(KERN_INFO "pause: %d, %d; %04x %04x\n",
+		rx, tx, local, remote);
+#endif
+	return flow;
+}  /* sw_determine_flow_ctrl */
+
+/**
+ * port_get_link_speed - get current link status
+ * @port:	The port instance.
+ *
+ * This routine reads PHY registers to determine the current link status of the
+ * switch ports.
+ */
+static int port_get_link_speed(struct ksz_port *port)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *linked = NULL;
+	struct ksz_port_state *state;
+	struct ksz_sw *sw = port->sw;
+	u16 data;
+	u16 link;
+	u16 dbg_link;
+	u16 status;
+	u32 local;
+	u32 remote;
+	int i;
+	int p;
+	int change = 0;
+
+	/*
+	 * Only check port which has interrupts triggered.
+	 * If no interrupt poll all the ports with PHY.
+	 */
+	if (!sw->phy_intr)
+		sw->phy_intr = sw->PORT_MASK;
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+
+		/* Port does not have PHY. */
+		if (p >= sw->phy_port_cnt)
+			break;
+
+		if (!(sw->phy_intr & (1 << p)))
+			continue;
+
+		info = &sw->port_info[p];
+		state = &sw->port_state[p];
+
+#ifndef NO_PHY_READ
+		port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &data);
+		local = data;
+		local <<= 16;
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &data);
+		local |= data;
+		port_r16(sw, p, REG_PORT_PHY_1000_STATUS, &data);
+		remote = data;
+		remote &= PORT_PHY_1000_STATIC_STATUS;
+		remote <<= 16;
+		port_r16(sw, p, REG_PORT_PHY_REMOTE_CAPABILITY, &data);
+		remote |= data;
+		port_r16(sw, p, P_LINK_STATUS, &link);
+
+		/* Read second time in case the status is not latched. */
+		if (!(link & PORT_LINK_STATUS))
+			port_r16(sw, p, P_LINK_STATUS, &link);
+
+		/* SPI has problem reading the link status from the PHY. */
+		dbg_link = link;
+		port_r16(sw, p, P_SPEED_STATUS, &status);
+		if (!(sw->features & NEW_CAP) &&
+		    ((link & PORT_AUTO_NEG_ACKNOWLEDGE) && (status &
+		    (PORT_STAT_SPEED_1000MBIT |
+		     PORT_STAT_SPEED_100MBIT |
+		     PORT_STAT_SPEED_10MBIT))))
+			link |= PORT_LINK_STATUS;
+#else
+		link = PORT_LINK_STATUS;
+		dbg_link = link;
+		local = 0x10001000;
+		remote = 0x10001000;
+#endif
+		/*
+		 * The partner capability register is updated but the
+		 * auto-negotiation is not completed yet.
+		 */
+		link &= (PORT_AUTO_NEG_ACKNOWLEDGE | PORT_LINK_STATUS);
+
+		if (link & PORT_LINK_STATUS) {
+
+			/* Remember the first linked port. */
+#if 1
+			if (!linked && p != sw->HOST_PORT)
+#else
+			if (!linked)
+#endif
+				linked = info;
+		}
+
+		/* No change to status. */
+		if (local == info->advertised && (u8) link == info->link)
+			continue;
+
+		if (!(dbg_link & PORT_LINK_STATUS) &&
+		    dbg_link & PORT_AUTO_NEG_ACKNOWLEDGE)
+dbg_msg(" link? %d=%04x\n", p, dbg_link);
+#ifdef DEBUG
+		printk(KERN_INFO
+			"%d=advertised: %08X-%08X; partner: %08X-%08X\n", p,
+			local, info->advertised, remote, info->partner);
+#endif
+		info->report = true;
+		info->advertised = local;
+		info->partner = remote;
+		info->link = (u8) link;
+		if (link & PORT_LINK_STATUS) {
+#ifdef NO_PHY_READ
+			status = PORT_STAT_SPEED_100MBIT |
+				PORT_STAT_FULL_DUPLEX;
+#endif
+			info->tx_rate = 10 * TX_RATE_UNIT;
+			if (status & PORT_STAT_SPEED_100MBIT)
+				info->tx_rate = 100 * TX_RATE_UNIT;
+			else if (status & PORT_STAT_SPEED_1000MBIT)
+				info->tx_rate = 1000 * TX_RATE_UNIT;
+
+			info->duplex = 1;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				info->duplex = 2;
+
+			if (media_connected != info->state) {
+#ifndef NO_PHY_READ
+				SW_D flow_ctrl;
+
+				port_r(sw, p, REG_PORT_STATUS_0, &flow_ctrl);
+#ifdef DEBUG
+				printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR"\n",
+					flow_ctrl & (PORT_RX_FLOW_CTRL |
+					PORT_TX_FLOW_CTRL));
+#endif
+				info->flow_ctrl = sw_determine_flow_ctrl(sw,
+					port, local, remote);
+				if (flow_ctrl & PORT_RX_FLOW_CTRL)
+					info->flow_ctrl |= 0x10;
+				if (flow_ctrl & PORT_TX_FLOW_CTRL)
+					info->flow_ctrl |= 0x20;
+				if (sw->info)
+					port_cfg_back_pressure(sw, p,
+						(1 == info->duplex));
+#endif
+				change |= 1 << i;
+			}
+			info->state = media_connected;
+			state->tx_rate = info->tx_rate;
+		} else {
+			if (media_disconnected != info->state) {
+				change |= 1 << i;
+
+				/* Indicate the link just goes down. */
+				state->link_down = 1;
+			}
+			info->state = media_disconnected;
+		}
+#ifdef CONFIG_1588_PTP
+		if (sw->features & PTP_HW) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			if (media_disconnected == info->state)
+				ptp->linked[p] = 0;
+			else {
+				ptp->linked[p] = info->tx_rate / TX_RATE_UNIT;
+				ptp->linked[p] |= 0x80000000;
+			}
+		}
+#endif
+#ifdef KSZ_MRP
+		if (sw->features & MRP_SUPPORT) {
+			u32 speed;
+			struct mrp_info *mrp = &sw->mrp;
+
+			if (media_disconnected == info->state)
+				speed = 0;
+			else
+				speed = info->tx_rate / TX_RATE_UNIT;
+			mrp_set_speed(mrp, p, speed);
+		}
+#endif
+		state->state = info->state;
+	}
+	sw->phy_intr = 0;
+
+	if (linked && media_disconnected == port->linked->state)
+		port->linked = linked;
+
+#ifdef CONFIG_1588_PTP
+	if ((sw->features & PTP_HW) && change) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		for (i = sw->phy_port_cnt; i < ptp->ports; i++) {
+			if (media_disconnected == port->linked->state)
+				ptp->linked[i] = 0;
+			else
+				ptp->linked[i] = 1000;
+		}
+		if (ptp->started)
+			schedule_work(&ptp->set_latency);
+	}
+#endif
+#ifdef DEBUG
+	if (change)
+		dbp_link(port, sw, change);
+#endif
+#ifdef KSZ_IBA
+	change |= sw->link_change;
+	sw->link_change = 0;
+#endif
+	if (change)
+		schedule_work(&port->link_update);
+	return change;
+}  /* port_get_link_speed */
+
+/**
+ * port_set_link_speed - set port speed
+ * @port:	The port instance.
+ *
+ * This routine sets the link speed of the switch ports.
+ */
+static void port_set_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	u16 data;
+	u16 ctrl;
+	u16 local;
+	u16 status;
+	u32 adv;
+	u32 cfg;
+	int i;
+	int p;
+
+#ifdef NO_PHY_READ
+if (port)
+	return;
+#endif
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+
+		/* Port does not have PHY. */
+		if (p >= sw->phy_port_cnt)
+			break;
+
+		/* Host port is not at either end. */
+		if (p == sw->HOST_PORT)
+			continue;
+
+		port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &local);
+		if (!(local & PORT_AUTO_NEG_SYM_PAUSE))
+			dbg_msg(" no sym pause: %d %04x\n", p, local);
+		if (local & PORT_AUTO_NEG_ASYM_PAUSE)
+			dbg_msg(" has asym pause: %d %04x\n", p, local);
+		adv = ctrl;
+		adv <<= 16;
+		adv |= local;
+		port_r16(sw, p, P_SPEED_STATUS, &status);
+		port_r16(sw, p, P_NEG_RESTART_CTRL, &data);
+
+		cfg = 0;
+
+		/*
+		 * Do not need to restart auto-negotiation if desired settings
+		 * are same.
+		 */
+		if ((data & PORT_AUTO_NEG_ENABLE) &&
+		    (status &
+		    (PORT_STAT_SPEED_1000MBIT |
+		     PORT_STAT_SPEED_100MBIT |
+		     PORT_STAT_SPEED_10MBIT)))
+			cfg = adv;
+
+		local = port_advertised_flow_ctrl(port, local);
+
+		ctrl |= PORT_AUTO_NEG_1000BT_FD | PORT_AUTO_NEG_1000BT;
+		local |= PORT_AUTO_NEG_100BTX_FD | PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD | PORT_AUTO_NEG_10BT;
+
+		/* Check if manual configuration is specified by the user. */
+		if (port->speed || port->duplex) {
+			if (port->speed && port->speed != 1000)
+				ctrl &= ~(PORT_AUTO_NEG_1000BT_FD |
+					PORT_AUTO_NEG_1000BT);
+			if (10 == port->speed)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX);
+			else if (100 == port->speed)
+				local &= ~(PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			else if (1000 == port->speed)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			if (1 == port->duplex) {
+				ctrl &= ~PORT_AUTO_NEG_1000BT_FD;
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_10BT_FD);
+			} else if (2 == port->duplex) {
+				ctrl &= ~PORT_AUTO_NEG_1000BT;
+				local &= ~(PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT);
+			}
+		}
+		adv = ctrl;
+		adv <<= 16;
+		adv |= local;
+		if (adv != cfg) {
+			port_w16(sw, p, REG_PORT_PHY_1000_CTRL, ctrl);
+			port_w16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, local);
+			port_r16(sw, p, P_NEG_RESTART_CTRL, &data);
+			data |= PORT_AUTO_NEG_ENABLE;
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w16(sw, p, P_NEG_RESTART_CTRL, data);
+
+			/* Link is going down. */
+			sw->port_state[p].state = media_disconnected;
+		}
+	}
+}  /* port_set_link_speed */
+
+/**
+ * port_force_link_speed - force port speed
+ * @port:	The port instance.
+ *
+ * This routine forces the link speed of the switch ports.
+ */
+static void port_force_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	u16 data;
+	int i;
+	int p;
+
+#ifdef NO_PHY_READ
+if (port)
+	return;
+#endif
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+
+		/* Port does not have PHY. */
+		if (p >= sw->phy_port_cnt) {
+			break;
+		}
+
+		/* Host port is not at either end. */
+		if (p == sw->HOST_PORT)
+			continue;
+
+		port_r16(sw, p, P_PHY_CTRL, &data);
+		data &= ~(PORT_AUTO_NEG_ENABLE |
+			PORT_SPEED_100MBIT | PORT_SPEED_1000MBIT);
+		if (100 == port->speed)
+			data |= PORT_SPEED_100MBIT;
+		else if (1000 == port->speed)
+			data |= PORT_SPEED_1000MBIT;
+		if (1 == port->duplex)
+			data &= ~PORT_FULL_DUPLEX;
+		else if (2 == port->duplex)
+			data |= PORT_FULL_DUPLEX;
+		port_w16(sw, p, P_PHY_CTRL, data);
+	}
+}  /* port_force_link_speed */
+
+static void port_mmd_read(struct ksz_sw *sw, int port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+		MMD_SETUP(PORT_MMD_OP_INDEX, devid));
+	port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, reg);
+	if (len > 1)
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_INCR_RW, devid));
+	else
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_NO_INCR, devid));
+	while (len) {
+		port_r16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, buf);
+		buf++;
+		len--;
+	}
+}  /* port_mmd_read */
+
+static void port_mmd_write(struct ksz_sw *sw, int port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+		MMD_SETUP(PORT_MMD_OP_INDEX, devid));
+	port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, reg);
+	if (len > 1)
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_INCR_W, devid));
+	else
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_NO_INCR, devid));
+	while (len) {
+		port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, *buf);
+		buf++;
+		len--;
+	}
+}  /* port_mmd_write */
+
+static void port_setup_eee(struct ksz_sw *sw, int port)
+{
+	u16 val[0x20];
+
+dbg_msg("%s %d\n", __func__, port);
+	if (sw->features & NEW_CAP) {
+#if 1
+if (0 == port) {
+int i;
+		port_r16(sw, port, REG_PORT_PHY_CTRL, val);
+dbg_msg("%04x=%04x\n", REG_PORT_PHY_CTRL, val[0]);
+		port_r16(sw, port, REG_PORT_PHY_REMOTE_LB_LED, val);
+dbg_msg("%04x=%04x\n", REG_PORT_PHY_REMOTE_LB_LED, val[0]);
+
+dbg_msg(" %x\n", MMD_DEVICE_ID_DSP);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCE, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCE, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCC, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCC, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCA, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCA, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCB, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCB, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xC8, val, 1);
+dbg_msg(" %04x=%04x\n", 0xC8, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xD9, val, 1);
+dbg_msg(" %04x=%04x\n", 0xD9, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xC9, val, 1);
+dbg_msg(" %04x=%04x\n", 0xC9, val[0]);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x79, &val[0x09],
+			18);
+for (i = 0; i < 18; i++)
+dbg_msg("%04x ", val[0x09 + i]);
+dbg_msg("\n");
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x8F, val, 1);
+dbg_msg(" %04x=%04x\n", 0x8F, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x9D, val, 1);
+dbg_msg(" %04x=%04x\n", 0x9D, val[0]);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x75, val, 1);
+dbg_msg(" %04x=%04x\n", 0x75, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xD3, val, 1);
+dbg_msg(" %04x=%04x\n", 0xD3, val[0]);
+
+dbg_msg(" %x\n", MMD_DEVICE_ID_AFED);
+		port_mmd_read(sw, port, 0x1C, 0x0, val, 1);
+dbg_msg(" %04x=%04x\n", 0x0, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x4, val, 1);
+dbg_msg(" %04x=%04x\n", 0x4, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x6, val, 1);
+dbg_msg(" %04x=%04x\n", 0x6, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x9, val, 1);
+dbg_msg(" %04x=%04x\n", 0x9, val[0]);
+
+		port_mmd_read(sw, port, 0x1C, 0x13, &val[0x13], 12);
+for (i = 0; i < 12; i++)
+dbg_msg("%04x ", val[0x13 + i]);
+dbg_msg("\n");
+}
+#endif
+
+		port_w16(sw, port, REG_PORT_PHY_CTRL, 0x2100);
+		port_w16(sw, port, REG_PORT_PHY_REMOTE_LB_LED, 0x00f0);
+
+		val[0] = 0x0100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCE, val, 1);
+		val[0] = 0x0ff0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCC, val, 1);
+		val[0] = 0x0141;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCA, val, 1);
+		val[0] = 0x0fcf;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCB, val, 1);
+		val[0] = 0x0010;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xC8, val, 1);
+		val[0] = 0x0100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xD9, val, 1);
+		val[0] = 0x0280;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xC9, val, 1);
+
+		val[0x09] = 0x010A;
+		val[0x0a] = 0x00ED;
+		val[0x0b] = 0x00D3;
+		val[0x0c] = 0x00BC;
+		val[0x0d] = 0x00A8;
+		val[0x0e] = 0x0096;
+		val[0x0f] = 0x0085;
+		val[0x10] = 0x0077;
+		val[0x11] = 0x006A;
+		val[0x12] = 0x005E;
+		val[0x13] = 0x0054;
+		val[0x14] = 0x004B;
+		val[0x15] = 0x0043;
+		val[0x16] = 0x003C;
+		val[0x17] = 0x0035;
+		val[0x18] = 0x002F;
+		val[0x19] = 0x002A;
+		val[0x1a] = 0x0026;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x79, &val[0x09],
+			18);
+
+		val[0] = 0x6032;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x8F, val, 1);
+		val[0] = 0x248C;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x9D, val, 1);
+
+		val[0] = 0x0060;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x75, val, 1);
+		val[0] = 0x7777;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xD3, val, 1);
+
+		val[0] = 0x9400;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x0, val, 1);
+		val[0] = 0x0000;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x4, val, 1);
+		val[0] = 0x3100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x6, val, 1);
+		val[0] = 0xe01c;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x9, val, 1);
+
+		val[0x13] = 0x6eff;
+		val[0x14] = 0xe6ff;
+		val[0x15] = 0x6eff;
+		val[0x16] = 0xe6ff;
+		val[0x17] = 0x00ff;
+		val[0x18] = 0x43ff;
+		val[0x19] = 0xc3ff;
+		val[0x1a] = 0x6fff;
+		val[0x1b] = 0x07ff;
+		val[0x1c] = 0x0fff;
+		val[0x1d] = 0xe7ff;
+		val[0x1e] = 0xefff;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x13, &val[0x13],
+			12);
+
+		if (port == sw->HOST_PORT)
+			port_w16(sw, port, REG_PORT_PHY_CTRL, 0x1140);
+	} else {
+		memset(val, 0, sizeof(val));
+		port_mmd_write(sw, port, 0x1C, 0x10, val, 0x11);
+	}
+}  /* port_setup_eee */
+
+static void port_setup_9893(struct ksz_sw *sw, int port)
+{
+	u16 val[0x40];
+	int i;
+
+dbg_msg("%s %d\n", __func__, port);
+	memset(val, 0, sizeof(val));
+	port_mmd_read(sw, port, 0x1C, 0x0, val, 0x35);
+	if (0x0008 == val[6])
+		return;
+	for (i = 0; i < 0x35; i++) {
+dbg_msg("%04x ", val[i]);
+		if ((i % 0x10) == 0xf)
+dbg_msg("\n");
+	}
+dbg_msg("\n");
+
+#if 0
+	port_w16(sw, port, REG_PORT_PHY_CTRL, 0);
+#else
+	port_w16(sw, port, REG_PORT_PHY_CTRL, 0x2100);
+#endif
+
+	val[0] = 0x0240;
+	port_mmd_write(sw, port, 0x1C, 0x0, val, 1);
+
+	val[0x11] = 0x7777;
+	port_mmd_write(sw, port, 0x1C, 0x11, &val[0x11], 1);
+
+	val[0x13] = 0x0077;
+	val[0x14] = 0x0077;
+	val[0x15] = 0x7777;
+	val[0x16] = 0x7777;
+	val[0x17] = 0x0077;
+	val[0x18] = 0x4377;
+	val[0x19] = 0x4377;
+	val[0x1a] = 0x6777;
+	val[0x1b] = 0x0777;
+	val[0x1c] = 0x0777;
+	val[0x1d] = 0x6777;
+	val[0x1e] = 0x6777;
+	port_mmd_write(sw, port, 0x1C, 0x13, &val[0x13], 12);
+
+	val[0x20] = 0x0;
+	port_mmd_write(sw, port, 0x1C, 0x20, &val[0x20], 1);
+
+	val[0x6] = 0x0008;
+	port_mmd_write(sw, port, 0x1C, 0x6, &val[0x6], 1);
+
+	val[0x25] = 0x0;
+	val[0x26] = 0x1f10;
+	val[0x27] = 0x1f1f;
+	val[0x28] = 0x0f00;
+	val[0x29] = 0x0;
+	val[0x2a] = 0x0;
+	val[0x2b] = 0x0;
+	val[0x2c] = 0x0;
+	val[0x2d] = 0x0;
+	val[0x2e] = 0x0;
+	val[0x2f] = 0x0;
+	val[0x30] = 0x0;
+	val[0x31] = 0x0;
+	val[0x32] = 0x0;
+	val[0x33] = 0x0;
+	val[0x34] = 0x0;
+	port_mmd_write(sw, port, 0x1C, 0x25, &val[0x25], 16);
+
+	val[0] = 0x80e6;
+	port_mmd_write(sw, port, 0x1, 0xd8, val, 1);
+	val[0] = 0x02ff;
+	port_mmd_write(sw, port, 0x1, 0xc9, val, 1);
+
+#if 0
+	port_w16(sw, port, REG_PORT_PHY_CTRL, 0x1340);
+#else
+	port_w16(sw, port, REG_PORT_PHY_CTRL, 0x1140);
+#endif
+}  /* port_setup_9893 */
+
+/**
+ * sw_enable - enable the switch
+ * @sw:		The switch instance.
+ *
+ * This routine enables the switch with a specific configuration.
+ */
+static void sw_enable(struct ksz_sw *sw)
+{
+	int port;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (port == sw->HOST_PORT)
+			continue;
+		if (sw->dev_count > 1)
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		else
+			port_set_stp_state(sw, port, STP_STATE_FORWARDING);
+	}
+	if (sw->dev_count > 1)
+		port_set_stp_state(sw, sw->HOST_PORT, STP_STATE_SIMPLE);
+	else
+		port_set_stp_state(sw, sw->HOST_PORT, STP_STATE_FORWARDING);
+
+	/*
+	 * There may be some entries in the dynamic MAC table before the
+	 * the learning is turned off.  Once the entries in the table the
+	 * switch may keep updating them even learning is off.
+	 */
+	if (sw->dev_count > 1)
+		sw_flush_dyn_mac_table(sw, sw->mib_port_cnt);
+}  /* sw_enable */
+
+static void sw_init_cached_regs(struct ksz_sw *sw)
+{
+	sw->cached.ptp_clk_ctrl = sw->reg->r16(sw, REG_PTP_CLK_CTRL);
+	sw->cached.ptp_unit_index = sw->reg->r32(sw, REG_PTP_UNIT_INDEX__4);
+}
+
+/**
+ * sw_init - initialize the switch
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the hardware switch engine for default operation.
+ */
+static void sw_init(struct ksz_sw *sw)
+{
+	memset(sw->tx_pad, 0, 60);
+	sw->tx_start = 0;
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+return;
+}
+#endif
+	sw_init_cached_regs(sw);
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+	sw_init_phy_addr(sw);
+#endif
+
+	sw_init_broad_storm(sw);
+
+	sw_init_prio(sw);
+
+	sw_init_prio_rate(sw);
+
+	sw_init_vlan(sw);
+
+	sw_init_acl(sw);
+#if 0
+	if (!sw_chk(sw, REG_SWITCH_CTRL_1,
+			SWITCH_TX_FLOW_CTRL | SWITCH_RX_FLOW_CTRL))
+		sw->overrides |= PAUSE_FLOW_CTRL;
+#endif
+}  /* sw_init */
+
+/**
+ * sw_setup - setup the switch
+ * @sw:		The switch instance.
+ *
+ * This routine setup the hardware switch engine for default operation.
+ */
+static void sw_setup(struct ksz_sw *sw)
+{
+	int port;
+
+	sw->port_intr_mask = sw->PORT_MASK;
+	sw->intr_mask = TRIG_TS_INT | APB_TIMEOUT_INT;
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+return;
+}
+#endif
+	sw_set_global_ctrl(sw);
+	sw_setup_reserved_multicast(sw);
+#if 0
+	sw->reg->w16(sw, REG_SW_ISP_TPID__2, 0x88A8);
+#endif
+/*
+ * THa  2015/12/03
+ * The new chip does not require legal packet check to be disabled for the tail
+ * tagging to work, but it still counts packets as oversized.
+ */
+#if 0
+	if (!(sw->features & NEW_CAP))
+#endif
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE, 1);
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		sw->info->port_cfg[port].intr_mask = 0;
+		port_cfg_back_pressure(sw, port, 1);
+		if (port < sw->phy_port_cnt)
+			port_cfg_force_flow_ctrl(sw, port, 0);
+		else
+			port_cfg_force_flow_ctrl(sw, port, 1);
+#ifndef KSZ_DLR
+		if (!(sw->features & ACL_CORRUPT_BUG))
+#endif
+			sw->info->port_cfg[port].intr_mask |= PORT_ACL_INT;
+		if (port == sw->HOST_PORT)
+			continue;
+
+#ifdef CONFIG_1588_PTP
+		sw->info->port_cfg[port].intr_mask |= PORT_PTP_INT;
+#endif
+	}
+/*
+ * THa  2015/10/01
+ * Increasing wait time in this register avoids the PTP transmit problem.
+ */
+	dbg_msg("eee txq wait: %04x\n",
+		sw->reg->r16(sw, REG_SW_EEE_TXQ_WAIT_TIME__2));
+	sw->reg->w16(sw, REG_SW_EEE_TXQ_WAIT_TIME__2, 0x0040);
+
+#ifndef KSZ9897_FPGA
+	for (port = 0; port < sw->phy_port_cnt; port++) {
+		u16 val = 0;
+
+#if 0
+		port_r16(sw, port, REG_PORT_PHY_DIGITAL_DEBUG_1, &val);
+		val |= PORT_REG_CLK_SPEED_25_MHZ;
+		port_w16(sw, port, REG_PORT_PHY_DIGITAL_DEBUG_1, val);
+		delay_milli(1);
+#endif
+
+		if (sw->features & SETUP_PHY)
+			port_setup_9893(sw, port);
+		port_setup_eee(sw, port);
+#ifdef NO_EEE
+		/* Disable EEE for now. */
+		port_mmd_read(sw, port, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+/*
+ * THa  2015/09/30
+ * EEE in gigabit causes PTP messages not to be sent immediately.
+ * Just advertise EEE in 100 causes link not to be established.
+ * EEE in 100 has too much PTP jitter.
+ */
+		val = 0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+#endif
+
+		/*
+		 * Switch actually cannot do auto-negotiation with old 10Mbit
+		 * hub.
+		 */
+		port_r16(sw, port, P_PHY_CTRL, &val);
+		val &= ~PORT_FULL_DUPLEX;
+		port_w16(sw, port, P_PHY_CTRL, val);
+
+/*
+ * THa  2015/10/07
+ * The S2 chip has a bug that writing to the 0xN13E register will cause the
+ * 100Mbit link to be unstable.
+ * One way to workaround is to use 25 MHz clock speed for register access.
+ */
+#if 0
+		/* Enable port PHY interrupt. */
+		sw->info->port_cfg[port].intr_mask |= PORT_PHY_INT;
+
+		port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &val);
+
+		/* Normally it should be low? */
+		val |= PORT_INT_PIN_HIGH;
+		port_w16(sw, port, REG_PORT_PHY_PHY_CTRL, val);
+		val = LINK_DOWN_INT | LINK_UP_INT;
+
+#if 1
+/*
+ * THa  2014/06/25
+ * SPI cannot just read PHY interrupt status register to clear interrupts.
+ * No way to clear the interrupts so cannot enable them here.
+ */
+		if (!(sw->features & NEW_CAP))
+			val = 0;
+#endif
+		port_w8(sw, port, REG_PORT_PHY_INT_ENABLE, val);
+
+/*
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+		port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &val);
+
+/*
+ * THa  2014/11/18
+ * The new KSZ9893 chip has a bug related to IBA.  When writing PHY related
+ * registers with SPI while IBA is enabled APB interrupt can be triggered.
+ * Somehow the hardware expects those writes be 32-bit.  By itself this bug is
+ * just an annoyance, but port 2 has additional problem.
+ * After IBA are run the next initialization triggers a more serioud bug.
+ * Instead of generating APB interrupt the PHY_PHY_CTRL register 0x213E is
+ * reset to 0 by the hardware.  Writing to the PHY_INT_ENABLE register, even
+ * with a zero value, will generate the PHY interrupt with no actual PHY
+ * status.  As the interrupt pin is no longer high, the interrupt will keep
+ * coming until it is masked out or the interrupt pin is set to high again.
+ */
+#endif
+	}
+#endif
+#if 0
+	for (port = 0; port < sw->phy_port_cnt; port++) {
+#ifdef KSZ_IBA
+		if (port == sw->HOST_PORT) {
+			port_w8(sw, port, REG_PORT_MAC_CTRL_2, 0);
+			continue;
+		}
+#endif
+		port_w8(sw, port, REG_PORT_MAC_CTRL_2,
+#if 1
+			PORT_100BT_EEE_DISABLE |
+#endif
+			PORT_1000BT_EEE_DISABLE);
+	}
+#endif
+#ifdef KSZ9897_FPGA
+printk("setup PHY ports\n");
+if (3 == sw->phy_port_cnt)
+	do {
+		int i;
+		struct ksz_port_info *info;
+
+		for (i = 0; i < sw->phy_port_cnt; i++) {
+			port_w8(sw, i, 0x111, 1);
+			port_w8(sw, i, 0x110, 3);
+			info = &sw->port_info[i];
+			info->state = media_connected;
+			info->tx_rate = 100 * TX_RATE_UNIT;
+			info->duplex = 2;
+		}
+	} while (0);
+#endif
+
+	sw_setup_broad_storm(sw);
+
+	sw_setup_prio(sw);
+
+	sw_setup_mirror(sw);
+
+	sw->info->multi_net = SWITCH_MAC_TABLE_ENTRIES;
+	if (sw->features & STP_SUPPORT) {
+		sw_setup_stp(sw);
+#ifdef CONFIG_KSZ_STP
+		sw_setup_multi(sw);
+#ifdef CONFIG_1588_PTP
+		sw_setup_ptp(sw);
+#endif
+#endif
+	}
+#ifdef KSZ_IBA
+	sw_setup_iba(sw);
+#endif
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		sw_setup_dlr(sw);
+#endif
+	sw_enable(sw);
+}  /* sw_setup */
+
+static void sw_reset(struct ksz_sw *sw)
+{
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+return;
+}
+#endif
+	sw->overrides &= ~VLAN_SET;
+	if (sw->features & NEW_CAP) {
+		int p;
+		u8 byte_before;
+		u8 byte_after;
+
+		sw_cfg(sw, REG_SW_OPERATION, SW_RESET, 1);
+		delay_micro(1);
+
+/*
+ * THa  2015/10/07
+ * The S2 chip has a bug that writing to the 0xN13E register will cause the
+ * 100Mbit link to be unstable.
+ * One way to workaround is to use 25 MHz clock speed for register access.
+ */
+#if 0
+		/* PHY interrupt level is not in normal position. */
+		for (p = 0; p < sw->phy_port_cnt; p++) {
+			u16 val;
+
+			port_r16(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, &val);
+			val |= PORT_REG_CLK_SPEED_25_MHZ;
+			port_w16(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, val);
+			delay_milli(1);
+
+			port_r16(sw, p, REG_PORT_PHY_PHY_CTRL, &val);
+
+			val |= PORT_INT_PIN_HIGH;
+/*
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+			port_w16(sw, p, REG_PORT_PHY_PHY_CTRL, val);
+		}
+#endif
+		for (p = sw->phy_port_cnt; p < sw->mib_port_cnt; p++) {
+			port_r(sw, p, REG_PORT_XMII_CTRL_1, &byte_before);
+			byte_after = byte_before ^(PORT_MII_NOT_1GBIT |
+				PORT_MII_MAC_MODE | PORT_MII_SEL_M);
+			port_w(sw, p, REG_PORT_XMII_CTRL_1, byte_after);
+			port_w(sw, p, REG_PORT_XMII_CTRL_1, byte_before);
+		}
+		sw_reset_acl(sw);
+		sw->overrides &= ~TAIL_TAGGING;
+		sw->overrides &= ~PTP_TAG;
+		sw->overrides &= ~TAG_REMOVE;
+		sw_dis_intr(sw);
+		return;
+	}
+
+	/* There is no global reset function yet. */
+	sw_dis_vlan(sw);
+	do {
+		int p;
+
+		/* Need to turn on ACL to write to ACL table. */
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			port_cfg_acl(sw, p, 1);
+			port_w16(sw, p, REG_PORT_ACL_BYTE_EN_MSB, 0xffff);
+		}
+		sw_reset_acl_hw(sw);
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (p == sw->HOST_PORT)
+				continue;
+			if (sw->dev_count > 1)
+				sw_cfg_port_base_vlan(sw, p, sw->PORT_MASK);
+			port_set_stp_state(sw, p, STP_STATE_FORWARDING);
+		}
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			sw_cfg_def_vid(sw, p, 1);
+			port_cfg_tail_tag(sw, p, 0);
+			port_set_authen_mode(sw, p, 0);
+#ifndef NO_ACL
+/*
+ * THa  2014/06/18
+ * Cannot read ACL in next bootup if there are lots of traffic going through
+ * the port.
+ */
+			if (sw->features & ACL_CORRUPT_BUG)
+				port_cfg_acl(sw, p, 1);
+			else
+				port_cfg_acl(sw, p, 0);
+#endif
+		}
+	} while (0);
+	sw->overrides &= ~TAIL_TAGGING;
+	sw->overrides &= ~PTP_TAG;
+	sw->overrides &= ~TAG_REMOVE;
+}  /* sw_reset */
+
+static int sw_chk_reg(struct ksz_sw *sw, u32 reg, size_t count)
+{
+	size_t i;
+
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE) {
+		if (!check_sw_reg_range(reg))
+			return false;
+	}
+	return true;
+}
+
+static int sw_reg_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	if (sw_chk_reg(sw, reg, count)) {
+		sw_r(sw, reg, buf, count);
+		return count;
+	}
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		*addr = 0;
+		if (check_sw_reg_range(reg))
+			*addr = SW_R(sw, reg);
+	}
+	return i;
+}
+
+static int sw_reg_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	if (sw_chk_reg(sw, reg, count)) {
+		sw_w(sw, reg, buf, count);
+		return count;
+	}
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		if (check_sw_reg_range(reg))
+			SW_W(sw, reg, *addr);
+	}
+	return i;
+}
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.lock			= sw_lock,
+	.unlock			= sw_unlock,
+
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r24			= sw_r24,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w24			= sw_w24,
+	.w32			= sw_w32,
+
+	.r			= sw_r,
+	.w			= sw_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+
+	.r_dyn_mac_hw		= sw_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= sw_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= sw_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= sw_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= sw_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= sw_r_sta_mac_hw,
+	.w_sta_mac_hw		= sw_w_sta_mac_hw,
+	.r_vlan_hw		= sw_r_vlan_hw,
+	.w_vlan_hw		= sw_w_vlan_hw,
+	.r_mib_cnt_hw		= sw_r_mib_cnt_hw,
+	.r_acl_hw		= sw_r_acl_hw,
+	.w_acl_hw		= sw_w_acl_hw,
+};
+
+#ifdef KSZ_IBA
+/**
+ * sw_set_spi - use SPI for access
+ * @sw:		The switch instance.
+ * @iba:	The IBA instance.
+ *
+ * This routine uses default hardware access like SPI for register access.
+ */
+static void sw_set_spi(struct ksz_sw *sw, struct ksz_iba_info *iba)
+{
+	sw->reg = &sw_reg_ops;
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg = &ptp_reg_ops;
+	}
+#endif
+	iba->use_iba = 0;
+}  /* sw_set_spi */
+
+/**
+ * sw_set_ops - try to use SPI for access
+ * @sw:		The switch instance.
+ * @iba:	The IBA instance.
+ *
+ * This routine tries to use IBA for register access.
+ */
+static void sw_set_ops(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_ops);
+	struct ksz_iba_info *iba = &sw->info->iba;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = NULL;
+#endif
+
+	if (sw->reg == &sw_iba_ops)
+		return;
+
+	if (sw->HOST_PORT < sw->phy_port_cnt && !netif_carrier_ok(iba->dev)) {
+		schedule_delayed_work(&sw->set_ops, 1);
+		return;
+	}
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		ptp = &sw->ptp_hw;
+#endif
+	mutex_lock(&sw->lock);
+	mutex_lock(sw->hwlock);
+	mutex_lock(sw->reglock);
+	if (netif_running(iba->dev)) {
+#ifdef CONFIG_1588_PTP
+		if (ptp)
+			ptp->reg = &ptp_iba_ops;
+#endif
+		sw->reg = &sw_iba_ops;
+		iba->cnt = 0;
+		iba->use_iba = 1;
+	}
+dbg_msg("changed: %p\n", sw->reg);
+if (sw->reg == &sw_iba_ops)
+dbg_msg("iba set\n");
+	if (iba->use_iba) {
+		u32 id;
+
+		sw->intr_using = 2;
+		iba->use_iba |= 0x80;
+		id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+		iba->use_iba &= ~0x80;
+dbg_msg("id = %08x\n", id);
+#if 1
+/*
+ * THa  2016/01/03
+ * KSZ9563 S1 does not respond the very first time when using RGMII.
+ */
+		if (id == 0xdeadbeaf && (sw->features & SETUP_PHY)) {
+			id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+dbg_msg("id = %08x\n", id);
+		}
+#endif
+		sw->intr_using = 0;
+		if (id == 0xdeadbeaf)
+			sw_set_spi(sw, iba);
+	}
+	mutex_unlock(sw->reglock);
+	mutex_unlock(sw->hwlock);
+	mutex_unlock(&sw->lock);
+
+	if (!iba->use_iba)
+		return;
+#if 1
+/*
+ * THa  2014/06/25
+ * SPI cannot just read PHY interrupt status register to clear interrupts.
+ *
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+	if (!(sw->features & NEW_CAP)) {
+		int port;
+		u8 val = LINK_DOWN_INT | LINK_UP_INT;
+
+		sw->ops->acquire(sw);
+		for (port = 0; port < sw->phy_port_cnt; port++) {
+			port_w8(sw, port, REG_PORT_PHY_INT_ENABLE, val);
+		}
+		sw->ops->release(sw);
+	}
+#endif
+#ifdef TEST_IBA
+	if (iba->use_iba) {
+u32 id;
+		sw->ops->acquire(sw);
+id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+		sw_init(sw);
+		sw->ops->release(sw);
+	}
+#endif
+}  /* sw_set_ops */
+
+/**
+ * sw_set_ops - try to use SPI for access
+ * @sw:		The switch instance.
+ * @dev:	The network device.
+ * @mac_addr:	The MAC address used.
+ *
+ * This routine setup the IBA with the network device and its MAC address if
+ * the device exists.  Otherwise it uses the default access like SPI.
+ */
+static void sw_set_dev(struct ksz_sw *sw, struct net_device *dev, u8 *mac_addr)
+{
+	struct ksz_iba_info *iba = &sw->info->iba;
+	int delay_tick = 2;
+
+	if (sw->HOST_PORT < sw->phy_port_cnt)
+		delay_tick = 10;
+	if (!dev) {
+		if (sw->reg != &sw_reg_ops) {
+#ifdef CONFIG_1588_PTP
+			struct ptp_info *ptp = NULL;
+
+			if (sw->features & PTP_HW)
+				ptp = &sw->ptp_hw;
+#endif
+			mutex_lock(&sw->lock);
+			mutex_lock(sw->hwlock);
+			mutex_lock(sw->reglock);
+			sw_set_spi(sw, iba);
+			mutex_unlock(sw->reglock);
+			mutex_unlock(sw->hwlock);
+			mutex_unlock(&sw->lock);
+		}
+	} else if (sw->features & IBA_SUPPORT)
+		schedule_delayed_work(&sw->set_ops, delay_tick);
+	mutex_lock(sw->hwlock);
+	iba->dev = dev;
+	prepare_iba(iba, iba->dst, mac_addr);
+	mutex_unlock(sw->hwlock);
+}  /* sw_set_dev */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Micrel LinkMD routines
+ */
+
+#if 0
+enum {
+	CABLE_UNKNOWN,
+	CABLE_GOOD,
+	CABLE_CROSSED,
+	CABLE_REVERSED,
+	CABLE_CROSSED_REVERSED,
+	CABLE_OPEN,
+	CABLE_SHORT
+};
+
+#define STATUS_FULL_DUPLEX		0x01
+#define STATUS_CROSSOVER		0x02
+#define STATUS_REVERSED			0x04
+
+#define LINK_10MBPS_FULL		0x00000001
+#define LINK_10MBPS_HALF		0x00000002
+#define LINK_100MBPS_FULL		0x00000004
+#define LINK_100MBPS_HALF		0x00000008
+#define LINK_1GBPS_FULL			0x00000010
+#define LINK_1GBPS_HALF			0x00000020
+#define LINK_10GBPS_FULL		0x00000040
+#define LINK_10GBPS_HALF		0x00000080
+#define LINK_SYM_PAUSE			0x00000100
+#define LINK_ASYM_PAUSE			0x00000200
+
+#define LINK_AUTO_MDIX			0x00010000
+#define LINK_MDIX			0x00020000
+#define LINK_AUTO_POLARITY		0x00040000
+
+#define CABLE_LEN_MAXIMUM		15000
+#define CABLE_LEN_MULTIPLIER		41
+
+#define PHY_RESET_TIMEOUT		10
+
+/**
+ * hw_get_link_md -
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to get the LinkMD status.
+ */
+static void hw_get_link_md(struct ksz_sw *sw, int port)
+{
+	SW_D crossover;
+	SW_D ctrl;
+	SW_D data;
+	SW_D link;
+	u16 len;
+	int i;
+	int timeout;
+	struct ksz_port_info *port_info = &sw->port_info[port];
+
+	port_r(sw, port, P_SPEED_STATUS, &data);
+	port_r(sw, port, P_LINK_STATUS, &link);
+	port_info->status[0] = CABLE_UNKNOWN;
+	if (link & (PORT_1000_LINK_GOOD | PORT_100_LINK_GOOD)) {
+		int stat = 0;
+
+		port_info->status[0] = CABLE_GOOD;
+		port_info->length[0] = 1;
+		port_info->status[1] = CABLE_GOOD;
+		port_info->length[1] = 1;
+		port_info->status[2] = CABLE_GOOD;
+		port_info->length[2] = 1;
+
+#if 0
+		if (link & PORT_MDIX_STATUS)
+			stat |= STATUS_CROSSOVER;
+		if (data & PORT_REVERSED_POLARITY)
+			stat |= STATUS_REVERSED;
+		if ((stat & (STATUS_CROSSOVER | STATUS_REVERSED)) ==
+				(STATUS_CROSSOVER | STATUS_REVERSED))
+			port_info->status[0] = CABLE_CROSSED_REVERSED;
+		else if ((stat & STATUS_CROSSOVER) == STATUS_CROSSOVER)
+			port_info->status[0] = CABLE_CROSSED;
+		else if ((stat & STATUS_REVERSED) == STATUS_REVERSED)
+			port_info->status[0] = CABLE_REVERSED;
+#endif
+		return;
+	}
+
+	/* Put in 10 Mbps mode. */
+	port_r(sw, port, P_PHY_CTRL, &ctrl);
+	data = ctrl;
+	data &= ~(PORT_AUTO_NEG_ENABLE | PORT_FULL_DUPLEX |
+		PORT_SPEED_1000MBIT | PORT_SPEED_100MBIT);
+	port_w(sw, port, P_PHY_CTRL, data);
+
+	port_r(sw, port, P_NEG_RESTART_CTRL, &data);
+	crossover = data;
+
+#if 0
+	for (i = 1; i <= 2; i++) {
+		data = crossover;
+
+		/* Disable auto MDIX. */
+		data |= PORT_AUTO_MDIX_DISABLE;
+		if (0 == i)
+			data &= ~PORT_FORCE_MDIX;
+		else
+			data |= PORT_FORCE_MDIX;
+
+		/* Disable transmitter. */
+		data |= PORT_TX_DISABLE;
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Wait at most 1 second.*/
+		delay_milli(100);
+
+		/* Enable transmitter. */
+		data &= ~PORT_TX_DISABLE;
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Start cable diagnostic test. */
+		port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		data |= PORT_START_CABLE_DIAG;
+		port_w(sw, port, REG_PORT_LINK_MD_CTRL, data);
+		timeout = PHY_RESET_TIMEOUT;
+		do {
+			if (!--timeout)
+				break;
+			delay_milli(10);
+			port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		} while ((data & PORT_START_CABLE_DIAG));
+
+		port_info->length[i] = 0;
+		port_info->status[i] = CABLE_UNKNOWN;
+
+		if (!(data & PORT_START_CABLE_DIAG)) {
+#if (SW_SIZE == (1))
+			port_r8(sw, port, REG_PORT_LINK_MD_RESULT, &link);
+			len = data & PORT_CABLE_FAULT_COUNTER_H;
+			len <<= 16;
+			len |= link;
+#else
+			len = data & PORT_CABLE_FAULT_COUNTER;
+#endif
+			port_info->length[i] = len *
+				CABLE_LEN_MULTIPLIER;
+			if (data & PORT_CABLE_10M_SHORT)
+				port_info->length[i] = 1;
+			data &= PORT_CABLE_DIAG_RESULT;
+			switch (data) {
+			case PORT_CABLE_STAT_NORMAL:
+				port_info->status[i] = CABLE_GOOD;
+				port_info->length[i] = 1;
+				break;
+			case PORT_CABLE_STAT_OPEN:
+				port_info->status[i] = CABLE_OPEN;
+				break;
+			case PORT_CABLE_STAT_SHORT:
+				port_info->status[i] = CABLE_SHORT;
+				break;
+			}
+		}
+	}
+#endif
+
+	port_w(sw, port, P_PHY_CTRL, ctrl);
+	if (ctrl & PORT_AUTO_NEG_ENABLE) {
+		crossover |= PORT_AUTO_NEG_RESTART;
+		port_w(sw, port, P_NEG_RESTART_CTRL, crossover);
+	}
+
+	port_info->length[0] = port_info->length[1];
+	port_info->status[0] = port_info->status[1];
+	for (i = 2; i < 3; i++) {
+		if (CABLE_GOOD == port_info->status[0]) {
+			if (port_info->status[i] != CABLE_GOOD) {
+				port_info->status[0] = port_info->status[i];
+				port_info->length[0] = port_info->length[i];
+				break;
+			}
+		}
+	}
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+static void get_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	u64 *counter)
+{
+	int i;
+	int mib;
+	int port;
+	struct ksz_port_mib *port_mib;
+
+	memset(counter, 0, sizeof(u64) * TOTAL_SWITCH_COUNTER_NUM);
+	for (i = 0, port = first; i < cnt; i++, port++) {
+		if (cnt > 1 && port == sw->HOST_PORT)
+			continue;
+		port_mib = &sw->port_mib[port];
+		for (mib = port_mib->mib_start; mib < sw->mib_cnt; mib++)
+			counter[mib] += port_mib->counter[mib];
+	}
+}
+
+static struct {
+	int rx;
+	int tx;
+} mib_display[TOTAL_SWITCH_COUNTER_NUM / 2] = {
+	{ MIB_RX_TOTAL, MIB_TX_TOTAL },
+	{ MIB_RX_HI_PRIO, MIB_TX_HI_PRIO },
+	{ MIB_RX_PAUSE, MIB_TX_PAUSE },
+	{ MIB_RX_BROADCAST, MIB_TX_BROADCAST },
+	{ MIB_RX_MULTICAST, MIB_TX_MULTICAST },
+	{ MIB_RX_UNICAST, MIB_TX_UNICAST },
+	{ MIB_RX_DROPS, MIB_TX_DROPS },
+	{ MIB_RX_OCTET_64, MIB_RX_OCTET_65_127 },
+	{ MIB_RX_OCTET_128_255, MIB_RX_OCTET_256_511 },
+	{ MIB_RX_OCTET_512_1023, MIB_RX_OCTET_1024_1522 },
+	{ MIB_RX_OCTET_1523_2000, MIB_RX_OCTET_2001 },
+	{ MIB_RX_UNDERSIZE, MIB_RX_OVERSIZE },
+	{ MIB_RX_FRAGMENT, MIB_RX_JABBER },
+	{ MIB_RX_SYMBOL_ERR, MIB_RX_CRC_ERR },
+	{ MIB_RX_ALIGNMENT_ERR, MIB_RX_CTRL_8808 },
+	{ MIB_TX_LATE_COLLISION, MIB_TX_DEFERRED },
+	{ MIB_TX_TOTAL_COLLISION, MIB_TX_EXCESS_COLLISION },
+	{ MIB_TX_SINGLE_COLLISION, MIB_TX_MULTI_COLLISION },
+};
+
+static int display_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	char *buf)
+{
+	int mib;
+	int len = 0;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	get_sw_mib_counters(sw, first, cnt, counter);
+	for (mib = 0; mib < TOTAL_SWITCH_COUNTER_NUM / 2; mib++) {
+		int rx = mib_display[mib].rx;
+		int tx = mib_display[mib].tx;
+		if (buf)
+			len += sprintf(buf + len,
+				"%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+		else
+			printk(KERN_INFO "%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+	}
+	return len;
+}  /* display_sw_mib_counters */
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_sw_info(int cnt, char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	if (TOTAL_PORT_NUM != cnt)
+		return len;
+
+	len += sprintf(buf + len, "\ndynamic_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's dynamic MAC table\n");
+	len += sprintf(buf + len, "static_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's static MAC table\n");
+	len += sprintf(buf + len, "vlan_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's VLAN table\n");
+
+	len += sprintf(buf + len, "\naging:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable aging\n");
+	len += sprintf(buf + len, "fast_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable fast aging\n");
+	len += sprintf(buf + len, "link_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable link change auto aging\n");
+
+	len += sprintf(buf + len, "\nbcast_per:\t");
+	len += sprintf(buf + len,
+		"set broadcast storm percentage\n");
+	len += sprintf(buf + len, "mcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable multicast storm protection\n");
+	len += sprintf(buf + len, "diffserv_map:\t");
+	len += sprintf(buf + len,
+		"set DiffServ value.  Use \"decimal=hexadecimal\" format\n");
+	len += sprintf(buf + len, "p_802_1p_map:\t");
+	len += sprintf(buf + len,
+		"set 802.1p value.  Use \"decimal=hexadecimal\" format\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nvlan:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1Q VLAN\n");
+	len += sprintf(buf + len, "null_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to replace null vid\n");
+	len += sprintf(buf + len, "macaddr:\t");
+	len += sprintf(buf + len,
+		"set switch MAC address\n");
+	len += sprintf(buf + len, "mirror_mode:\t");
+	len += sprintf(buf + len,
+		"set to 1 to use mirror rx AND tx mode\n");
+
+	len += sprintf(buf + len, "\nigmp_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IGMP snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_option:");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD option snooping\n");
+
+	len += sprintf(buf + len, "\naggr_backoff:\t");
+	len += sprintf(buf + len,
+		"disable/enable aggressive backoff in half-duplex mode\n");
+	len += sprintf(buf + len, "no_exc_drop:\t");
+	len += sprintf(buf + len,
+		"disable/enable no excessive collision drop\n");
+	len += sprintf(buf + len, "buf_reserve:\t");
+	len += sprintf(buf + len,
+		"disable/enable buffer reserve\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nhuge_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable huge packet support\n");
+	len += sprintf(buf + len, "legal_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet\n");
+	len += sprintf(buf + len, "length_check:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet length check\n");
+
+	len += sprintf(buf + len, "\nback_pressure:\t");
+	len += sprintf(buf + len,
+		"set back pressure mode\n");
+	len += sprintf(buf + len, "sw_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port flow control\n");
+	len += sprintf(buf + len, "sw_half_duplex:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port half-duplex mode\n");
+#ifdef SWITCH_10_MBIT
+	len += sprintf(buf + len, "sw_10_mbit:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port 10Mbit mode\n");
+#endif
+	len += sprintf(buf + len, "rx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable receive flow control\n");
+	len += sprintf(buf + len, "tx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable transmit flow control\n");
+	len += sprintf(buf + len, "fair_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable fair flow control mode\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vlan_bound:\t");
+	len += sprintf(buf + len,
+		"disable/enable unicast VLAN boundary\n");
+
+	len += sprintf(buf + len, "pass_pause:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass PAUSE frames for debugging\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nswitch port settings:\n");
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len,
+		"display the port's duplex setting\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"display the port's link speed\n");
+	len += sprintf(buf + len, "linkmd:\t\t");
+	len += sprintf(buf + len,
+		"write to start LinkMD test.  read for result\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the port's MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set default VID value\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set VLAN membership\n");
+
+	len += sprintf(buf + len, "bcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable/enable broadcast storm protection\n");
+	len += sprintf(buf + len, "diffserv:\t");
+	len += sprintf(buf + len,
+		"disable/enable DiffServ priority\n");
+	len += sprintf(buf + len, "p_802_1p:\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1p priority\n");
+
+	len += sprintf(buf + len, "port_based:\t");
+	len += sprintf(buf + len,
+		"disable/enable port-based priority\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "prio_queue:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue\n");
+	len += sprintf(buf + len, "tx_p0_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 control\n");
+	len += sprintf(buf + len, "tx_p1_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 control\n");
+	len += sprintf(buf + len, "tx_p2_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 control\n");
+	len += sprintf(buf + len, "tx_p3_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 control\n");
+	len += sprintf(buf + len, "tx_p0_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 ratio\n");
+	len += sprintf(buf + len, "tx_p1_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 ratio\n");
+	len += sprintf(buf + len, "tx_p2_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 ratio\n");
+	len += sprintf(buf + len, "tx_p3_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 ratio\n");
+	len += sprintf(buf + len, "prio_rate:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue rate limiting\n");
+	len += sprintf(buf + len, "rx_limit:\t");
+	len += sprintf(buf + len,
+		"set rx rate limiting mode\n");
+	len += sprintf(buf + len, "cnt_ifg:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count IPG\n");
+	len += sprintf(buf + len, "cnt_pre:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count preamble\n");
+	len += sprintf(buf + len, "rx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 3 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 3 rate in 64Kbps unit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\npass_all:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass all frames for debugging\n");
+	len += sprintf(buf + len, "tail_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable tail tagging\n");
+	len += sprintf(buf + len, "rx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable rx\n");
+	len += sprintf(buf + len, "tx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable tx\n");
+	len += sprintf(buf + len, "learn:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable learning\n");
+
+	len += sprintf(buf + len, "mirror_port:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror port\n");
+	len += sprintf(buf + len, "mirror_rx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror receive\n");
+	len += sprintf(buf + len, "mirror_tx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror transmit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nnon_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to discard non-VID packets\n");
+	len += sprintf(buf + len, "ingress:\t");
+	len += sprintf(buf + len,
+		"disable/enable ingress VLAN filtering\n");
+	len += sprintf(buf + len, "drop_tagged:\t");
+	len += sprintf(buf + len,
+		"disable/enable drop tagged packet feature\n");
+	len += sprintf(buf + len, "replace_prio:\t");
+	len += sprintf(buf + len,
+		"disable/enable replace 802.1p priority feature\n");
+	len += sprintf(buf + len, "back_pressure:\t");
+	len += sprintf(buf + len,
+		"disable/enable back pressure in half-duplex mode\n");
+	len += sprintf(buf + len, "force_flow_ctrl:");
+	len += sprintf(buf + len,
+		"set to 1 to force flow control\n");
+	len += sprintf(buf + len, "\nmacaddr:\t");
+	len += sprintf(buf + len,
+		"set port MAC address\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nstatic MAC table:\n");
+	len += sprintf(buf + len, "addr:\t\t");
+	len += sprintf(buf + len,
+		"set MAC address\n");
+	len += sprintf(buf + len, "ports:\t\t");
+	len += sprintf(buf + len,
+		"set destination ports\n");
+	len += sprintf(buf + len, "override:\t");
+	len += sprintf(buf + len,
+		"set override bit\n");
+	len += sprintf(buf + len, "use_fid:\t");
+	len += sprintf(buf + len,
+		"set use FID bit\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	len += sprintf(buf + len, "\nVLAN table:\n");
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set VID\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set membership\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	return len;
+}  /* display_sw_info */
+
+static ssize_t sysfs_sw_read(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, ssize_t len, char *buf)
+{
+	int i;
+	int j;
+	u16 map;
+	struct ksz_sw_info *info = sw->info;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_sw_info(sw->mib_port_cnt, buf, len);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->duplex);
+		if (media_connected == port->linked->state) {
+			if (1 == port->linked->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port->linked->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->speed);
+		if (media_connected == port->linked->state)
+			len += sprintf(buf + len, "%u\n",
+				port->linked->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u\n", port->force_link);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->flow_ctrl);
+		switch (port->flow_ctrl) {
+		case PHY_FLOW_CTRL:
+			len += sprintf(buf + len, "flow control\n");
+			break;
+		case PHY_TX_ONLY:
+			len += sprintf(buf + len, "tx only\n");
+			break;
+		case PHY_RX_ONLY:
+			len += sprintf(buf + len, "rx only\n");
+			break;
+		default:
+			len += sprintf(buf + len, "no flow control\n");
+			break;
+		}
+		break;
+	case PROC_SET_SW_MIB:
+		if (!port)
+			break;
+		len += display_sw_mib_counters(sw, port->first_port,
+			port->mib_port_cnt, buf + len);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		len += sprintf(buf + len, "%u%%\n", info->broad_per);
+		break;
+	case PROC_SET_DIFFSERV:
+		for (i = 0; i < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * KS_PRIO_IN_REG);
+			map = info->diffserv[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->diffserv[i]);
+		}
+		break;
+	case PROC_SET_802_1P:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * KS_PRIO_IN_REG);
+			map = info->p_802_1p[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->p_802_1p[i]);
+		}
+		break;
+	case PROC_SET_SW_VID:
+		len += sprintf(buf + len, "0x%04x\n", sw->vid);
+		break;
+	case PROC_GET_HOST_PORT:
+		len += sprintf(buf + len, "%u\n", sw->HOST_PORT + 1);
+		break;
+	case PROC_GET_PORTS:
+		len += sprintf(buf + len, "%u\n", sw->mib_port_cnt);
+		break;
+	case PROC_GET_DEV_START:
+	{
+		int start = 0;
+
+		if (sw->dev_offset)
+			start = 100;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_VLAN_START:
+	{
+		int start = 0;
+
+		if (sw->features & VLAN_PORT)
+			start = VLAN_PORT_START;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_STP:
+		len += sprintf(buf + len, "%u\n",
+			!!(sw->features & STP_SUPPORT));
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", sw->features);
+		len += sprintf(buf + len, "\t%08x = STP support\n",
+			STP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port forwarding\n",
+			VLAN_PORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port remove tag\n",
+			VLAN_PORT_REMOVE_TAG);
+		len += sprintf(buf + len, "\t%08x = VLAN port tag tailing\n",
+			VLAN_PORT_TAGGING);
+		len += sprintf(buf + len, "\t%08x = MRP support\n",
+			MRP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = IBA support\n",
+			IBA_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = new capabilities\n",
+			NEW_CAP);
+		len += sprintf(buf + len, "\t%08x = AVB support\n",
+			AVB_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = Redundancy support\n",
+			REDUNDANCY_SUPPORT);
+#ifdef KSZ_DLR
+		len += sprintf(buf + len, "\t%08x = DLR\n",
+			DLR_HW);
+#endif
+#ifdef KSZ_HSR
+		len += sprintf(buf + len, "\t%08x = HSR\n",
+			HSR_HW);
+#endif
+		len += sprintf(buf + len, "\t%08x = DSA support\n",
+			DSA_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = different MAC addresses\n",
+			DIFF_MAC_ADDR);
+		len += sprintf(buf + len, "\t%08x = QuietWire\n",
+			QW_HW);
+#ifdef CONFIG_1588_PTP
+		len += sprintf(buf + len, "\t%08x = 1588 PTP\n",
+			PTP_HW);
+#endif
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", sw->overrides);
+		len += sprintf(buf + len, "\t%08x = flow control\n",
+			PAUSE_FLOW_CTRL);
+		len += sprintf(buf + len, "\t%08x = fast aging\n",
+			FAST_AGING);
+#ifdef KSZ_IBA
+		len += sprintf(buf + len, "\t%08x = IBA test\n",
+			IBA_TEST);
+#endif
+		len += sprintf(buf + len, "\t%08x = ACL intr monitor\n",
+			ACL_INTR_MONITOR);
+		len += sprintf(buf + len, "\t%08x = ptp tag\n",
+			PTP_TAG);
+		len += sprintf(buf + len, "\t%08x = tag is removed\n",
+			TAG_REMOVE);
+		len += sprintf(buf + len, "\t%08x = tail tagging\n",
+			TAIL_TAGGING);
+		break;
+	case PROC_DYNAMIC:
+		sw_d_dyn_mac_table(sw);
+		break;
+	case PROC_STATIC:
+		sw_d_sta_mac_table(sw);
+		sw_d_mac_table(sw);
+		break;
+	case PROC_VLAN:
+		sw_d_vlan_table(sw);
+		break;
+	}
+	return len;
+}  /* sysfs_sw_read */
+
+static ssize_t sysfs_sw_read_hw(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	u8 data[8];
+	u32 val;
+
+	switch (proc_num) {
+	case PROC_SET_AGING:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_LUE_CTRL_1,
+				SW_AGING_ENABLE));
+		break;
+	case PROC_SET_FAST_AGING:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_LUE_CTRL_1,
+				SW_FAST_AGING));
+		break;
+	case PROC_SET_LINK_AGING:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, S_LINK_AGING_CTRL,
+				SW_LINK_AUTO_AGING));
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		len += sprintf(buf + len, "%u\n",
+			!sw_chk(sw, REG_SW_MAC_CTRL_1,
+				MULTICAST_STORM_DISABLE));
+		break;
+	case PROC_ENABLE_VLAN:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_LUE_CTRL_0,
+				SW_VLAN_ENABLE));
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, S_REPLACE_VID_CTRL,
+				SW_REPLACE_VID));
+		break;
+	case PROC_SET_DROP_INVALID_VID:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_LUE_CTRL_0,
+				SW_DROP_INVALID_VID));
+		break;
+	case PROC_SET_MAC_ADDR:
+		sw_get_addr(sw, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X\n",
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		break;
+	case PROC_SET_MIRROR_MODE:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk_mirror_rx_tx(sw));
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, S_MIRROR_CTRL,
+				SW_IGMP_SNOOP));
+		break;
+#ifdef SW_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, S_MIRROR_CTRL,
+				SW_IPV6_MLD_SNOOP));
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, S_MIRROR_CTRL,
+				SW_IPV6_MLD_OPTION));
+		break;
+#endif
+	case PROC_SET_AGGR_BACKOFF:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_0,
+				SW_AGGR_BACKOFF));
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_1,
+				NO_EXC_COLLISION_DROP));
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_QM_CTRL,
+				UNICAST_VLAN_BOUNDARY));
+		break;
+	case PROC_SET_DOUBLE_TAG:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_OPERATION,
+				SW_DOUBLE_TAG));
+		break;
+	case PROC_SET_ISP_TAG:
+		len += sprintf(buf + len, "0x%04x\n",
+			sw->reg->r16(sw, REG_SW_ISP_TPID__2));
+		break;
+	case PROC_SET_HSR_TAG:
+		len += sprintf(buf + len, "0x%04x\n",
+			sw->reg->r16(sw, REG_SW_HSR_TPID__2));
+		break;
+	case PROC_SET_MTU:
+		len += sprintf(buf + len, "%u\n",
+			sw->reg->r16(sw, REG_SW_MTU__2));
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val = !!(val & SW_UNK_UNICAST_ENABLE);
+		len += sprintf(buf + len, "%u\n", val);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val &= ~SW_UNK_UNICAST_ENABLE;
+		len += sprintf(buf + len, "0x%x\n", val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val = !!(val & SW_UNK_MULTICAST_ENABLE);
+		len += sprintf(buf + len, "%u\n", val);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val &= ~SW_UNK_MULTICAST_ENABLE;
+		len += sprintf(buf + len, "0x%x\n", val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val = !!(val & SW_UNK_VID_ENABLE);
+		len += sprintf(buf + len, "%u\n", val);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val &= ~SW_UNK_VID_ENABLE;
+		len += sprintf(buf + len, "0x%x\n", val);
+		break;
+	case PROC_SET_JUMBO_PACKET:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_1,
+				SW_JUMBO_PACKET));
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		len += sprintf(buf + len, "%u\n",
+			!sw_chk(sw, REG_SW_MAC_CTRL_1,
+				SW_LEGAL_PACKET_DISABLE));
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_0,
+				SW_CHECK_LENGTH));
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_1,
+				SW_BACK_PRESSURE));
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_1,
+				FAIR_FLOW_CTRL));
+		break;
+	case PROC_SET_PASS_PAUSE:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_MAC_CTRL_4,
+				SW_PASS_PAUSE));
+		break;
+	case PROC_ENABLE_PME:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_PME_CTRL,
+				PME_ENABLE));
+		break;
+	case PROC_ENABLE_PME_POLARITY:
+		len += sprintf(buf + len, "%u\n",
+			sw_chk(sw, REG_SW_PME_CTRL,
+				PME_POLARITY));
+		break;
+	case PROC_SET_NO_COLOR:
+		len += sprintf(buf + len, "%u\n",
+			sw_r_shift(sw, REG_SW_MRI_CTRL_8,
+				SW_COLOR_M, SW_NO_COLOR_S));
+		break;
+	case PROC_SET_COLOR_RED:
+		len += sprintf(buf + len, "%u\n",
+			sw_r_shift(sw, REG_SW_MRI_CTRL_8,
+				SW_COLOR_M, SW_RED_COLOR_S));
+		break;
+	case PROC_SET_COLOR_YELLOW:
+		len += sprintf(buf + len, "%u\n",
+			sw_r_shift(sw, REG_SW_MRI_CTRL_8,
+				SW_COLOR_M, SW_YELLOW_COLOR_S));
+		break;
+	case PROC_SET_COLOR_GREEN:
+		len += sprintf(buf + len, "%u\n",
+			sw_r_shift(sw, REG_SW_MRI_CTRL_8,
+				SW_COLOR_M, SW_GREEN_COLOR_S));
+		break;
+	}
+	return len;
+}  /* sysfs_sw_read_hw */
+
+static int sysfs_sw_write(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, int num, const char *buf)
+{
+	int changes;
+	int count;
+	unsigned int val;
+	u8 data[8];
+	int processed = true;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		sw_init(sw);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		if (num <= 2)
+			port->duplex = (u8) num;
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		if (0 == num || 10 == num || 100 == num || 1000 == num)
+			port->speed = (u16) num;
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		port->force_link = (u8) num;
+		if (port->force_link)
+			port_force_link_speed(port);
+		else
+			port_set_link_speed(port);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		if (num <= PHY_RX_ONLY)
+			port->flow_ctrl = (u8) num;
+		break;
+	case PROC_SET_SW_MIB:
+		for (count = 0; count < sw->mib_port_cnt; count++) {
+			struct ksz_port_mib *mib = &sw->port_mib[count];
+
+			memset((void *) mib->counter, 0, sizeof(u64) *
+				TOTAL_SWITCH_COUNTER_NUM);
+		}
+		break;
+	case PROC_SET_SW_REG:
+		count = sscanf(buf, "%x=%x", (unsigned int *) &num, &val);
+		if (1 == count)
+			printk(KERN_INFO SW_SIZE_STR"\n",
+				SW_R(sw, num));
+		else if (2 == count)
+			SW_W(sw, num, val);
+		break;
+	case PROC_SET_SW_VID:
+		sw->vid = num;
+		break;
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = sw->features ^ num;
+		sw->features = num;
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		sw->overrides = num;
+		break;
+	case PROC_DYNAMIC:
+		sw_flush_dyn_mac_table(sw, sw->mib_port_cnt);
+		break;
+	case PROC_STATIC:
+		sw_clr_sta_mac_table(sw);
+		break;
+	case PROC_SET_AGING:
+		sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_AGING_ENABLE, num);
+		break;
+	case PROC_SET_FAST_AGING:
+		sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING, num);
+		break;
+	case PROC_SET_LINK_AGING:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING, num);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		hw_cfg_broad_storm(sw, num);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE,
+			!num);
+		break;
+	case PROC_SET_DIFFSERV:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_tos_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_SET_802_1P:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_802_1p_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_ENABLE_VLAN:
+		if (!num)
+			sw_dis_vlan(sw);
+		else
+			sw_ena_vlan(sw);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID, num);
+		break;
+	case PROC_SET_DROP_INVALID_VID:
+		sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_DROP_INVALID_VID, num);
+		break;
+	case PROC_SET_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			sw_set_addr(sw, data);
+		}
+		break;
+	}
+	case PROC_SET_MIRROR_MODE:
+		sw_cfg_mirror_rx_tx(sw, num);
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP, num);
+		break;
+#ifdef SW_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IPV6_MLD_SNOOP, num);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IPV6_MLD_OPTION, num);
+		break;
+#endif
+	case PROC_SET_AGGR_BACKOFF:
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_AGGR_BACKOFF, num);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, NO_EXC_COLLISION_DROP, num);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		sw_cfg(sw, REG_SW_QM_CTRL, UNICAST_VLAN_BOUNDARY, num);
+		break;
+	case PROC_SET_DOUBLE_TAG:
+		sw_cfg(sw, REG_SW_OPERATION, SW_DOUBLE_TAG, num);
+		break;
+	case PROC_SET_ISP_TAG:
+		sw->reg->w16(sw, REG_SW_ISP_TPID__2, (u16) num);
+		break;
+	case PROC_SET_HSR_TAG:
+		sw->reg->w16(sw, REG_SW_HSR_TPID__2, (u16) num);
+		break;
+	case PROC_SET_MTU:
+		if (2000 <= num && num <= 9000)
+			sw->reg->w16(sw, REG_SW_MTU__2, (u16) num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		sw_cfg(sw, REG_SW_LUE_UNK_UCAST_CTRL__4,
+			SW_UNK_UNICAST_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORTS:
+		num &= ~SW_UNK_UNICAST_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val &= SW_UNK_UNICAST_ENABLE;
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4, val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		sw_cfg(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			SW_UNK_MULTICAST_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORTS:
+		num &= ~SW_UNK_MULTICAST_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val &= SW_UNK_MULTICAST_ENABLE;
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4, val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		sw_cfg(sw, REG_SW_LUE_UNK_VID_CTRL__4,
+			SW_UNK_VID_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORTS:
+		num &= ~SW_UNK_VID_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val &= SW_UNK_VID_ENABLE;
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_VID_CTRL__4, val);
+		break;
+	case PROC_SET_JUMBO_PACKET:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_JUMBO_PACKET, num);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE,
+			!num);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH, num);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_BACK_PRESSURE, num);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, FAIR_FLOW_CTRL, num);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		sw_cfg(sw, REG_SW_MAC_CTRL_4, SW_PASS_PAUSE, num);
+		break;
+	case PROC_ENABLE_PME:
+		sw_cfg(sw, REG_SW_PME_CTRL, PME_ENABLE, num);
+		break;
+	case PROC_ENABLE_PME_POLARITY:
+		sw_cfg(sw, REG_SW_PME_CTRL, PME_POLARITY, num);
+		break;
+	case PROC_SET_NO_COLOR:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_NO_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_RED:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_RED_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_YELLOW:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_YELLOW_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_GREEN:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_GREEN_COLOR_S, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_sw_write */
+
+static ssize_t sysfs_port_read(struct ksz_sw *sw, int proc_num, int port,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *port_cfg;
+	struct ksz_port_info *port_info;
+	int i;
+	int j;
+	u32 map;
+
+	port_cfg = &sw->info->port_cfg[port];
+	port_info = &sw->port_info[port];
+	switch (proc_num) {
+	case PROC_GET_PORT_DUPLEX:
+		if (media_connected == port_info->state) {
+			if (1 == port_info->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port_info->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_GET_PORT_SPEED:
+		if (media_connected == port_info->state)
+			len += sprintf(buf + len, "%u\n",
+				port_info->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_MIB:
+		len += display_sw_mib_counters(sw, port, 1, buf + len);
+		break;
+	case PROC_SET_LINK_MD:
+		len += sprintf(buf + len, "%u:%u %u:%u %u:%u\n",
+			port_info->length[0], port_info->status[0],
+			port_info->length[1], port_info->status[1],
+			port_info->length[2], port_info->status[2]);
+		break;
+	case PROC_SET_PORT_BASED:
+		len += sprintf(buf + len, "%u\n",
+			port_cfg->port_prio);
+		break;
+	case PROC_SET_DEF_VID:
+		len += sprintf(buf + len, "0x%04x\n",
+			port_cfg->vid);
+		break;
+	case PROC_SET_MEMBER:
+		len += sprintf(buf + len, "0x%02x\n",
+			port_cfg->member);
+		break;
+	case PROC_SET_LIMIT:
+		len += sprintf(buf + len, "%u\n",
+			((port_cfg->rate_limit >> PORT_IN_LIMIT_MODE_S) &
+			PORT_IN_LIMIT_MODE_M));
+		break;
+	case PROC_SET_LIMIT_PORT_BASED:
+		len += sprintf(buf + len, "%u\n",
+			((port_cfg->rate_limit >> PORT_IN_PORT_BASED_S)
+			& 1));
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		len += sprintf(buf + len, "%u\n",
+			((port_cfg->rate_limit >> PORT_IN_PACKET_BASED_S)
+			& 1));
+		break;
+	case PROC_SET_LIMIT_FLOW_CTRL:
+		len += sprintf(buf + len, "%u\n",
+			((port_cfg->rate_limit >> PORT_IN_FLOW_CTRL_S)
+			& 1));
+		break;
+	case PROC_SET_LIMIT_CNT_IFG:
+		len += sprintf(buf + len, "%u\n",
+			((port_cfg->rate_limit >> PORT_COUNT_IFG_S)
+			& 1));
+		break;
+	case PROC_SET_LIMIT_CNT_PRE:
+		len += sprintf(buf + len, "%u\n",
+			((port_cfg->rate_limit >> PORT_COUNT_PREAMBLE_S)
+			& 1));
+		break;
+	case PROC_SET_RX_P0_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[0] : port_cfg->rx_rate[0],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P1_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[1] : port_cfg->rx_rate[1],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P2_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[2] : port_cfg->rx_rate[2],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P3_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[3] : port_cfg->rx_rate[3],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P4_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[4] : port_cfg->rx_rate[4],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P5_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[5] : port_cfg->rx_rate[5],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P6_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[6] : port_cfg->rx_rate[6],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_RX_P7_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[7] : port_cfg->rx_rate[7],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[0] : port_cfg->tx_rate[0],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[1] : port_cfg->tx_rate[1],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[2] : port_cfg->tx_rate[2],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[3] : port_cfg->tx_rate[3],
+			port_cfg->packet_based ? "pbs" : "bbs");
+		break;
+	case PROC_SET_COLOR_MAP:
+		for (i = 0; i < DIFFSERV_ENTRIES / 16; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * 16);
+			map = port_cfg->color_map[i];
+			for (j = 0; j < 16; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & POLICE_COLOR_MAP_M);
+				map >>= POLICE_COLOR_MAP_S;
+			}
+			len += sprintf(buf + len, "\t%08x\n",
+				port_cfg->color_map[i]);
+		}
+		break;
+	case PROC_SET_TC_MAP:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / 8; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * 8);
+			map = port_cfg->tc_map[i];
+			for (j = 0; j < 8; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & PORT_TC_MAP_M);
+				map >>= PORT_TC_MAP_S;
+			}
+			len += sprintf(buf + len, "\t%08x\n",
+				port_cfg->tc_map[i]);
+		}
+		break;
+	case PROC_SET_MMD_ID:
+		len += sprintf(buf + len, "0x%x\n", port_cfg->mmd_id);
+		break;
+	case PROC_SET_MMD_REG:
+		len += sprintf(buf + len, "0x%x\n", port_cfg->mmd_reg);
+		break;
+	}
+	return len;
+}  /* sysfs_port_read */
+
+static ssize_t sysfs_port_read_hw(struct ksz_sw *sw, int proc_num, int port,
+	ssize_t len, char *buf)
+{
+	u16 val;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	switch (proc_num) {
+	case PROC_ENABLE_BROADCAST_STORM:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_broad_storm(sw, port));
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_diffserv(sw, port));
+		break;
+	case PROC_ENABLE_802_1P:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_802_1p(sw, port));
+		break;
+	case PROC_ENABLE_VLAN_PRIO:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_vlan_prio(sw, port));
+		break;
+	case PROC_ENABLE_MAC_PRIO:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_mac_prio(sw, port));
+		break;
+	case PROC_ENABLE_ACL_PRIO:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_acl_prio(sw, port));
+		break;
+	case PROC_SET_HIGHEST_PRIO:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_highest_prio(sw, port));
+		break;
+	case PROC_SET_OR_PRIO:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_or_prio(sw, port));
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		len += sprintf(buf + len, "%d\n",
+			port_chk_prio(sw, port));
+		break;
+	case PROC_SET_REPLACE_VID:
+		len += sprintf(buf + len, "%u\n",
+			port_chk32(sw, port, REG_PORT_MTI_QUEUE_CTRL_0__4,
+				MTI_PVID_REPLACE));
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_replace_prio(sw, port));
+		break;
+	case PROC_ENABLE_PRIO_RATE:
+		len += sprintf(buf + len, "%d\n",
+			sw_chk_prio_rate(sw, port));
+		break;
+	case PROC_SET_DROP_NON_VLAN:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_drop_non_vlan(sw, port));
+		break;
+	case PROC_SET_DROP_TAG:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_drop_tag(sw, port));
+		break;
+	case PROC_SET_MIRROR_PORT:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_mirror_sniffer(sw, port));
+		break;
+	case PROC_SET_MIRROR_RX:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_mirror_rx(sw, port));
+		break;
+	case PROC_SET_MIRROR_TX:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_mirror_tx(sw, port));
+		break;
+	case PROC_SET_DIS_NON_VID:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_dis_non_vid(sw, port));
+		break;
+	case PROC_SET_INGRESS:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_in_filter(sw, port));
+		break;
+	case PROC_SET_SRC_ADDR_FILTER:
+		len += sprintf(buf + len, "%u\n",
+			port_chk(sw, port, REG_PORT_LUE_CTRL,
+				PORT_SRC_ADDR_FILTER));
+		break;
+	case PROC_SET_VLAN_LOOKUP_0:
+		len += sprintf(buf + len, "%u\n",
+			port_chk(sw, port, REG_PORT_LUE_CTRL,
+				PORT_VLAN_LOOKUP_VID_0));
+		break;
+	case PROC_SET_MSTP:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_mstp(sw, port));
+		break;
+	case PROC_SET_RX:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_rx(sw, port));
+		break;
+	case PROC_SET_TX:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_tx(sw, port));
+		break;
+	case PROC_SET_LEARN:
+		len += sprintf(buf + len, "%u\n",
+			!port_chk_dis_learn(sw, port));
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_back_pressure(sw, port));
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_force_flow_ctrl(sw, port));
+		break;
+	case PROC_SET_PASS_ALL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk(sw, port, REG_PORT_MAC_CTRL_1,
+				PORT_PASS_ALL));
+		break;
+	case PROC_SET_TAIL_TAG:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_tail_tag(sw, port));
+		break;
+	case PROC_SET_CUSTOM_VID:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_CUSTOM_VID, &data);
+		len += sprintf(buf + len, "%04x\n", data);
+		break;
+	}
+	case PROC_SET_SR_1_VID:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_1_VID, &data);
+		len += sprintf(buf + len, "%04x\n", data);
+		break;
+	}
+	case PROC_SET_SR_2_VID:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_2_VID, &data);
+		len += sprintf(buf + len, "%04x\n", data);
+		break;
+	}
+	case PROC_SET_SR_1_TYPE:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_1_TYPE, &data);
+		len += sprintf(buf + len, "%04x\n", data);
+		break;
+	}
+	case PROC_SET_SR_2_TYPE:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_2_TYPE, &data);
+		len += sprintf(buf + len, "%04x\n", data);
+		break;
+	}
+	case PROC_SET_PME_CTRL:
+		len += sprintf(buf + len, "%u\n",
+			port_r_s(sw, port, REG_PORT_PME_CTRL, 7, 0));
+		break;
+	case PROC_SET_PME_STATUS:
+		len += sprintf(buf + len, "%u\n",
+			port_r_s(sw, port, REG_PORT_PME_STATUS, 7, 0));
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_authen_mode(sw, port));
+		break;
+	case PROC_SET_ACL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_acl(sw, port));
+		break;
+	case PROC_SET_P_INDEX:
+		len += sprintf(buf + len, "%u\n",
+			sw->info->port_cfg[port].p_index);
+		break;
+	case PROC_SET_Q_INDEX:
+		len += sprintf(buf + len, "%u\n",
+			sw->info->port_cfg[port].q_index);
+		break;
+	case PROC_SET_POLICE_PACKET_TYPE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_police_packet_type(sw, port));
+		break;
+	case PROC_SET_NON_DSCP_COLOR:
+		len += sprintf(buf + len, "%u\n",
+			port_get_non_dscp_color(sw, port));
+		break;
+	case PROC_ENABLE_PORT_BASED_POLICING:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_port_based_policing(sw, port));
+		break;
+	case PROC_ENABLE_POLICE_DROP_ALL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_police_drop_all(sw, port));
+		break;
+	case PROC_ENABLE_COLOR_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_color_mark(sw, port));
+		break;
+	case PROC_ENABLE_COLOR_REMAP:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_color_remap(sw, port));
+		break;
+	case PROC_ENABLE_DROP_SRP:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_drop_srp(sw, port));
+		break;
+	case PROC_ENABLE_COLOR_AWARE:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_color_aware(sw, port));
+		break;
+	case PROC_ENABLE_POLICE:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_police(sw, port));
+		break;
+	case PROC_SET_Q_CIR:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_cir(sw, port));
+		break;
+	case PROC_SET_Q_PIR:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_pir(sw, port));
+		break;
+	case PROC_SET_Q_CBS:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_cbs(sw, port));
+		break;
+	case PROC_SET_Q_PBS:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_pbs(sw, port));
+		break;
+	case PROC_SET_WRED_MAX_THRESHOLD:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_max(sw, port));
+		break;
+	case PROC_SET_WRED_MIN_THRESHOLD:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_min(sw, port));
+		break;
+	case PROC_SET_WRED_MULTIPLIER:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_multiplier(sw, port));
+		break;
+	case PROC_GET_WRED_AVG_SIZE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_avg_size(sw, port));
+		break;
+	case PROC_SET_WRED_Q_MAX_THRESHOLD:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_max(sw, port));
+		break;
+	case PROC_SET_WRED_Q_MIN_THRESHOLD:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_min(sw, port));
+		break;
+	case PROC_SET_WRED_Q_MULTIPLIER:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_multiplier(sw, port));
+		break;
+	case PROC_GET_WRED_Q_AVG_SIZE:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_avg_size(sw, port));
+		break;
+	case PROC_SET_WRED_RANDOM_DROP:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_random_drop(sw, port));
+		break;
+	case PROC_SET_WRED_DROP_GYR:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_gyr(sw, port));
+		break;
+	case PROC_SET_WRED_DROP_YR:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_yr(sw, port));
+		break;
+	case PROC_SET_WRED_DROP_R:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_r(sw, port));
+		break;
+	case PROC_SET_WRED_DROP_ALL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_all(sw, port));
+		break;
+	case PROC_GET_WRED_PMON:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_pmon(sw, port));
+		break;
+	case PROC_SET_SCHEDULE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_schedule_mode(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_SET_SHAPING:
+		len += sprintf(buf + len, "%u\n",
+			port_get_shaping(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+#ifdef MTI_PREEMPT_ENABLE
+	case PROC_SET_PREEMPT:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_preempt(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+#endif
+	case PROC_SET_TX_RATIO:
+		len += sprintf(buf + len, "%u\n",
+			port_get_tx_ratio(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_SET_CREDIT_HI_WATER_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_get_hi_water_mark(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_SET_CREDIT_LO_WATER_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_get_lo_water_mark(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_SET_CREDIT_INCREMENT:
+		len += sprintf(buf + len, "%u\n",
+			port_get_increment(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_SET_SRP:
+		len += sprintf(buf + len, "%u\n",
+			port_get_srp(sw, port));
+		break;
+	case PROC_SET_QM_DROP:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_drop(sw, port));
+		break;
+	case PROC_SET_QM_BURST_SIZE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_burst_size(sw, port));
+		break;
+	case PROC_SET_QM_RESV_SPACE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_resv_space(sw, port));
+		break;
+	case PROC_SET_QM_HI_WATER_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_hi_water_mark(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_SET_QM_LO_WATER_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_lo_water_mark(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_GET_QM_TX_USED:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_tx_used(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_GET_QM_TX_AVAIL:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_tx_avail(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_GET_QM_TX_CALCULATED:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_tx_calculated(sw, port,
+				sw->info->port_cfg[port].q_index));
+		break;
+	case PROC_GET_RX_FLOW_CTRL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk(sw, port, REG_PORT_STATUS_0,
+				PORT_RX_FLOW_CTRL));
+		break;
+	case PROC_GET_TX_FLOW_CTRL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk(sw, port, REG_PORT_STATUS_0,
+				PORT_TX_FLOW_CTRL));
+		break;
+	case PROC_SET_MMD_VAL:
+		port_mmd_read(sw, port, cfg->mmd_id, cfg->mmd_reg, &val, 1);
+		len += sprintf(buf + len, "0x%04x\n", val);
+		break;
+	}
+	return len;
+}  /* sysfs_port_read_hw */
+
+static int sysfs_port_write(struct ksz_sw *sw, int proc_num, int port,
+	int num, const char *buf)
+{
+	int count;
+	unsigned int val;
+	u16 mmd_val;
+	int processed = true;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	switch (proc_num) {
+	case PROC_SET_PORT_MIB:
+	{
+		struct ksz_port_mib *mib = &sw->port_mib[port];
+
+		memset((void *) mib->counter, 0, sizeof(u64) *
+			TOTAL_SWITCH_COUNTER_NUM);
+		break;
+	}
+	case PROC_ENABLE_BROADCAST_STORM:
+		if (!num)
+			sw_dis_broad_storm(sw, port);
+		else
+			sw_ena_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		if (!num)
+			sw_dis_diffserv(sw, port);
+		else
+			sw_ena_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		if (!num)
+			sw_dis_802_1p(sw, port);
+		else
+			sw_ena_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_VLAN_PRIO:
+		port_cfg_vlan_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_MAC_PRIO:
+		port_cfg_mac_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_ACL_PRIO:
+		port_cfg_acl_prio(sw, port, num);
+		break;
+	case PROC_SET_HIGHEST_PRIO:
+		port_cfg_highest_prio(sw, port, num);
+		break;
+	case PROC_SET_OR_PRIO:
+		port_cfg_or_prio(sw, port, num);
+		break;
+	case PROC_SET_PORT_BASED:
+		sw_cfg_port_based(sw, port, num);
+		break;
+	case PROC_SET_DEF_VID:
+		sw_cfg_def_vid(sw, port, num);
+		break;
+	case PROC_SET_MEMBER:
+		sw_cfg_port_base_vlan(sw, port, (u16) num);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		if (0 <= num && num < 2)
+			sw_set_multi_queue(sw, port, num);
+		break;
+	case PROC_SET_REPLACE_VID:
+		sw_cfg_replace_null_vid(sw, port, num);
+		break;
+	case PROC_SET_LIMIT:
+		hw_cfg_rx_limit(sw, port, (u8) num);
+		break;
+	case PROC_SET_LIMIT_PORT_BASED:
+		hw_cfg_in_port_based(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		hw_cfg_in_packet_based(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_FLOW_CTRL:
+		hw_cfg_in_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_CNT_IFG:
+		hw_cfg_cnt_ifg(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_CNT_PRE:
+		hw_cfg_cnt_pre(sw, port, num);
+		break;
+	case PROC_SET_RX_P0_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_RX_P1_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_RX_P2_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_RX_P3_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_RX_P4_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 4, num);
+		break;
+	case PROC_SET_RX_P5_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 5, num);
+		break;
+	case PROC_SET_RX_P6_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 6, num);
+		break;
+	case PROC_SET_RX_P7_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 7, num);
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		sw_cfg_replace_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_PRIO_RATE:
+		if (!num)
+			sw_dis_prio_rate(sw, port);
+		else
+			sw_ena_prio_rate(sw, port);
+		break;
+#if 0
+	case PROC_SET_LINK_MD:
+		hw_get_link_md(sw, port);
+		break;
+#endif
+	case PROC_SET_COLOR_MAP:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			port_cfg_color_map(sw, port, (u8) num, (u32) val);
+		break;
+	case PROC_SET_TC_MAP:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			port_cfg_tc_map(sw, port, (u8) num, (u32) val);
+		break;
+	case PROC_SET_DROP_NON_VLAN:
+		port_cfg_drop_non_vlan(sw, port, num);
+		break;
+	case PROC_SET_DROP_TAG:
+		port_cfg_drop_tag(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		port_cfg_mirror_sniffer(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_RX:
+		port_cfg_mirror_rx(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_TX:
+		port_cfg_mirror_tx(sw, port, num);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		port_cfg_dis_non_vid(sw, port, num);
+		break;
+	case PROC_SET_INGRESS:
+		port_cfg_in_filter(sw, port, num);
+		break;
+	case PROC_SET_SRC_ADDR_FILTER:
+		port_cfg(sw, port, REG_PORT_LUE_CTRL, PORT_SRC_ADDR_FILTER,
+			num);
+		break;
+	case PROC_SET_VLAN_LOOKUP_0:
+		port_cfg(sw, port, REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+			num);
+		break;
+	case PROC_SET_MSTP:
+		port_cfg_mstp(sw, port, num);
+		break;
+	case PROC_SET_RX:
+		port_cfg_rx(sw, port, num);
+		break;
+	case PROC_SET_TX:
+		port_cfg_tx(sw, port, num);
+		break;
+	case PROC_SET_LEARN:
+		port_cfg_dis_learn(sw, port, !num);
+#if 1
+		if (!num)
+			sw_flush_dyn_mac_table(sw, port);
+#endif
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		port_cfg_back_pressure(sw, port, num);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		port_cfg_force_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_PASS_ALL:
+		port_cfg(sw, port, REG_PORT_MAC_CTRL_1, PORT_PASS_ALL, num);
+		break;
+	case PROC_SET_TAIL_TAG:
+		port_cfg_tail_tag(sw, port, num);
+		break;
+	case PROC_SET_CUSTOM_VID:
+		port_w16(sw, port, REG_PORT_CUSTOM_VID, (u16) num);
+		break;
+	case PROC_SET_SR_1_VID:
+		port_w16(sw, port, REG_PORT_AVB_SR_1_VID, (u16) num);
+		break;
+	case PROC_SET_SR_2_VID:
+		port_w16(sw, port, REG_PORT_AVB_SR_2_VID, (u16) num);
+		break;
+	case PROC_SET_SR_1_TYPE:
+		port_w16(sw, port, REG_PORT_AVB_SR_1_TYPE, (u16) num);
+		break;
+	case PROC_SET_SR_2_TYPE:
+		port_w16(sw, port, REG_PORT_AVB_SR_2_TYPE, (u16) num);
+		break;
+	case PROC_SET_PME_CTRL:
+		port_w_s(sw, port, REG_PORT_PME_CTRL, 7, 0, (u8) num);
+		break;
+	case PROC_SET_PME_STATUS:
+		port_w_s(sw, port, REG_PORT_PME_STATUS, 7, 0, (u8) num);
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		port_set_authen_mode(sw, port, num);
+		break;
+	case PROC_SET_ACL:
+		port_cfg_acl(sw, port, num);
+		break;
+	case PROC_SET_P_INDEX:
+		if (0 <= num && num < sw->mib_port_cnt)
+			sw->info->port_cfg[port].p_index = (u8) num;
+		break;
+	case PROC_SET_Q_INDEX:
+		if (0 <= num && num < 4)
+			sw->info->port_cfg[port].q_index = (u8) num;
+		break;
+	case PROC_SET_POLICE_PACKET_TYPE:
+		port_set_police_packet_type(sw, port, num);
+		break;
+	case PROC_SET_NON_DSCP_COLOR:
+		port_set_non_dscp_color(sw, port, num);
+		break;
+	case PROC_ENABLE_PORT_BASED_POLICING:
+		port_cfg_port_based_policing(sw, port, num);
+		break;
+	case PROC_ENABLE_POLICE_DROP_ALL:
+		port_cfg_police_drop_all(sw, port, num);
+		break;
+	case PROC_ENABLE_COLOR_MARK:
+		port_cfg_color_mark(sw, port, num);
+		break;
+	case PROC_ENABLE_COLOR_REMAP:
+		port_cfg_color_remap(sw, port, num);
+		break;
+	case PROC_ENABLE_DROP_SRP:
+		port_cfg_drop_srp(sw, port, num);
+		break;
+	case PROC_ENABLE_COLOR_AWARE:
+		port_cfg_color_aware(sw, port, num);
+		break;
+	case PROC_ENABLE_POLICE:
+		port_cfg_police(sw, port, num);
+		break;
+	case PROC_SET_Q_CIR:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_cir(sw, port, num);
+		break;
+	case PROC_SET_Q_PIR:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_pir(sw, port, num);
+		break;
+	case PROC_SET_Q_CBS:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_cbs(sw, port, num);
+		break;
+	case PROC_SET_Q_PBS:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_pbs(sw, port, num);
+		break;
+	case PROC_SET_WRED_MAX_THRESHOLD:
+		port_set_wred_max(sw, port, num);
+		break;
+	case PROC_SET_WRED_MIN_THRESHOLD:
+		port_set_wred_min(sw, port, num);
+		break;
+	case PROC_SET_WRED_MULTIPLIER:
+		port_set_wred_multiplier(sw, port, num);
+		break;
+	case PROC_SET_WRED_Q_MAX_THRESHOLD:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_wred_q_max(sw, port, num);
+		break;
+	case PROC_SET_WRED_Q_MIN_THRESHOLD:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_wred_q_min(sw, port, num);
+		break;
+	case PROC_SET_WRED_Q_MULTIPLIER:
+		port_cfg_index(sw, port,
+			sw->info->port_cfg[port].p_index,
+			sw->info->port_cfg[port].q_index);
+		port_set_wred_q_multiplier(sw, port, num);
+		break;
+	case PROC_SET_WRED_RANDOM_DROP:
+		port_cfg_wred_random_drop(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_GYR:
+		port_cfg_wred_drop_gyr(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_YR:
+		port_cfg_wred_drop_yr(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_R:
+		port_cfg_wred_drop_r(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_ALL:
+		port_cfg_wred_drop_all(sw, port, num);
+		break;
+	case PROC_SET_SCHEDULE:
+		port_set_schedule_mode(sw, port,
+			sw->info->port_cfg[port].q_index, (u8) num);
+		break;
+	case PROC_SET_SHAPING:
+		port_set_shaping(sw, port,
+			sw->info->port_cfg[port].q_index, (u8) num);
+		break;
+#ifdef MTI_PREEMPT_ENABLE
+	case PROC_SET_PREEMPT:
+		port_cfg_preempt(sw, port,
+			sw->info->port_cfg[port].q_index, num);
+		break;
+#endif
+	case PROC_SET_TX_RATIO:
+		port_set_tx_ratio(sw, port,
+			sw->info->port_cfg[port].q_index, (u8) num);
+		break;
+	case PROC_SET_CREDIT_HI_WATER_MARK:
+		port_set_hi_water_mark(sw, port,
+			sw->info->port_cfg[port].q_index, (u16) num);
+		break;
+	case PROC_SET_CREDIT_LO_WATER_MARK:
+		port_set_lo_water_mark(sw, port,
+			sw->info->port_cfg[port].q_index, (u16) num);
+		break;
+	case PROC_SET_CREDIT_INCREMENT:
+		port_set_increment(sw, port,
+			sw->info->port_cfg[port].q_index, (u16) num);
+		break;
+	case PROC_SET_SRP:
+		port_set_srp(sw, port, (u8) num);
+		break;
+	case PROC_SET_QM_DROP:
+		port_set_qm_drop(sw, port, num);
+		break;
+	case PROC_SET_QM_BURST_SIZE:
+		port_set_qm_burst_size(sw, port, (u8) num);
+		break;
+	case PROC_SET_QM_RESV_SPACE:
+		port_set_qm_resv_space(sw, port, (u16) num);
+		break;
+	case PROC_SET_QM_HI_WATER_MARK:
+		port_set_qm_hi_water_mark(sw, port,
+			sw->info->port_cfg[port].q_index, (u16) num);
+		break;
+	case PROC_SET_QM_LO_WATER_MARK:
+		port_set_qm_lo_water_mark(sw, port,
+			sw->info->port_cfg[port].q_index, (u16) num);
+		break;
+	case PROC_SET_MMD_ID:
+		cfg->mmd_id = (u16) num;
+		break;
+	case PROC_SET_MMD_REG:
+		cfg->mmd_reg = (u16) num;
+		break;
+	case PROC_SET_MMD_VAL:
+		mmd_val = (u16) num;
+		port_mmd_write(sw, port, cfg->mmd_id, cfg->mmd_reg,
+			&mmd_val, 1);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_port_write */
+
+static ssize_t sysfs_mac_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_mac_table *entry;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		len += sprintf(buf + len, "0x%03x\n", entry->fid);
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		len += sprintf(buf + len, "%u\n", entry->use_fid);
+		break;
+	case PROC_SET_STATIC_MSTP:
+		len += sprintf(buf + len, "%u\n", entry->mstp);
+		break;
+	case PROC_SET_STATIC_PRIO:
+		len += sprintf(buf + len, "%u\n", entry->prio);
+		break;
+	case PROC_SET_STATIC_SRC:
+		len += sprintf(buf + len, "%u\n", entry->src);
+		break;
+	case PROC_SET_STATIC_DST:
+		len += sprintf(buf + len, "%u\n", entry->dst);
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		len += sprintf(buf + len, "%u\n", entry->override);
+		break;
+	case PROC_SET_STATIC_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		len += sprintf(buf + len, "0x%04x\n", entry->ports);
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			entry->addr[0], entry->addr[1],
+			entry->addr[2], entry->addr[3],
+			entry->addr[4], entry->addr[5]);
+		break;
+	case PROC_SET_STATIC_TYPE:
+		len += sprintf(buf + len, "%u\n", sw->alu_type);
+		break;
+	case PROC_SET_STATIC_INDEX:
+		len += sprintf(buf + len, "0x%03x\n", sw->alu_index);
+		break;
+	case PROC_SET_STATIC_INFO:
+		if (sw->alu_dirty) {
+			if (2 == sw->alu_type) {
+				u8 mac_addr[ETH_ALEN];
+				u16 fid;
+				u16 mac_index;
+
+				memcpy(mac_addr, entry->addr, ETH_ALEN);
+				fid = entry->fid;
+				sw_r_dyn_mac_table(sw, sw->alu_index,
+					mac_addr, fid, entry, &mac_index);
+				if (!sw->alu_index && mac_index)
+					sw->alu_index = mac_index;
+			} else if (!entry->dirty)
+				sw_r_sta_mac_table(sw, sw->alu_index,
+					sw->alu_type, entry);
+			sw->alu_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x.%u: %02X:%02X:%02X:%02X:%02X:%02X "
+			"%04x m:%u p:%u s:%u d:%u o:%u %u:%02x [%u]\n",
+			sw->alu_index, sw->alu_type,
+			entry->addr[0], entry->addr[1], entry->addr[2],
+			entry->addr[3], entry->addr[4], entry->addr[5],
+			entry->ports, entry->mstp, entry->prio,
+			entry->src, entry->dst, entry->override,
+			entry->use_fid, entry->fid,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_mac_read */
+
+static int sysfs_mac_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_mac_table *entry;
+	int processed = true;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		if (0 <= num && num <= ALU_V_FID_M) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		if (num)
+			entry->use_fid = 1;
+		else
+			entry->use_fid = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_MSTP:
+		if (0 <= num && num <= ALU_V_MSTP_M) {
+			entry->mstp = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_PRIO:
+		if (0 <= num && num <= ALU_V_PRIO_AGE_CNT_M) {
+			entry->prio = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_SRC:
+		if (num)
+			entry->src = 1;
+		else
+			entry->src = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_DST:
+		if (num)
+			entry->dst = 1;
+		else
+			entry->dst = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		if (num)
+			entry->override = 1;
+		else
+			entry->override = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		if (2 == sw->alu_type)
+			sw_w_dyn_mac_table(sw, sw->alu_index,
+				entry->addr, entry->fid, entry);
+		else
+			sw_w_sta_mac_table(sw, sw->alu_index,
+				sw->alu_type, entry);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->ports = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->addr[i] = (u8) n[i];
+			entry->dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_STATIC_TYPE:
+		if (0 <= num && num < 3) {
+			sw->alu_type = num;
+			sw->alu_dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_INDEX:
+		if (0 <= num && num < 0x1000) {
+			sw->alu_index = num;
+			sw->alu_dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_INFO:
+		sw->alu_dirty = 1;
+		entry->dirty = 0;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mac_write */
+
+static ssize_t sysfs_vlan_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_vlan_table *entry;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_VLAN_PORTS:
+		len += sprintf(buf + len, "0x%04x\n", entry->ports);
+		break;
+	case PROC_SET_VLAN_UNTAG:
+		len += sprintf(buf + len, "0x%04x\n", entry->untag);
+		break;
+	case PROC_SET_VLAN_FID:
+		len += sprintf(buf + len, "0x%03x\n", entry->fid);
+		break;
+	case PROC_SET_VLAN_MSTP:
+		len += sprintf(buf + len, "0x%x\n", entry->mstp);
+		break;
+	case PROC_SET_VLAN_PRIO:
+		len += sprintf(buf + len, "0x%x\n", entry->prio);
+		break;
+	case PROC_SET_VLAN_OPTION:
+		len += sprintf(buf + len, "%u\n", entry->option);
+		break;
+	case PROC_SET_VLAN_VID:
+		len += sprintf(buf + len, "0x%03x\n", sw->vlan_index);
+		break;
+	case PROC_SET_VLAN_INFO:
+		if (sw->vlan_dirty) {
+			if (!entry->dirty)
+				sw_r_vlan_table(sw, sw->vlan_index, entry);
+			sw->vlan_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: 0x%03x m:%x p:%x o:%u %04x %04x [%u]\n",
+			sw->vlan_index, entry->fid, entry->mstp, entry->prio,
+			entry->option, entry->untag, entry->ports,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_vlan_read */
+
+static int sysfs_vlan_write(struct ksz_sw *sw, int proc_num, int num)
+{
+	struct ksz_vlan_table *entry;
+	int processed = true;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		sw_w_vlan_table(sw, sw->vlan_index, entry);
+		sw->vlan_dirty = 0;
+		sw->overrides |= VLAN_SET;
+		break;
+	case PROC_SET_VLAN_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->ports = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_UNTAG:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->untag = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_FID:
+		if (0 <= num && num < 0x1000) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_VID:
+		if (0 <= num && num < 0x1000) {
+			sw->vlan_index = num;
+			sw->vlan_dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_MSTP:
+		if (0 <= num && num <= VLAN_MSTP_M) {
+			entry->mstp = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_PRIO:
+		if (0 <= num && num <= VLAN_PRIO_M) {
+			entry->prio = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_OPTION:
+		if (num)
+			entry->option = 1;
+		else
+			entry->option = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_VLAN_INFO:
+		sw->vlan_dirty = 1;
+		entry->dirty = 0;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_vlan_write */
+
+static ssize_t sysfs_acl_read(struct ksz_sw *sw, int proc_num, int port,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		len += sprintf(buf + len, "0x%x\n", acl->first_rule);
+		break;
+	case PROC_SET_ACL_RULESET:
+		len += sprintf(buf + len, "0x%04x\n", acl->ruleset);
+		break;
+	case PROC_SET_ACL_MODE:
+		len += sprintf(buf + len, "%u\n", acl->mode);
+		break;
+	case PROC_SET_ACL_ENABLE:
+		len += sprintf(buf + len, "%u\n", acl->enable);
+		break;
+	case PROC_SET_ACL_SRC:
+		len += sprintf(buf + len, "%u\n", acl->src);
+		break;
+	case PROC_SET_ACL_EQUAL:
+		len += sprintf(buf + len, "%u\n", acl->equal);
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		len += sprintf(buf + len, "%u\n", action->prio_mode);
+		break;
+	case PROC_SET_ACL_PRIO:
+		len += sprintf(buf + len, "%u\n", action->prio);
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		len += sprintf(buf + len, "%u\n", action->vlan_prio_replace);
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		len += sprintf(buf + len, "%u\n", action->vlan_prio);
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		len += sprintf(buf + len, "%u\n", action->map_mode);
+		break;
+	case PROC_SET_ACL_PORTS:
+		len += sprintf(buf + len, "0x%04x\n", action->ports);
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5]);
+		break;
+	case PROC_SET_ACL_TYPE:
+		len += sprintf(buf + len, "0x%04x\n", acl->eth_type);
+		break;
+	case PROC_SET_ACL_CNT:
+		len += sprintf(buf + len, "%u\n", acl->cnt);
+		break;
+	case PROC_SET_ACL_MSEC:
+		len += sprintf(buf + len, "%u\n", acl->msec);
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		len += sprintf(buf + len, "%u\n", acl->intr_mode);
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+		len += sprintf(buf + len, "%u.%u.%u.%u\n",
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3]);
+		break;
+	case PROC_SET_ACL_IP_MASK:
+		len += sprintf(buf + len, "%u.%u.%u.%u\n",
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3]);
+		break;
+	case PROC_SET_ACL_PROTOCOL:
+		len += sprintf(buf + len, "%u\n", acl->protocol);
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		len += sprintf(buf + len, "%u\n", acl->port_mode);
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		len += sprintf(buf + len, "%u\n", acl->max_port);
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		len += sprintf(buf + len, "%u\n", acl->min_port);
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		len += sprintf(buf + len, "%u\n", acl->seqnum);
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		len += sprintf(buf + len, "%u\n", acl->tcp_flag_enable);
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		len += sprintf(buf + len, "0x%x\n", acl->tcp_flag);
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		len += sprintf(buf + len, "0x%x\n", acl->tcp_flag_mask);
+		break;
+	case PROC_SET_ACL_INDEX:
+		len += sprintf(buf + len, "0x%x\n", cfg->acl_index);
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		len += sprintf(buf + len, "0x%x\n", cfg->acl_act_index);
+		break;
+	case PROC_SET_ACL_ACTION:
+		len += acl_action_info(action, cfg->acl_act_index, buf, len);
+		break;
+	case PROC_SET_ACL_INFO:
+		len += acl_info(acl, cfg->acl_index, buf, len);
+		break;
+	case PROC_GET_ACL_TABLE:
+		sw_d_acl_table(sw, port);
+		break;
+	}
+	return len;
+}  /* sysfs_acl_read */
+
+static int sysfs_acl_write(struct ksz_sw *sw, int proc_num, int port, int num,
+	const char *buf)
+{
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	int processed = true;
+
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		acl->first_rule = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_RULESET:
+		sscanf(buf, "%x", &num);
+		acl->ruleset = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_MODE:
+		if (0 <= num && num < 4) {
+			acl->mode = num;
+			sw_w_acl_table(sw, port, cfg->acl_index, acl);
+		}
+		break;
+	case PROC_SET_ACL_ENABLE:
+		if (0 <= num && num < 4) {
+			acl->enable = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_SRC:
+		if (num)
+			acl->src = 1;
+		else
+			acl->src = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		if (num)
+			acl->equal = 1;
+		else
+			acl->equal = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		if (0 <= num && num < 4) {
+			action->prio_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PRIO:
+		if (0 <= num && num < 4) {
+			action->prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		if (num)
+			action->vlan_prio_replace = 1;
+		else
+			action->vlan_prio_replace = 0;
+		action->action_changed = 1;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		if (0 <= num && num < 4) {
+			action->vlan_prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		if (0 <= num && num < 4) {
+			action->map_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PORTS:
+		sscanf(buf, "%x", &num);
+		if (0 <= num && num <= sw->PORT_MASK) {
+			action->ports = (u16) num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				acl->mac[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_TYPE:
+		sscanf(buf, "%x", &num);
+		acl->eth_type = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_CNT:
+		if (0 <= num && num <= ACL_CNT_M) {
+			acl->cnt = (u16) num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MSEC:
+		if (num)
+			acl->msec = 1;
+		else
+			acl->msec = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		if (num)
+			acl->intr_mode = 1;
+		else
+			acl->intr_mode = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_addr[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_IP_MASK:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_mask[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_PROTOCOL:
+		acl->protocol = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		if (0 <= num && num < 4) {
+			acl->port_mode = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		acl->max_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		acl->min_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		acl->seqnum = num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		if (num)
+			acl->tcp_flag_enable = 1;
+		else
+			acl->tcp_flag_enable = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		acl->tcp_flag = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		acl->tcp_flag_mask = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_act_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION:
+		if (num)
+			sw_w_acl_action(sw, port, cfg->acl_act_index, action);
+		else
+			sw_r_acl_table(sw, port, cfg->acl_act_index, action);
+		break;
+	case PROC_SET_ACL_INFO:
+		sw_r_acl_table(sw, port, cfg->acl_index, acl);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_acl_write */
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef KSZSW_REGS_SIZE
+static ssize_t kszsw_registers_read(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	unsigned reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off > KSZSW_REGS_SIZE))
+		return 0;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->get(sw, reg, count, buf);
+	sw->ops->release(sw);
+	return i;
+}
+
+static ssize_t kszsw_registers_write(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	unsigned reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off >= KSZSW_REGS_SIZE))
+		return -EFBIG;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->set(sw, reg, count, buf);
+	sw->ops->release(sw);
+	return i;
+}
+
+static struct bin_attribute kszsw_registers_attr = {
+	.attr = {
+		.name	= "registers",
+		.mode	= S_IRUSR | S_IWUSR,
+	},
+	.size	= KSZSW_REGS_SIZE,
+	.read	= kszsw_registers_read,
+	.write	= kszsw_registers_write,
+};
+#endif
+
+static void sw_cfg_mac(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+	int override, int use_fid, u16 fid)
+{
+	struct ksz_mac_table mac;
+
+	memset(&mac, 0, sizeof(struct ksz_mac_table));
+	memcpy(mac.addr, dest, ETH_ALEN);
+	mac.ports = ports;
+	mac.override = override;
+	mac.use_fid = use_fid;
+	mac.fid = fid;
+	mac.valid = mac.ports != 0;
+	if (!mac.valid && mac.override) {
+		mac.override = 0;
+		mac.valid = 1;
+	}
+	sw_w_dyn_mac_table(sw, 0, mac.addr, mac.fid, &mac);
+}  /* sw_cfg_mac */
+
+static void sw_cfg_vlan(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_vlan_table vlan;
+
+	if (0xffff == ports)
+		ports = sw->PORT_MASK;
+	memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+	vlan.vid = vid;
+	vlan.fid = fid;
+	vlan.ports = ports;
+	vlan.valid = vlan.ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+}  /* sw_cfg_vlan */
+
+static u8 sw_alloc_mac(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_mac */
+
+static void sw_free_mac(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_mac */
+
+static u8 sw_alloc_vlan(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_vlan */
+
+static void sw_free_vlan(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_vlan */
+
+static int sw_get_id(struct ksz_sw *sw, u8 *id1, u8 *id2)
+{
+	int id;
+	int i;
+	int j;
+
+	id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+	i = id;
+	i >>= 8;
+	i &= 0xffff;
+	j = i & 0xff;
+	i >>= 8;
+	*id1 = (u8) i;
+	*id2 = (u8) j;
+	return id;
+}
+
+static void sw_cfg_tail_tag(struct ksz_sw *sw, int enable)
+{
+	port_cfg_tail_tag(sw, sw->HOST_PORT, enable);
+}
+
+static void sw_cfg_each_port(struct ksz_sw *sw, int p, int cpu)
+{
+	if (cpu)
+		p = sw->HOST_PORT;
+	else if (p >= sw->HOST_PORT)
+		p++;
+	if (!cpu)
+		sw_cfg_port_base_vlan(sw, p, sw->HOST_MASK | (1 << p));
+	else
+		sw_cfg_port_base_vlan(sw, p, sw->PORT_MASK);
+}
+
+static int sw_port_to_phy_addr(struct ksz_sw *sw, int p)
+{
+	if (p >= sw->HOST_PORT)
+		p++;
+	if (0 <= p && p <= sw->mib_port_cnt)
+		return p;
+	return -1;
+}
+
+static void sw_set_port_addr(struct ksz_sw *sw, int p, u8 *addr)
+{
+}
+
+static void sw_cfg_src_filter(struct ksz_sw *sw, int set)
+{
+	int p;
+
+	if (!(sw->features & NEW_CAP))
+		return;
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (p == sw->HOST_PORT)
+			continue;
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_SRC_ADDR_FILTER, set);
+	}
+}  /* sw_cfg_src_filter */
+
+#ifdef CONFIG_KSZ_STP
+static void sw_set_multi(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct netdev_hw_addr *ha;
+	int i;
+	int found;
+	int owner;
+	int port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port + 1;
+	owner = 1 << port;
+
+	/* Remove old multicast entries. */
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			/* Remove device ownership. */
+			alu->owner &= ~owner;
+			if (!port)
+				alu->forward &= ~FWD_MAIN_DEV;
+			else if (alu->owner <= 1)
+				alu->forward &= ~FWD_STP_DEV;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->ports = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+	netdev_for_each_mc_addr(ha, dev) {
+		if (!(*ha->addr & 1))
+			continue;
+		if (info->multi_net == info->multi_sys)
+			break;
+		found = 0;
+		for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+			entry = &info->mac_table[i];
+			alu = &info->alu_table[i];
+			if (alu->valid &&
+			    !memcmp(entry->addr, ha->addr, ETH_ALEN)) {
+				found = i + 1;
+				break;
+			}
+			if (!alu->valid && !found &&
+			    i >= SWITCH_MAC_TABLE_ENTRIES &&
+			    i < info->multi_net)
+				found = i + 1;
+		}
+		if (!found) {
+			info->multi_net++;
+			found = info->multi_net;
+		}
+		found--;
+		if (found >= SWITCH_MAC_TABLE_ENTRIES &&
+		    found < info->multi_net) {
+			entry = &info->mac_table[found];
+			alu = &info->alu_table[found];
+			if (port)
+				alu->forward |= FWD_STP_DEV;
+			else
+				alu->forward |= FWD_MAIN_DEV;
+			alu->owner |= owner;
+			alu->valid = 1;
+			memcpy(entry->addr, ha->addr, ETH_ALEN);
+			entry->ports = sw->PORT_MASK;
+			entry->valid = 1;
+		}
+	}
+}  /* sw_set_multi */
+
+static void sw_add_frame(struct ksz_sw *sw, u32 crc, unsigned long now,
+	unsigned long expired, int num, int port, int max,
+	struct ksz_frame_table *table, int *cnt)
+{
+	struct ksz_frame_table *entry;
+	int i;
+
+	/* Table full. */
+	if (max == *cnt) {
+		for (i = 0; i < max; i++) {
+			entry = &table[i];
+			if (entry->expired &&
+					time_after(now, entry->expired)) {
+				entry->expired = 0;
+				--(*cnt);
+			}
+		}
+	}
+	for (i = 0; i < max; i++) {
+		entry = &table[i];
+		if (!entry->expired) {
+			entry->crc = crc;
+			entry->cnt = num;
+			entry->port = port;
+			if (0 == expired)
+				expired = 1;
+			entry->expired = expired;
+			++(*cnt);
+			break;
+		}
+	}
+}  /* sw_add_frame */
+
+static int sw_del_frame(struct ksz_sw *sw, u32 crc, unsigned long now,
+	int port, int max, struct ksz_frame_table *table, int *cnt)
+{
+	struct ksz_frame_table *entry;
+	int i;
+	int num = 0;
+
+	for (i = 0; i < max; i++) {
+		entry = &table[i];
+		if (!entry->expired)
+			continue;
+		if (crc == entry->crc && port != entry->port) {
+			if (time_after(now, entry->expired)) {
+				entry->expired = 0;
+				--(*cnt);
+				break;
+			}
+			--entry->cnt;
+
+			/* No need to retain the entry. */
+			if (!entry->cnt) {
+				entry->expired = 0;
+				--(*cnt);
+			}
+			return i + 1;
+		}
+		++num;
+		if (num == *cnt)
+			break;
+	}
+	return 0;
+}  /* sw_del_frame */
+
+static void sw_add_rx(struct ksz_sw *sw, u32 crc, unsigned long now,
+	unsigned long expired, int num, int port)
+{
+	struct ksz_rx_table *info = &sw->info->rx_table;
+
+	sw_add_frame(sw, crc, now, expired, num, port,
+		RX_TABLE_ENTRIES, info->table, &info->cnt);
+}  /* sw_add_rx */
+
+static int sw_del_rx(struct ksz_sw *sw, u32 crc, unsigned long now, int port)
+{
+	struct ksz_rx_table *info = &sw->info->rx_table;
+
+	return sw_del_frame(sw, crc, now, port, RX_TABLE_ENTRIES, info->table,
+		&info->cnt);
+}  /* sw_del_rx */
+
+static void sw_add_tx(struct ksz_sw *sw, u32 crc, unsigned long now,
+	unsigned long expired, int num, int port)
+{
+	struct ksz_tx_table *info = &sw->info->tx_table;
+
+	sw_add_frame(sw, crc, now, expired, num, port,
+		TX_TABLE_ENTRIES, info->table, &info->cnt);
+}  /* sw_add_tx */
+
+static int sw_del_tx(struct ksz_sw *sw, u32 crc, unsigned long now, int port)
+{
+	struct ksz_tx_table *info = &sw->info->tx_table;
+
+	return sw_del_frame(sw, crc, now, port, TX_TABLE_ENTRIES, info->table,
+		&info->cnt);
+}  /* sw_del_tx */
+
+static int sw_blocked_rx(struct ksz_sw *sw, u8 *data)
+{
+	int i;
+
+	for (i = 0; i < sw->info->blocked_rx_cnt; i++)
+		if (!memcmp(data, sw->info->blocked_rx[i], ETH_ALEN))
+			return true;
+	if (BLOCKED_RX_ENTRIES == i)
+		sw->info->blocked_rx_cnt = 0;
+	memcpy(sw->info->blocked_rx[sw->info->blocked_rx_cnt++], data,
+		ETH_ALEN);
+	return false;
+}  /* sw_blocked_rx */
+
+static int sw_block_rx(struct ksz_sw *sw, u8 *data, int len, int port)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	u32 crc;
+	int i;
+	int forward = 0;
+
+	for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+#if 0
+		if (!len && STATIC_MAC_TABLE_ENTRIES == i)
+			break;
+#endif
+
+		entry = &sw->info->mac_table[i];
+		if (!entry->valid ||
+		    memcmp(data, entry->addr, ETH_ALEN))
+			continue;
+
+#if 0
+		/* Block if received port is closed. */
+		if (!entry->override && !(sw->rx_ports & (1 << port)))
+			break;
+#endif
+
+		alu = &sw->info->alu_table[i];
+		forward = alu->forward;
+
+		/* Allow to reach host as the frame is not forwarded. */
+		if (alu->forward & FWD_HOST)
+			break;
+		if (!len)
+			break;
+
+		/* Remember the frame when forwarding to STP device. */
+		if ((alu->forward & FWD_STP_DEV) && sw->info->fwd_ports > 1) {
+			unsigned long now;
+
+			/* Port is zero-based. */
+			port++;
+			crc = ether_crc(len, data);
+			now = jiffies;
+			sw_add_rx(sw, crc, now, now + 1000 / HZ,
+				sw->info->fwd_ports - 1, port);
+		}
+		break;
+	}
+
+	/*
+	 * Check port state in case it is changed after processing arrived
+	 * BPDU.
+	 */
+	if (forward && len && !i)
+		schedule_delayed_work(sw->stp_monitor, 1);
+	return forward;
+}  /* sw_block_rx */
+
+static int sw_block_tx(struct ksz_sw *sw, u8 *data, int len, int port)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	u32 crc = 0;
+	unsigned long now = 0;
+	int block = false;
+	int forward = 0;
+
+	for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &sw->info->mac_table[i];
+		if (!entry->valid ||
+		    memcmp(data, entry->addr, ETH_ALEN))
+			continue;
+
+		alu = &sw->info->alu_table[i];
+		forward = alu->forward;
+
+		/* No need to block. */
+		if (alu->forward & FWD_HOST)
+			break;
+
+		/* Check frame is not forwarded by software. */
+		if (port && (alu->forward & FWD_STP_DEV) &&
+		    sw->info->fwd_ports > 1) {
+			crc = ether_crc(len, data);
+			now = jiffies;
+			if (sw_del_rx(sw, crc, now, port)) {
+				if ((1 << (port - 1)) & sw->info->member) {
+					block = true;
+					forward = 0;
+				}
+			}
+		}
+		break;
+	}
+
+	/* Check duplicate frames sent by main and STP devices. */
+	if ((forward & (FWD_MAIN_DEV | FWD_STP_DEV)) ==
+	    (FWD_MAIN_DEV | FWD_STP_DEV)) {
+
+		/* Re-use CRC if already calculated. */
+		if (!now) {
+			crc = ether_crc(len, data);
+			now = jiffies;
+		}
+		if (sw_del_tx(sw, crc, now, !!port))
+			block = true;
+		else
+			sw_add_tx(sw, crc, now, now + 100 / HZ, 1, !!port);
+	}
+	return block;
+}  /* sw_block_tx */
+
+static int sw_stp_rx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, int port, int *forward)
+{
+	if ((sw->features & STP_SUPPORT) && br_port_exists(dev)) {
+		*forward = sw_block_rx(sw, skb->data, skb->len, port);
+		if (!*forward && sw->dev_offset && dev != sw->netdev[0]) {
+			dev = sw->netdev[0];
+			if ((dev->flags & IFF_PROMISC) ||
+			    ((dev->flags & IFF_ALLMULTI) &&
+			    (skb->data[0] & 1)))
+				*forward = FWD_MAIN_DEV;
+		}
+		return true;
+	}
+	return false;
+}  /* sw_stp_rx */
+#endif
+
+static struct net_device *sw_rx_dev(struct ksz_sw *sw, u8 *data, u32 *len,
+	int *tag, int *port)
+{
+	int index;
+	struct net_device *dev;
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+
+	/* Get received port number. */
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+#if 0
+		u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+dbg_msg(" 1 vid: %x\n", vlan_tci);
+#endif
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			ptr += VLAN_HLEN;
+dbg_msg(" 2 vid: %x\n", vlan_tci);
+		}
+	}
+	if (eth->h_proto == htons(0x9100)) {
+		u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+dbg_msg(" 1 isp: %x\n", vlan_tci);
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			ptr += VLAN_HLEN;
+			vlan_tci = ntohs(vlan->h_vlan_TCI);
+dbg_msg(" 2 vid: %x\n", vlan_tci);
+		}
+	}
+	if (sw->overrides & TAIL_TAGGING) {
+		(*len)--;
+		*tag = data[*len - 4];
+		sw->tag.timestamp = 0;
+		sw->tag.ports = *tag;
+		if (*tag & 0x80) {
+			u32 rx_ts;
+
+			memcpy(&rx_ts, &data[*len - 8], 4);
+			rx_ts = ntohl(rx_ts);
+			sw->tag.timestamp = rx_ts;
+			(*len) -= 4;
+		}
+		*tag &= ~0x80;
+
+		/* In case tagging is not working right. */
+		if (*tag >= sw->mib_port_cnt)
+			*tag = 0;
+
+		/* Save receiving port. */
+		*port = *tag;
+	}
+	index = *tag;
+	if (index > sw->HOST_PORT)
+		--index;
+	dev = sw->netdev[index + sw->dev_offset];
+#ifdef KSZ_DLR
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(DLR_TAG_TYPE))
+			return dev;
+	}
+#endif
+#ifdef KSZ_IBA
+	if (vlan->h_vlan_proto == htons(IBA_TAG_TYPE))
+		return sw->netdev[0];
+#endif
+	if (sw->dev_count > 1) {
+		u8 stp;
+
+		stp = sw->info->stp & sw->info->stp_down;
+		if (stp & (1 << *tag))
+			return NULL;
+		if (!netif_running(dev))
+			return NULL;
+	}
+	if (sw->features & VLAN_PORT_TAGGING) {
+		(*tag)++;
+		if (!(sw->vlan_id & (1 << *tag)))
+			*tag = 0;
+	}
+	return dev;
+}  /* sw_rx_dev */
+
+static int pkt_matched(struct sk_buff *skb, struct net_device *dev, void *ptr,
+	int (*match_multi)(void *ptr, u8 *addr), u8 h_promiscuous)
+{
+	int drop = false;
+	u8 bcast_addr[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	if (skb->data[0] & 0x01) {
+		if (memcmp(skb->data, bcast_addr, ETH_ALEN))
+			drop = match_multi(ptr, skb->data);
+	} else if (h_promiscuous && memcmp(skb->data, dev->dev_addr, ETH_ALEN))
+		drop = true;
+	if (drop)
+		return 0;
+	return skb->len;
+}  /* pkt_matched */
+
+static int sw_match_pkt(struct ksz_sw *sw, struct net_device **dev,
+	void **priv, int (*get_promiscuous)(void *ptr),
+	int (*match_multi)(void *ptr, u8 *data), struct sk_buff *skb,
+	u8 h_promiscuous)
+{
+	int s_promiscuous;
+
+	if (sw->dev_count <= 1)
+		return true;
+	s_promiscuous = get_promiscuous(*priv);
+	if (!s_promiscuous && !pkt_matched(skb, *dev, *priv, match_multi,
+			h_promiscuous)) {
+		int matched = false;
+
+		/* There is a parent network device. */
+		if (sw->dev_offset) {
+			matched = true;
+			*dev = sw->netdev[0];
+			*priv = netdev_priv(*dev);
+			s_promiscuous = get_promiscuous(*priv);
+			if (!s_promiscuous && !pkt_matched(skb, *dev, *priv,
+					match_multi, h_promiscuous))
+				matched = false;
+		}
+		return matched;
+	}
+	return true;
+}  /* sw_match_pkt */
+
+static struct net_device *sw_parent_rx(struct ksz_sw *sw,
+	struct net_device *dev, struct sk_buff *skb, int forward,
+	struct net_device **parent_dev, struct sk_buff **parent_skb)
+{
+	if (sw->dev_offset && dev != sw->netdev[0]) {
+		*parent_dev = sw->netdev[0];
+		if (!forward)
+			forward = FWD_MAIN_DEV | FWD_STP_DEV;
+		if ((forward & (FWD_MAIN_DEV | FWD_STP_DEV)) ==
+		    (FWD_MAIN_DEV | FWD_STP_DEV))
+			*parent_skb = skb_copy(skb, GFP_ATOMIC);
+		else if (!(forward & FWD_STP_DEV))
+			dev = *parent_dev;
+	}
+	return dev;
+}  /* sw_parent_rx */
+
+static int sw_port_vlan_rx(struct ksz_sw *sw, struct net_device *dev,
+	struct net_device *parent_dev, struct sk_buff *skb, int forward,
+	int tag, void *ptr, void (*rx_tstamp)(void *ptr, struct sk_buff *skb))
+{
+	struct sk_buff *vlan_skb;
+	struct net_device *vlan_dev = dev;
+
+	/* Add VLAN tag manually. */
+	if (!(forward & FWD_VLAN_DEV))
+		return false;
+
+	if (!tag || !(sw->features & VLAN_PORT))
+		return false;
+	if (tag > sw->HOST_PORT)
+		--tag;
+	tag += VLAN_PORT_START;
+	vlan_skb = skb_copy(skb, GFP_ATOMIC);
+	if (!vlan_skb)
+		return false;
+	skb_reset_mac_header(vlan_skb);
+	vlan_skb = __vlan_hwaccel_put_tag(vlan_skb, tag);
+#ifdef CONFIG_1588_PTP
+	do {
+		struct ptp_info *ptp = ptr;
+
+		if (rx_tstamp && (ptp->rx_en & 1))
+			rx_tstamp(ptp, vlan_skb);
+	} while (0);
+#endif
+	if (parent_dev && dev != parent_dev) {
+		vlan_dev = parent_dev;
+		vlan_skb->dev = vlan_dev;
+	}
+	vlan_skb->protocol = eth_type_trans(vlan_skb, vlan_dev);
+	netif_rx(vlan_skb);
+	return true;
+}  /* sw_port_vlan_rx */
+
+static int sw_drop_icmp(struct sk_buff *skb, int extra_skb)
+{
+	int drop = 0;
+
+	if (skb && extra_skb &&	skb->protocol == htons(ETH_P_IP)) {
+		struct iphdr *iph = (struct iphdr *) skb->data;
+
+		drop = (iph->protocol == IPPROTO_ICMP);
+	}
+	return drop;
+}  /* sw_drop_icmp */
+
+static int sw_drv_rx(struct ksz_sw *sw, struct sk_buff *skb, int port)
+{
+	int ret = 1;
+
+#ifdef KSZ_IBA
+	if (sw->features & IBA_SUPPORT) {
+		ret = iba_rcv(&sw->info->iba, skb);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW) {
+		ret = dlr_rcv(&sw->info->dlr, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+	return ret;
+}  /* sw_drv_rx */
+
+static int sw_get_tx_len(struct ksz_sw *sw, struct sk_buff *skb)
+{
+	int len = skb->len;
+
+	if (!(sw->overrides & TAIL_TAGGING))
+		return len;
+	if (len < 60)
+		len = 60;
+	len += 2;
+	if (sw->overrides & PTP_TAG)
+		len += 4;
+	return len;
+}  /* sw_get_tx_len */
+
+static void sw_add_tail_tag(struct ksz_sw *sw, struct sk_buff *skb, int ports)
+{
+	struct ksz_sw_tx_tag tx_tag;
+	u8 *trailer;
+	u8 *tag;
+	int len = 2;
+	int ptp_len = 0;
+
+	/* PTP is enabled and so requires extra 4 bytes. */
+	if (sw->overrides & PTP_TAG)
+		ptp_len = 4;
+	len += ptp_len;
+	if (sw->TAIL_TAG_LOOKUP < 0x100)
+		len--;
+	trailer = skb_put(skb, len);
+	memset(&tx_tag, 0, sizeof(struct ksz_sw_tx_tag));
+	tx_tag.ports = ports;
+	if (!tx_tag.ports)
+		tx_tag.ports = sw->TAIL_TAG_LOOKUP;
+	else
+		tx_tag.ports |= sw->TAIL_TAG_OVERRIDE;
+	tx_tag.ports = htons(tx_tag.ports);
+	tag = (u8 *) &tx_tag;
+	memcpy(trailer, &tag[4 - ptp_len], ptp_len + 2);
+	if (sw->TAIL_TAG_LOOKUP < 0x100)
+		trailer[ptp_len] = trailer[ptp_len + 1];
+}  /* sw_add_tail_tag */
+
+static int sw_get_tail_tag(u8 *trailer, int *port)
+{
+	int len = 1;
+
+	if (*trailer & 0x80)
+		len += 4;
+	*trailer &= ~0x80;
+	*port = *trailer;
+	return len;
+}  /* sw_get_tail_tag */
+
+static int append_tag(u16 lookup, u8 *pad, u8 *tag, int len, int ptp_len,
+	int addlen)
+{
+	memcpy(&pad[len], &tag[4 - ptp_len], ptp_len + 2);
+
+	/* Only one byte for the tag. */
+	if (lookup < 0x100) {
+		pad[len + ptp_len] = pad[len + ptp_len + 1];
+		addlen--;
+	}
+	return addlen;
+}
+
+static int add_frag(void *from, char *to, int offset, int len, int odd,
+	struct sk_buff *skb)
+{
+	memcpy(to + offset, from, len);
+	return 0;
+}
+
+static struct sk_buff *sw_check_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct ksz_port *priv, void *ptr,
+	int (*update_msg)(u8 *data, u32 port, u32 overrides))
+{
+	int len;
+	int port;
+	struct sk_buff *org_skb;
+	struct ksz_sw_tx_tag tx_tag;
+	u8 *tag;
+	int update_dst = (sw->overrides & TAIL_TAGGING);
+	int ptp_len = 0;
+
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = ptr;
+#endif
+
+	if (!update_dst)
+		return skb;
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+	if (skb->protocol == htons(ETH_P_TRAILER))
+		return skb;
+#endif
+#ifdef KSZ_DLR
+	if (skb->protocol == htons(DLR_TAG_TYPE))
+		return skb;
+#endif
+
+	org_skb = skb;
+	port = 0;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port + 1;
+
+	do {
+		u16 prio;
+		u16 vid;
+
+		if (!(sw->features & VLAN_PORT) || port ||
+				vlan_get_tag(skb, &vid))
+			break;
+		prio = vid & VLAN_PRIO_MASK;
+		vid &= VLAN_VID_MASK;
+		if (vid < VLAN_PORT_START)
+			break;
+		vid -= VLAN_PORT_START;
+		if (vid > sw->HOST_PORT)
+			vid++;
+		if (!vid || vid > sw->mib_port_cnt)
+			break;
+		port = vid;
+
+		if (sw->vid || prio) {
+			struct vlan_ethhdr *vlan =
+				(struct vlan_ethhdr *) skb->data;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vlan_tci &= ~VLAN_VID_MASK;
+			vlan_tci |= sw->vid;
+			vlan->h_vlan_TCI = htons(vlan_tci);
+
+		/* Need to remove VLAN tag manually. */
+		} else if (!(sw->overrides & TAG_REMOVE)) {
+			u8 *data;
+
+			len = VLAN_ETH_HLEN - 2;
+			data = &skb->data[len];
+			memmove(data - VLAN_HLEN, data, skb->len - len);
+			skb->len -= VLAN_HLEN;
+		}
+	} while (0);
+	if (!(sw->overrides & TAIL_TAGGING))
+		return skb;
+
+	/* PTP is enabled and so requires extra 4 bytes. */
+	if (sw->overrides & PTP_TAG)
+		ptp_len = 4;
+
+	tx_tag.ports = 0;
+	if (port)
+		tx_tag.ports = 1 << (port - 1);
+
+	/* Socket buffer has no fragments. */
+	if (!skb_shinfo(skb)->nr_frags) {
+#ifdef NET_SKBUFF_DATA_USES_OFFSET
+		len = skb_end_pointer(skb) - skb->data;
+#else
+		len = skb->end - skb->data;
+#endif
+		if (skb->len + ptp_len + 2 > len || len < 60 + ptp_len + 2) {
+			len = (skb->len + ptp_len + 5) & ~3;
+			if (len < 68)
+				len = 68;
+			skb = dev_alloc_skb(len);
+			if (!skb)
+				return NULL;
+			memcpy(skb->data, org_skb->data, org_skb->len);
+			skb->len = org_skb->len;
+			copy_old_skb(org_skb, skb);
+		}
+		if (skb->len < 60) {
+			memset(&skb->data[skb->len], 0, 60 - skb->len);
+			skb->len = 60;
+		}
+		len = skb->len;
+
+#ifdef CONFIG_KSZ_STP
+		if ((sw->features & STP_SUPPORT) &&
+		    skb->protocol != htons(ETH_P_IBA)) {
+			int forward = sw_block_rx(sw, skb->data, 0, 0);
+
+			/*
+			 * The static MAC table was programmed to forward only
+			 * to host.
+			 * Need destination ports to send out.
+			 */
+			if (forward & FWD_HOST) {
+				if (!port)
+					port = sw->tx_ports & ~sw->HOST_MASK;
+				if (port && (forward & FWD_HOST_OVERRIDE))
+					port |= sw->TAIL_TAG_OVERRIDE;
+				tx_tag.ports = port;
+			}
+		}
+#endif
+	}
+	if (!tx_tag.ports)
+		tx_tag.ports = sw->TAIL_TAG_LOOKUP;
+	tx_tag.timestamp = 0;
+
+#ifdef CONFIG_1588_PTP
+	if (ptp)
+		ptp_set_tx_info(ptp, skb->data, &tx_tag);
+#endif
+#if 0
+tx_tag.ports |= sw->TAIL_TAG_OVERRIDE;
+#endif
+#if 0
+if (port == sw->HOST_PORT + 1)
+printk(" tag: %x\n", tx_tag.ports);
+#endif
+
+	tx_tag.ports = htons(tx_tag.ports);
+	tx_tag.timestamp = htonl(tx_tag.timestamp);
+	tag = (u8 *) &tx_tag;
+
+	/* Socket buffer has no fragments. */
+	if (!skb_shinfo(skb)->nr_frags) {
+		len = append_tag(sw->TAIL_TAG_LOOKUP, skb->data, tag, len,
+			ptp_len, ptp_len + 2);
+		skb_put(skb, len);
+	} else {
+		struct sock dummy;
+		struct sock *sk;
+
+		sk = skb->sk;
+		if (!sk) {
+			sk = &dummy;
+			sk->sk_allocation = GFP_KERNEL;
+			atomic_set(&sk->sk_wmem_alloc, 1);
+		}
+
+		/* Clear last tag. */
+		memset(&sw->tx_pad[sw->tx_start], 0, sizeof(tx_tag));
+		sw->tx_start = 0;
+		len = ptp_len + 2;
+		if (skb->len < 60) {
+			sw->tx_start = 60 - skb->len;
+			len += sw->tx_start;
+		}
+		len = append_tag(sw->TAIL_TAG_LOOKUP, sw->tx_pad, tag,
+			sw->tx_start, ptp_len, len);
+		skb_append_datato_frags(sk, skb, add_frag, sw->tx_pad, len);
+	}
+	return skb;
+}  /* sw_check_skb */
+
+static struct sk_buff *sw_check_tx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, struct ksz_port *priv)
+{
+	void *ptr = NULL;
+	int (*update_msg)(u8 *data, u32 port, u32 overrides) = NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptr = ptp;
+#if 0
+		update_msg = ptp->ops->update_msg;
+#endif
+	}
+#endif
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		int port = 0;
+
+		/* This device is associated with a switch port. */
+		if (1 == priv->port_cnt)
+			port = priv->first_port + 1;
+		if ((br_port_exists(dev) || !port) &&
+		    sw_block_tx(sw, skb->data, skb->len, port)) {
+			dev_kfree_skb_irq(skb);
+			return NULL;
+		}
+	}
+#endif
+	return sw_check_skb(sw, skb, priv, ptr, update_msg);
+}  /* sw_check_tx */
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+static struct sk_buff *tail_xmit(struct sk_buff *skb, struct net_device *dev,
+	struct ksz_sw *sw)
+{
+	struct sk_buff *nskb;
+	int padlen;
+	int addlen = 8;
+
+	if (skb->protocol == htons(ETH_P_TRAILER))
+		return skb;
+
+	/*
+	 * We have to make sure that the trailer ends up as the very
+	 * last 4 bytes of the packet.  This means that we have to pad
+	 * the packet to the minimum ethernet frame size, if necessary,
+	 * before adding the trailer.
+	 */
+	padlen = 0;
+	if (skb->len < 60)
+		padlen = 60 - skb->len;
+
+	nskb = alloc_skb(NET_IP_ALIGN + skb->len + padlen + addlen, GFP_ATOMIC);
+	if (nskb == NULL) {
+		dev_kfree_skb_irq(skb);
+		return NULL;
+	}
+	skb_reserve(nskb, NET_IP_ALIGN);
+
+	skb_reset_mac_header(nskb);
+	skb_set_network_header(nskb, skb_network_header(skb) - skb->head);
+	skb_set_transport_header(nskb, skb_transport_header(skb) - skb->head);
+	skb_copy_and_csum_dev(skb, skb_put(nskb, skb->len));
+	nskb->dev = skb->dev;
+	dev_kfree_skb_irq(skb);
+
+	if (padlen) {
+		u8 *pad = skb_put(nskb, padlen);
+		memset(pad, 0, padlen);
+	}
+
+	sw->net_ops->add_tail_tag(sw, nskb, 0);
+
+	nskb->protocol = htons(ETH_P_TRAILER);
+
+	return nskb;
+}
+#endif
+
+static struct sk_buff *sw_final_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, struct ksz_port *port)
+{
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+	skb = tail_xmit(skb, dev, sw);
+	if (!skb)
+		return NULL;
+#endif
+
+	skb = sw->net_ops->check_tx(sw, dev, skb, port);
+	if (!skb)
+		return NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			ptp->ops->get_tx_tstamp(ptp, skb);
+	}
+#endif
+	return skb;
+}  /* sw_final_skb */
+
+static void sw_start(struct ksz_sw *sw, u8 *addr)
+{
+	int need_tail_tag = false;
+	int need_vlan = false;
+
+	sw->ops->acquire(sw);
+	sw_setup(sw);
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba) {
+printk("%s\n", __func__);
+	sw->ops->release(sw);
+return;
+}
+#endif
+	sw_set_addr(sw, addr);
+	if (1 == sw->dev_count)
+		sw_cfg_src_filter(sw, true);
+#if defined(CONFIG_1588_PTP) || defined(KSZ_DLR)
+	if (sw->features & (PTP_HW | DLR_HW | HSR_HW))
+		need_tail_tag = true;
+#endif
+	if (sw->dev_count > 1)
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT) {
+		if (sw->features & VLAN_PORT_REMOVE_TAG) {
+			struct ksz_vlan_table entry;
+			int p;
+
+			memset(&entry, 0, sizeof(struct ksz_vlan_table));
+			sw->ops->release(sw);
+			entry.fid = VLAN_PORT_START;
+			entry.untag = sw->PORT_MASK;
+			entry.ports = sw->PORT_MASK;
+			entry.valid = 1;
+			sw_w_vlan_table(sw, VLAN_PORT_START, &entry);
+			for (p = 0; p < sw->mib_port_cnt - 1; p++) {
+				entry.fid = VLAN_PORT_START + p + 1;
+				entry.untag = (1 << p);
+				entry.ports = (1 << p) | (1 << sw->HOST_PORT);
+				entry.valid = 1;
+				sw_w_vlan_table(sw, VLAN_PORT_START + p + 1,
+					&entry);
+			}
+			sw->ops->acquire(sw);
+			for (p = 0; p < sw->mib_port_cnt; p++) {
+				sw_cfg_def_vid(sw, p, VLAN_PORT_START);
+			}
+			need_vlan = true;
+			sw->overrides |= TAG_REMOVE;
+		}
+		if (sw->features & VLAN_PORT_TAGGING)
+			need_tail_tag = true;
+	}
+	if (sw->features & DSA_SUPPORT) {
+		int p;
+		int q;
+
+		need_tail_tag = true;
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			q = p;
+			if (q > sw->HOST_PORT)
+				q--;
+			sw_cfg_each_port(sw, q, p == sw->HOST_PORT);
+			port_set_stp_state(sw, p, STP_STATE_SIMPLE);
+		}
+	}
+	if (sw->features & MRP_SUPPORT)
+		need_vlan = true;
+	if (need_vlan)
+		sw_ena_vlan(sw);
+	if (need_tail_tag) {
+		port_cfg_tail_tag(sw, sw->HOST_PORT, 1);
+if (!(sw->overrides & TAIL_TAGGING))
+dbg_msg(" ! tail tag not set\n");
+	}
+#ifdef KSZ9897_FPGA
+#ifndef NO_SEC_TIMESTAMP
+sw_cfg(sw, 0x12B, 0xC, 0);
+#else
+sw_cfg(sw, 0x12B, 0xC, 1);
+#endif
+#endif
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg->start(ptp, true);
+if (!(sw->overrides & PTP_TAG))
+dbg_msg(" ! ptp tag not set\n");
+	}
+#endif
+}  /* sw_start */
+
+static int sw_stop(struct ksz_sw *sw, int complete)
+{
+	int reset = false;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		reset = ptp->ops->stop(ptp);
+	}
+#endif
+	sw->ops->acquire(sw);
+	if (!reset)
+		sw_reset(sw);
+	reset = true;
+	sw_init(sw);
+
+	/* PHY may not work after reset. */
+	if (sw->features & SETUP_PHY) {
+		int port;
+		u16 data;
+
+		for (port = 0; port < sw->phy_port_cnt; port++)
+			port_setup_9893(sw, port);
+		port_r16(sw, port, REG_PORT_XMII_CTRL_0, &data);
+dbg_msg("xmii: %04x\n", data);
+		data = 0x5008;
+		port_w16(sw, port, REG_PORT_XMII_CTRL_0, data);
+	}
+
+#if !defined(TEST_IBA)
+	/* Clean out static MAC table when the switch shutdown. */
+	if ((sw->features & STP_SUPPORT) && complete)
+		sw_clr_sta_mac_table(sw);
+	if (complete)
+		sw_clr_sta_mac_table(sw);
+#endif
+	sw->ops->release(sw);
+	return reset;
+}  /* sw_stop */
+
+static void sw_init_mib(struct ksz_sw *sw)
+{
+	int i;
+
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		sw->port_mib[i].mib_start = 0;
+		if (sw->next_jiffies < jiffies)
+			sw->next_jiffies = jiffies + HZ * 2;
+		else
+			sw->next_jiffies += MIB_READ_INTERVAL;
+		sw->counter[i].time = sw->next_jiffies;
+		sw->port_state[i].state = media_disconnected;
+		port_init_cnt(sw, i);
+	}
+	for (i = sw->phy_port_cnt; i < sw->mib_port_cnt; i++)
+		sw->port_state[i].state = media_connected;
+	sw->port_state[sw->HOST_PORT].state = media_connected;
+}  /* sw_init_mib */
+
+static void sw_open_dev(struct ksz_sw *sw, struct net_device *dev, u8 *addr)
+{
+	sw_init_mib(sw);
+
+	sw->net_ops->start(sw, addr);
+#ifdef KSZ_IBA
+	if (sw->need_link_up && sw->HOST_PORT >= sw->phy_port_cnt)
+		sw->link_change = 0x80000000;
+#endif
+	sw->main_dev = dev;
+}  /* sw_open_dev */
+
+static void sw_open_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port, u8 *state)
+{
+	int i;
+	int p;
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+		if (p >= sw->phy_port_cnt)
+			break;
+
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		sw->port_info[p].link = 0xFF;
+		sw->port_info[p].state = media_unknown;
+		sw->port_info[p].report = true;
+	}
+
+	sw->ops->acquire(sw);
+
+	/* Need to open the port in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+#ifdef CONFIG_KSZ_STP
+		if (!br_port_exists(dev))
+#endif
+			*state = STP_STATE_SIMPLE;
+		port_set_stp_state(sw, port->first_port, *state);
+	}
+
+	sw->phy_intr = sw->PORT_MASK;
+	if (port->force_link)
+		port_force_link_speed(port);
+	else
+		port_set_link_speed(port);
+	port_get_link_speed(port);
+	sw->ops->release(sw);
+}  /* sw_open_port */
+
+static void sw_close_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	/* Need to shut the port manually in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		sw->ops->acquire(sw);
+		port_set_stp_state(sw, port->first_port, STP_STATE_DISABLED);
+
+#ifdef CONFIG_KSZ_STP
+		/* Port is closed.  Need to change bridge setting. */
+		if ((sw->features & STP_SUPPORT) && br_port_exists(dev)) {
+			int pi;
+
+			pi = 1 << port->first_port;
+			if (sw->info->member & pi) {
+				sw->info->member &= ~pi;
+
+				/* No ports in forwarding state. */
+				if (!sw->info->member) {
+					port_set_stp_state(sw, sw->HOST_PORT,
+						STP_STATE_SIMPLE);
+					sw->ops->release(sw);
+					sw_block_addr(sw);
+					sw_block_multi(sw);
+					sw->ops->acquire(sw);
+				}
+				bridge_change(sw);
+			}
+		}
+#endif
+		sw->ops->release(sw);
+	}
+}  /* sw_close_port */
+
+static void sw_open(struct ksz_sw *sw)
+{
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		prep_dlr(&sw->info->dlr, sw->main_dev, sw->main_dev->dev_addr);
+#endif
+#ifdef KSZ_IBA
+	sw_set_dev(sw, sw->main_dev, sw->main_dev->dev_addr);
+#endif
+	/* Timer may already be started by the SPI device. */
+	if (!sw->monitor_timer_info->max)
+		ksz_start_timer(sw->monitor_timer_info,
+			sw->monitor_timer_info->period);
+}  /* sw_open */
+
+static void sw_close(struct ksz_sw *sw)
+{
+	flush_work(&sw->set_addr);
+#ifdef KSZ_IBA
+	sw_set_dev(sw, NULL, sw->main_dev->dev_addr);
+#endif
+	ksz_stop_timer(sw->monitor_timer_info);
+	cancel_delayed_work_sync(sw->link_read);
+	cancel_delayed_work_sync(sw->stp_monitor);
+}  /* sw_close */
+
+static void sw_delayed_set_addr(struct work_struct *work)
+{
+	struct ksz_sw *sw = container_of(work, struct ksz_sw, set_addr);
+
+	sw->ops->acquire(sw);
+	sw_set_addr(sw, sw->netdev[0]->dev_addr);
+	sw->ops->release(sw);
+}  /* sw_delayed_set_addr */
+
+static u8 sw_set_mac_addr(struct ksz_sw *sw, struct net_device *dev,
+	u8 promiscuous, int port)
+{
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		if (sw->features & DIFF_MAC_ADDR) {
+			sw->features &= ~DIFF_MAC_ADDR;
+			--promiscuous;
+		}
+		for (port = 0; port < sw->mib_port_cnt; port++) {
+			if (port == sw->HOST_PORT)
+				continue;
+			if (memcmp(sw->port_info[port].mac_addr,
+					dev->dev_addr, ETH_ALEN)) {
+				sw->features |= DIFF_MAC_ADDR;
+				++promiscuous;
+				break;
+			}
+		}
+	} else {
+		int i;
+
+		/* Make MAC address the same in all the ports. */
+		if (sw->dev_count > 1) {
+			int dev_count = sw->dev_count + sw->dev_offset;
+
+			for (i = 0; i < dev_count; i++) {
+				if (dev == sw->netdev[i])
+					continue;
+				memcpy(sw->netdev[i]->dev_addr,
+					dev->dev_addr, ETH_ALEN);
+			}
+			if (sw->features & DIFF_MAC_ADDR) {
+				sw->features &= ~DIFF_MAC_ADDR;
+				--promiscuous;
+			}
+		}
+		if (dev == sw->netdev[0])
+			schedule_work(&sw->set_addr);
+	}
+	if (dev != sw->netdev[0])
+		return promiscuous;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->set_identity(ptp, dev->dev_addr);
+	}
+#endif
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		dlr_change_addr(&sw->info->dlr, dev->dev_addr);
+#endif
+
+#ifdef KSZ_IBA
+	if (netif_running(dev)) {
+		sw_set_dev(sw, dev, dev->dev_addr);
+
+		/* A hack to accept IBA response. */
+		if (!promiscuous)
+			promiscuous = 2;
+	}
+#endif
+#ifdef CAPTURE_IBA
+promiscuous = 1;
+#endif
+	return promiscuous;
+}  /* sw_set_mac_addr */
+
+/*
+ * This enables multiple network device mode for the switch, which contains at
+ * least two physical ports.  Some users like to take control of the ports for
+ * running Spanning Tree Protocol.  The driver will create an additional eth?
+ * device for each port depending on the mode.
+ *
+ * Some limitations are the network devices cannot have different MTU and
+ * multicast hash tables.
+ */
+static int multi_dev;
+
+/*
+ * As most users select multiple network device mode to use Spanning Tree
+ * Protocol, this enables a feature in which most unicast and multicast packets
+ * are forwarded inside the switch and not passed to the host.  Only packets
+ * that need the host's attention are passed to it.  This prevents the host
+ * wasting CPU time to examine each and every incoming packets and do the
+ * forwarding itself.
+ *
+ * As the hack requires the private bridge header, the driver cannot compile
+ * with just the kernel headers.
+ *
+ * Enabling STP support also turns on multiple network device mode.
+ */
+static int stp;
+
+/*
+ * This enables fast aging in the switch.  Not sure what situation requires
+ * that.  However, fast aging is used to flush the dynamic MAC table when STP
+ * support is enabled.
+ */
+static int fast_aging;
+
+#ifdef KSZ_IBA
+static int iba = 1;
+#endif
+
+static void sw_setup_special(struct ksz_sw *sw, int *port_cnt,
+	int *mib_port_cnt, int *dev_cnt)
+{
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+	if (sw->stp) {
+		sw->fast_aging = 1;
+		sw->multi_dev = 1;
+#ifdef CONFIG_1588_PTP
+		sw->multi_dev = 5;
+#endif
+		sw->features |= STP_SUPPORT;
+	}
+	dbg_msg("%s %d %d %d\n", __func__,
+		sw->stp, sw->multi_dev, sw->fast_aging);
+	if (sw->fast_aging)
+		sw->overrides |= FAST_AGING;
+#ifdef KSZ_IBA
+	if (iba)
+		sw->features |= IBA_SUPPORT;
+#endif
+
+	/* Multiple network device interfaces are required. */
+	if (1 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt - 1;
+		sw->phy_offset = 1;
+	} else if (2 == sw->multi_dev)
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	else if (3 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt - 1;
+		sw->dev_offset = 1;
+	} else if (4 == sw->multi_dev)
+		sw->features |= VLAN_PORT;
+	else if (5 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt - 1;
+		sw->dev_offset = 1;
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	}
+
+	/* Single network device has multiple ports. */
+	if (1 == sw->dev_count) {
+		*port_cnt = sw->mib_port_cnt - 1;
+		*mib_port_cnt = sw->mib_port_cnt - 1;
+		if (0 < sw->HOST_PORT && sw->HOST_PORT < sw->mib_port_cnt - 1) {
+			(*port_cnt)++;
+			(*mib_port_cnt)++;
+		}
+	}
+	*dev_cnt = sw->dev_count;
+	if (3 == sw->multi_dev || 5 == sw->multi_dev)
+		(*dev_cnt)++;
+#ifdef CONFIG_1588_PTP
+	if (sw->features & VLAN_PORT) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->overrides |= PTP_PORT_FORWARD;
+	}
+#endif
+}  /* sw_setup_special */
+
+static void sw_setup_dev(struct ksz_sw *sw, struct net_device *dev,
+	char *dev_name, struct ksz_port *port, int i, int port_cnt,
+	int mib_port_cnt)
+{
+	int cnt;
+	int p;
+	int pi;
+
+	/* dev_offset is ether 0 or 1. */
+	p = i;
+	if (p)
+		p -= sw->dev_offset;
+	if (p >= sw->HOST_PORT)
+		p++;
+
+	if (sw->dev_offset) {
+		/*
+		 * First device associated with switch has been
+		 * created.
+		 */
+		if (i)
+			snprintf(dev->name, IFNAMSIZ, "%s.10%%d", dev_name);
+		else {
+			port_cnt = sw->mib_port_cnt - 1;
+			mib_port_cnt = sw->mib_port_cnt - 1;
+			if (0 < sw->HOST_PORT &&
+			    sw->HOST_PORT < sw->mib_port_cnt - 1) {
+				port_cnt++;
+				mib_port_cnt++;
+			}
+			sw->netdev[i] = dev;
+		}
+	}
+
+	port->port_cnt = port_cnt;
+	port->mib_port_cnt = mib_port_cnt;
+	port->first_port = p;
+	port->flow_ctrl = PHY_TX_ONLY;
+
+	port->sw = sw;
+	port->linked = &sw->port_info[port->first_port];
+
+	for (cnt = 0, pi = p; cnt < port_cnt; cnt++, pi++) {
+		if (pi < sw->phy_port_cnt)
+			sw->port_info[pi].state = media_disconnected;
+		if (pi == sw->HOST_PORT)
+			continue;
+		sw->netdev[i++] = dev;
+	}
+
+	INIT_WORK(&port->link_update, link_update_work);
+	if (sw->features & VLAN_PORT)
+		dev->features |= NETIF_F_HW_VLAN_FILTER;
+}  /* sw_setup_dev */
+
+static u8 sw_get_priv_state(struct net_device *dev)
+{
+	return STP_STATE_SIMPLE;
+}
+
+static void sw_set_priv_state(struct net_device *dev, u8 state)
+{
+}
+
+static int netdev_chk_running(struct net_device *dev)
+{
+	return netif_running(dev);
+}
+
+static int netdev_chk_stopped(struct net_device *dev)
+{
+	return netif_running(dev) && netif_queue_stopped(dev);
+}
+
+static void netdev_start_queue(struct net_device *dev)
+{
+	dev->trans_start = jiffies;
+	netif_start_queue(dev);
+}
+
+static void netdev_stop_queue(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+}
+
+static void netdev_wake_queue(struct net_device *dev)
+{
+	netif_wake_queue(dev);
+}
+
+static void sw_netdev_oper(struct ksz_sw *sw, struct net_device *dev,
+	int (*netdev_chk)(struct net_device *dev),
+	void (*netdev_oper)(struct net_device *dev))
+{
+	int port;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		netdev_oper(dev);
+		return;
+	}
+	for (port = 0; port < dev_count; port++) {
+		dev = sw->netdev[port];
+		if (!dev)
+			continue;
+		if (!netdev_chk || netdev_chk(dev))
+			netdev_oper(dev);
+	}
+}  /* sw_netdev_oper */
+
+static void sw_netdev_open_port(struct ksz_sw *sw, struct net_device *dev)
+{
+	struct ksz_port *port;
+	u8 state;
+	int p;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		port = sw->net_ops->get_priv_port(dev);
+		state = sw->net_ops->get_state(dev);
+		sw->net_ops->open_port(sw, dev, port, &state);
+		sw->net_ops->set_state(dev, state);
+		return;
+	}
+	for (p = 0; p < dev_count; p++) {
+		dev = sw->netdev[p];
+		if (!dev)
+			continue;
+		port = sw->net_ops->get_priv_port(dev);
+		state = sw->net_ops->get_state(dev);
+		sw->net_ops->open_port(sw, dev, port, &state);
+		sw->net_ops->set_state(dev, state);
+	}
+}  /* sw_netdev_open_port */
+
+static void sw_netdev_start_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_start_queue);
+}
+
+static void sw_netdev_stop_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_stop_queue);
+}
+
+static void sw_netdev_wake_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_stopped, netdev_wake_queue);
+}
+
+static struct ksz_sw_net_ops sw_net_ops = {
+	.setup_dev		= sw_setup_dev,
+	.setup_special		= sw_setup_special,
+	.get_state		= sw_get_priv_state,
+	.set_state		= sw_set_priv_state,
+
+	.start			= sw_start,
+	.stop			= sw_stop,
+	.open_dev		= sw_open_dev,
+	.open_port		= sw_open_port,
+	.close_port		= sw_close_port,
+	.open			= sw_open,
+	.close			= sw_close,
+
+	.netdev_start_queue	= sw_netdev_start_queue,
+	.netdev_stop_queue	= sw_netdev_stop_queue,
+	.netdev_wake_queue	= sw_netdev_wake_queue,
+	.netdev_open_port	= sw_netdev_open_port,
+
+	.set_mac_addr		= sw_set_mac_addr,
+
+	.get_tx_len		= sw_get_tx_len,
+	.add_tail_tag		= sw_add_tail_tag,
+	.get_tail_tag		= sw_get_tail_tag,
+	.check_tx		= sw_check_tx,
+	.rx_dev			= sw_rx_dev,
+	.match_pkt		= sw_match_pkt,
+	.parent_rx		= sw_parent_rx,
+	.port_vlan_rx		= sw_port_vlan_rx,
+	.drop_icmp		= sw_drop_icmp,
+	.final_skb		= sw_final_skb,
+	.drv_rx			= sw_drv_rx,
+
+#ifdef CONFIG_KSZ_STP
+	.get_port_state		= get_port_state,
+
+	.set_multi		= sw_set_multi,
+	.stp_rx			= sw_stp_rx,
+	.blocked_rx		= sw_blocked_rx,
+	.monitor_ports		= monitor_ports,
+#endif
+};
+
+static struct ksz_sw_ops sw_ops = {
+	.acquire		= sw_acquire,
+	.release		= sw_release,
+
+	.chk			= sw_chk,
+	.cfg			= sw_cfg,
+
+	.port_get_link_speed	= port_get_link_speed,
+	.port_set_link_speed	= port_set_link_speed,
+	.port_force_link_speed	= port_force_link_speed,
+
+	.port_r_cnt		= port_r_cnt,
+	.get_mib_counters	= get_sw_mib_counters,
+
+	.sysfs_read		= sysfs_sw_read,
+	.sysfs_read_hw		= sysfs_sw_read_hw,
+	.sysfs_write		= sysfs_sw_write,
+	.sysfs_port_read	= sysfs_port_read,
+	.sysfs_port_read_hw	= sysfs_port_read_hw,
+	.sysfs_port_write	= sysfs_port_write,
+	.sysfs_mac_read		= sysfs_mac_read,
+	.sysfs_mac_write	= sysfs_mac_write,
+	.sysfs_vlan_read	= sysfs_vlan_read,
+	.sysfs_vlan_write	= sysfs_vlan_write,
+	.sysfs_acl_read		= sysfs_acl_read,
+	.sysfs_acl_write	= sysfs_acl_write,
+
+	.cfg_mac		= sw_cfg_mac,
+	.cfg_vlan		= sw_cfg_vlan,
+	.alloc_mac		= sw_alloc_mac,
+	.free_mac		= sw_free_mac,
+	.alloc_vlan		= sw_alloc_vlan,
+	.free_vlan		= sw_free_vlan,
+
+	.get_id			= sw_get_id,
+	.cfg_tail_tag		= sw_cfg_tail_tag,
+	.cfg_each_port		= sw_cfg_each_port,
+	.port_to_phy_addr	= sw_port_to_phy_addr,
+	.set_port_addr		= sw_set_port_addr,
+
+	.cfg_src_filter		= sw_cfg_src_filter,
+	.flush_table		= sw_flush_dyn_mac_table,
+
+};
+
+static void sw_proc_intr(struct ksz_sw *sw)
+{
+	u32 intr_mask;
+	u32 status;
+	u16 port_intr_mask;
+	u8 port_status;
+	int port;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = &sw->ptp_hw;
+#endif
+	int no_intr_status = 0;
+
+	intr_mask = sw->intr_mask;
+	status = sw->reg->r32(sw, REG_SW_INT_STATUS__4);
+	if (!status)
+		++no_intr_status;
+	status &= sw->intr_mask;
+#ifdef CONFIG_1588_PTP
+	if (status & TRIG_TS_INT) {
+		if (ptp->started)
+			ptp->ops->proc_intr(ptp);
+		else
+			sw->intr_mask &= ~TRIG_TS_INT;
+	}
+#endif
+	if (status & APB_TIMEOUT_INT) {
+dbg_msg(" apb: %08x\n", sw->reg->r32(sw, REG_SW_APB_TIMEOUT_ADDR__4));
+		sw->reg->w32(sw, REG_SW_APB_TIMEOUT_ADDR__4,
+			APB_TIMEOUT_ACKNOWLEDGE);
+		status = sw->reg->r32(sw, REG_SW_INT_STATUS__4);
+		if (status & APB_TIMEOUT_INT)
+			sw->intr_mask &= ~APB_TIMEOUT_INT;
+	}
+	if (intr_mask != sw->intr_mask)
+		sw->reg->w32(sw, REG_SW_INT_MASK__4,
+			~sw->intr_mask & SWITCH_INT_MASK);
+
+	intr_mask = sw->port_intr_mask;
+	status = sw->reg->r32(sw, REG_SW_PORT_INT_STATUS__4);
+	if (!status)
+		++no_intr_status;
+#if 0
+dbg_msg("port irq: %08x\n", status);
+#endif
+#if 0
+	if (!status)
+		status |= 1;
+#endif
+	status &= sw->port_intr_mask;
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (status & 1) {
+			struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+			int stop_phy_irq = false;
+
+			if (!(sw->features & NEW_CAP))
+				stop_phy_irq = true;
+			port_intr_mask = cfg->intr_mask;
+			port_r(sw, port, REG_PORT_INT_STATUS, &port_status);
+#if 1
+			port_status &= cfg->intr_mask;
+#endif
+#if 0
+if (2 == no_intr_status && !port_status)
+port_status |= PORT_PHY_INT;
+#endif
+			if (port_status & PORT_PHY_INT) {
+				u8 val;
+
+				/* The status is cleared after read. */
+				port_r8(sw, port, REG_PORT_PHY_INT_STATUS,
+					&val);
+				if (val & (LINK_DOWN_INT | LINK_UP_INT))
+					sw->phy_intr |= (1 << port);
+dbg_msg(" phy: %d %x\n", port, val);
+
+/*
+ * THa  2014/11/18
+ * KSZ9893 IBA/SPI register reset bug.
+ */
+if (!val) {
+	u16 data;
+
+	port_r8(sw, port, REG_PORT_PHY_INT_ENABLE, &val);
+dbg_msg("!! %x %x %x %x\n", status, port_status, cfg->intr_mask, val);
+	port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &data);
+dbg_msg(" p1: %x\n", data);
+	if (!(data & PORT_INT_PIN_HIGH)) {
+		port_w16(sw, port, REG_PORT_PHY_PHY_CTRL, 0x4300);
+		port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &data);
+dbg_msg(" p2: %x\n", data);
+stop_phy_irq = true;
+	}
+}
+#ifndef KSZ_IBA
+				if (stop_phy_irq) {
+				port_w(sw, port, REG_PORT_INT_MASK,
+					(~cfg->intr_mask & PORT_INT_MASK) |
+					PORT_PHY_INT);
+				cfg->intr_mask &= ~PORT_PHY_INT;
+				}
+#endif
+			}
+#ifdef CONFIG_1588_PTP
+			if (port_status & PORT_PTP_INT) {
+				if (ptp->started)
+					proc_ptp_tx_intr(ptp, port);
+				else
+					cfg->intr_mask &= ~PORT_PTP_INT;
+			}
+#endif
+			if (port_status & PORT_ACL_INT) {
+#if 0
+				port_w(sw, port, REG_PORT_INT_STATUS,
+					PORT_ACL_INT);
+#else
+				port_w(sw, port, REG_PORT_INT_MASK,
+					(~cfg->intr_mask & PORT_INT_MASK) |
+					PORT_ACL_INT);
+				port_w(sw, port, REG_PORT_INT_MASK,
+					~cfg->intr_mask & PORT_INT_MASK);
+#endif
+#ifdef KSZ_DLR
+				if (sw->features & DLR_HW)
+					dlr_timeout(&sw->info->dlr, port);
+#endif
+				if (sw->overrides & ACL_INTR_MONITOR)
+					printk(KERN_INFO "  acl: %d %lx\n",
+						port, jiffies);
+			}
+			if (port_intr_mask != cfg->intr_mask)
+				port_w(sw, port, REG_PORT_INT_MASK,
+					~cfg->intr_mask & PORT_INT_MASK);
+		}
+		status >>= 1;
+	}
+	if (sw->phy_intr)
+		schedule_delayed_work(sw->link_read, 0);
+	if (intr_mask != sw->port_intr_mask)
+		sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4,
+			~sw->port_intr_mask & sw->PORT_MASK);
+	if (2 == no_intr_status)
+		printk(KERN_INFO "no intr status\n");
+}  /* sw_proc_intr */
diff --git a/drivers/net/ethernet/micrel/ksz_sw_9897.h b/drivers/net/ethernet/micrel/ksz_sw_9897.h
new file mode 100644
index 0000000..8938f32
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_sw_9897.h
@@ -0,0 +1,1059 @@
+/**
+ * Micrel KSZ9897 switch common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_SW_9897_H
+#define KSZ_SW_9897_H
+
+
+/* These definitions should be defined before this header file. */
+#ifndef PRIO_QUEUES
+#define PRIO_QUEUES			4
+#endif
+#define PRIO_QUEUES_M			(PRIO_QUEUES - 1)
+
+#ifndef KS_PRIO_IN_REG
+#define KS_PRIO_IN_REG			2
+#endif
+
+#ifndef SWITCH_PORT_NUM
+#define SWITCH_PORT_NUM			2
+#endif
+
+#ifndef SWITCH_COUNTER_NUM
+#define SWITCH_COUNTER_NUM		0x20
+#endif
+
+#ifndef SW_D
+#error "SW_D and other data bus parameters need to be defined."
+#endif
+
+#define TOTAL_PORT_NUM			(SWITCH_PORT_NUM + 1)
+
+
+#ifdef KSZ_IBA
+#include "ksz_iba.h"
+#endif
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_9897.h"
+#endif
+#ifdef KSZ_MRP
+#include "ksz_mrp.h"
+#endif
+#ifdef KSZ_DLR
+#include "ksz_dlr.h"
+#endif
+
+
+#define LEARNED_MAC_TABLE_ENTRIES	1024
+#define STATIC_MAC_TABLE_ENTRIES	16
+#define RESERVED_MCAST_TABLE_ENTRIES	0x30
+#define ACTUAL_MCAST_TABLE_ENTRIES	8
+#define SWITCH_MAC_TABLE_ENTRIES	16
+#define MULTI_MAC_TABLE_ENTRIES		40
+
+#define RX_TABLE_ENTRIES		128
+#define TX_TABLE_ENTRIES		8
+
+#define BLOCKED_RX_ENTRIES		8
+
+struct ksz_frame_table {
+	u32 crc;
+	int cnt;
+	int port;
+	unsigned long expired;
+};
+
+struct ksz_rx_table {
+	struct ksz_frame_table table[RX_TABLE_ENTRIES];
+	int cnt;
+};
+
+struct ksz_tx_table {
+	struct ksz_frame_table table[TX_TABLE_ENTRIES];
+	int cnt;
+};
+
+/**
+ * struct ksz_mac_table - Static MAC table data structure
+ * @mac_addr:	MAC address to filter.
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @override:	Override setting.
+ * @use_fid:	FID use setting.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_mac_table {
+	u8 addr[ETH_ALEN];
+	u32 ports;
+	u16 fid;
+	u8 mstp;
+	u8 prio;
+	u8 src:1;
+	u8 dst:1;
+	u8 override:1;
+	u8 use_fid:1;
+	u8 valid:1;
+	u8 ignore_use_fid:1;
+	u8 dirty:1;
+};
+
+#define FWD_HOST_OVERRIDE		(1 << 0)
+#define FWD_HOST			(1 << 1)
+#define FWD_STP_DEV			(1 << 2)
+#define FWD_MAIN_DEV			(1 << 3)
+#define FWD_VLAN_DEV			(1 << 4)
+
+struct ksz_alu_table {
+	u16 index;
+	u16 owner;
+	u8 forward;
+	u8 type;
+	u8 valid:1;
+};
+
+#define VLAN_TABLE_ENTRIES		16
+
+/**
+ * struct ksz_vlan_table - VLAN table data structure
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @untag:	Untag membership.
+ * @mstp:	MSTP number.
+ * @prio:	Priority
+ * @fo:		Forward option.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_vlan_table {
+	u16 vid;
+	u16 fid;
+	u32 ports;
+	u32 untag;
+	u8 mstp;
+	u8 prio;
+	u8 option:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define PRIO_802_1P_ENTRIES		8
+
+#define DIFFSERV_ENTRIES		64
+
+#define ACL_TABLE_ENTRIES		16
+
+struct ksz_acl_table {
+	u16 first_rule;
+	u16 ruleset;
+	u8 mac[ETH_ALEN];
+	u16 eth_type;
+	u8 protocol;
+	u8 ip4_addr[4];
+	u8 ip4_mask[4];
+	u32 seqnum;
+	u16 max_port;
+	u16 min_port;
+	u8 prio;
+	u8 vlan_prio;
+	u16 ports;
+	u16 cnt;
+	u8 tcp_flag_mask;
+	u8 tcp_flag;
+#if 0
+	u8 ip6_addr[16];
+	u8 ip6_mask[16];
+#endif
+	u32 mode:2;
+	u32 enable:2;
+	u32 src:1;
+	u32 equal:1;
+	u32 port_mode:2;
+	u32 tcp_flag_enable:1;
+	u32 msec:1;
+	u32 intr_mode:1;
+	u32 prio_mode:2;
+	u32 vlan_prio_replace:1;
+	u32 map_mode:2;
+	u32 changed:1;
+	u32 action_changed:1;
+	u32 action_selected:1;
+
+	u8 data[ACL_TABLE_LEN];
+};
+
+/**
+ * struct ksz_port_mib - Port MIB data structure
+ * @cnt_ptr:	Current pointer to MIB counter index.
+ * @mib_start:	The starting counter index.  Some ports do not start at 0.
+ * @counter:	64-bit MIB counter value.
+ * @dropped:	Temporary buffer to remember last read packet dropped values.
+ * @read_cnt:	Used to signal when to read the MIB counter.
+ * @read_max:	Used to indicate how often to read the MIB counter.
+ *
+ * MIB counters needs to be read periodically so that counters do not get
+ * overflowed and give incorrect values.  A right balance is needed to
+ * satisfy this condition and not waste too much CPU time.
+ */
+struct ksz_port_mib {
+	u8 cnt_ptr;
+	u8 mib_start;
+	u8 interval;
+	u8 reserved[1];
+
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+#if 0
+	u32 dropped[2];
+	u8 read_cnt[SWITCH_COUNTER_NUM];
+	u8 read_max[SWITCH_COUNTER_NUM];
+#endif
+};
+
+enum {
+	STP_STATE_DISABLED = 0,
+	STP_STATE_LISTENING,
+	STP_STATE_LEARNING,
+	STP_STATE_FORWARDING,
+	STP_STATE_BLOCKED,
+	STP_STATE_SIMPLE
+};
+
+/**
+ * struct ksz_port_cfg - Port configuration data structure
+ * @vid:	VID value.
+ * @member:	Port membership.
+ * @port_prio:	Port priority.
+ * @rate_ctrl:	Priority rate control.
+ * @rx_rate:	Receive priority rate.
+ * @tx_rate:	Transmit priority rate.
+ * @rate_limit: Priority rate limit value.
+ * @stp_state:	Current Spanning Tree Protocol state.
+ */
+struct ksz_port_cfg {
+	u16 vid;
+	u16 member;
+	u8 rate_ctrl[PRIO_QUEUES];
+	u32 rx_packet[RX_PRIO_QUEUES];
+	u32 rx_rate[RX_PRIO_QUEUES];
+	u32 tx_packet[PRIO_QUEUES];
+	u32 tx_rate[PRIO_QUEUES];
+	u32 color_map[DIFFSERV_ENTRIES / 16];
+	u32 tc_map[PRIO_802_1P_ENTRIES / 8];
+	u8 p_index;
+	u8 q_index;
+	u8 port_prio;
+	u8 rate_limit;
+	int packet_based;
+	u16 intr_mask;
+	int stp_state;
+
+	struct ksz_acl_table acl_info[ACL_TABLE_ENTRIES];
+	u16 acl_index;
+	u16 acl_act_index;
+
+	u16 mmd_id;
+	u16 mmd_reg;
+};
+
+/**
+ * struct ksz_sw_info - KSZ9897 switch information data structure
+ * @mac_table:	MAC table entries information.
+ * @multi_net:	Network multicast addresses used.
+ * @multi_sys:	System multicast addresses used.
+ * @blocked_rx:	Blocked receive addresses.
+ * @blocked_rx_cnt: Blocked receive addresses count.
+ * @vlan_table:	VLAN table entries information.
+ * @port_cfg:	Port configuration information.
+ * @rx_table:	Receive frame information.
+ * @tx_table:	Transmit frame information.
+ * @diffserv:	DiffServ priority settings.  Possible values from 6-bit of ToS
+ *		(bit7 ~ bit2) field.
+ * @p_802_1p:	802.1P priority settings.  Possible values from 3-bit of 802.1p
+ *		Tag priority field.
+ * @br_addr:	Bridge address.  Used for STP.
+ * @mac_addr:	Switch MAC address.
+ * @broad_per:	Broadcast storm percentage.
+ * @member:	Current port membership.  Used for STP.
+ * @stp:	STP port membership.  Used for STP.
+ * @stp_down:	STP port down membership.  Used for STP.
+ * @phy_addr:	PHY address used by first port.
+ */
+struct ksz_sw_info {
+	struct ksz_mac_table mac_table[MULTI_MAC_TABLE_ENTRIES];
+	struct ksz_alu_table alu_table[MULTI_MAC_TABLE_ENTRIES];
+	int multi_net;
+	int multi_sys;
+	u8 blocked_rx[BLOCKED_RX_ENTRIES][ETH_ALEN];
+	int blocked_rx_cnt;
+	struct ksz_port_cfg port_cfg[TOTAL_PORT_NUM];
+	struct ksz_rx_table rx_table;
+	struct ksz_tx_table tx_table;
+#ifdef KSZ_IBA
+	struct ksz_iba_info iba;
+#endif
+#ifdef KSZ_DLR
+	struct ksz_dlr_info dlr;
+#endif
+	struct ksz_mac_table mac_entry;
+	struct ksz_vlan_table vlan_entry;
+
+	SW_D diffserv[DIFFSERV_ENTRIES / KS_PRIO_IN_REG];
+	SW_D p_802_1p[PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG];
+
+	u8 br_addr[ETH_ALEN];
+	u8 mac_addr[ETH_ALEN];
+
+	u8 broad_per;
+	u8 member;
+	u8 stp;
+	u8 stp_down;
+	u8 fwd_ports;
+	u8 phy_addr;
+};
+
+/**
+ * struct ksz_port_state - Port state information data structure
+ * @state:	Connection status of the port.
+ * @link_down:	Indication the link has just gone down.
+ *
+ * It is pointless to read MIB counters when the port is disconnected.  The
+ * @state provides the connection status so that MIB counters are read only
+ * when the port is connected.  The @link_down indicates the port is just
+ * disconnected so that all MIB counters are read one last time to update the
+ * information.
+ */
+struct ksz_port_state {
+	uint state;
+	uint tx_rate;
+	u8 link_down;
+};
+
+#define TX_RATE_UNIT			10000
+
+/**
+ * struct ksz_port_info - Port information data structure
+ * @interface:	PHY interface.
+ * @state:	Connection status of the port.
+ * @tx_rate:	Transmit rate divided by 10000 to get Mbit.
+ * @duplex:	Duplex mode.
+ * @flow_ctrl:	Flow control.
+ * @link:	Link status.  Used to determine link.
+ * @advertised:	Advertised auto-negotiation setting.  Used to determine link.
+ * @partner:	Auto-negotiation partner setting.  Used to determine link.
+ * @status:	LinkMD status values.
+ * @length:	LinkMD length values.
+ * @mac_addr:	MAC address of the port.
+ */
+struct ksz_port_info {
+	phy_interface_t interface;
+	uint state;
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+	u8 link;
+	u8 link_down;
+	u32 advertised;
+	u32 partner;
+	u32 status[5];
+	u32 length[5];
+	u8 mac_addr[ETH_ALEN];
+	u32 report:1;
+};
+
+struct ksz_sw;
+struct ksz_port;
+
+struct ksz_sw_reg_ops {
+	void (*lock)(struct ksz_sw *sw);
+	void (*unlock)(struct ksz_sw *sw);
+
+	u8 (*r8)(struct ksz_sw *sw, unsigned reg);
+	u16 (*r16)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r24)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r32)(struct ksz_sw *sw, unsigned reg);
+	void (*w8)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w16)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w24)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w32)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	void (*r)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+	void (*w)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+
+	int (*get)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+	int (*set)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+
+	void (*r_dyn_mac_hw)(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+		u16 src_fid, struct ksz_mac_table *mac, u16 *entry);
+	void (*w_dyn_mac_hw)(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+		u16 src_fid, struct ksz_mac_table *mac);
+	void (*start_dyn_mac_hw)(struct ksz_sw *sw);
+	void (*g_dyn_mac_hw)(struct ksz_sw *sw, struct ksz_mac_table *mac);
+	u32 (*stop_dyn_mac_hw)(struct ksz_sw *sw);
+	void (*r_sta_mac_hw)(struct ksz_sw *sw, u32 ctrl[], int num,
+		struct ksz_mac_table *mac);
+	void (*w_sta_mac_hw)(struct ksz_sw *sw, u32 ctrl[], int num,
+		struct ksz_mac_table *mac);
+	void (*r_vlan_hw)(struct ksz_sw *sw, u32 data[], int num);
+	void (*w_vlan_hw)(struct ksz_sw *sw, u32 data[], int num);
+	void (*r_mib_cnt_hw)(struct ksz_sw *sw, int port, u32 addr[],
+		int num, u32 data[]);
+	void (*r_acl_hw)(struct ksz_sw *sw, int port, u16 addr, u8 data[]);
+	void (*w_acl_hw)(struct ksz_sw *sw, int port, u16 addr, u8 data[]);
+};
+
+struct ksz_sw_net_ops {
+	void (*setup_special)(struct ksz_sw *sw, int *port_cnt,
+		int *mib_port_cnt, int *dev_cnt);
+	void (*setup_dev)(struct ksz_sw *sw, struct net_device *dev,
+		char *dev_name, struct ksz_port *port, int i, int port_cnt,
+		int mib_port_cnt);
+	u8 (*get_state)(struct net_device *dev);
+	void (*set_state)(struct net_device *dev, u8 state);
+	struct ksz_port *(*get_priv_port)(struct net_device *dev);
+
+	void (*start)(struct ksz_sw *sw, u8 *addr);
+	int (*stop)(struct ksz_sw *sw, int complete);
+	void (*open_dev)(struct ksz_sw *sw, struct net_device *dev, u8 *addr);
+	void (*open_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port, u8 *state);
+	void (*close_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*open)(struct ksz_sw *sw);
+	void (*close)(struct ksz_sw *sw);
+
+	void (*netdev_start_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_stop_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_wake_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_open_port)(struct ksz_sw *sw, struct net_device *dev);
+
+	u8 (*set_mac_addr)(struct ksz_sw *sw, struct net_device *dev,
+		u8 promiscuous, int port);
+
+	int (*get_tx_len)(struct ksz_sw *sw, struct sk_buff *skb);
+	void (*add_tail_tag)(struct ksz_sw *sw, struct sk_buff *skb, int ports);
+	int (*get_tail_tag)(u8 *trailer, int *port);
+	struct sk_buff *(*check_tx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, struct ksz_port *priv);
+	struct net_device *(*rx_dev)(struct ksz_sw *sw, u8 *data, u32 *len,
+		int *tag, int *port);
+	int (*match_pkt)(struct ksz_sw *sw, struct net_device **dev,
+		void **priv, int (*get_promiscuous)(void *ptr),
+		int (*match_multi)(void *ptr, u8 *data),
+		struct sk_buff *skb, u8 h_promiscuous);
+	struct net_device *(*parent_rx)(struct ksz_sw *sw,
+		struct net_device *dev, struct sk_buff *skb, int forward,
+		struct net_device **parent_dev, struct sk_buff **parent_skb);
+	int (*port_vlan_rx)(struct ksz_sw *sw, struct net_device *dev,
+		struct net_device *parent_dev, struct sk_buff *skb,
+		int forward, int tag, void *ptr,
+		void (*rx_tstamp)(void *ptr, struct sk_buff *skb));
+	int (*drop_icmp)(struct sk_buff *skb, int extra_skb);
+	struct sk_buff *(*final_skb)(struct ksz_sw *sw, struct sk_buff *skb,
+		struct net_device *dev, struct ksz_port *port);
+	int (*drv_rx)(struct ksz_sw *sw, struct sk_buff *skb, int port);
+
+#ifdef CONFIG_KSZ_STP
+	u8 (*get_port_state)(struct net_device *dev,
+		struct net_device **br_dev);
+
+	void (*set_multi)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *priv);
+	int (*stp_rx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, int port, int *forward);
+	int (*blocked_rx)(struct ksz_sw *sw, u8 *data);
+	void (*monitor_ports)(struct ksz_sw *sw);
+#endif
+};
+
+struct ksz_sw_ops {
+	void (*acquire)(struct ksz_sw *sw);
+	void (*release)(struct ksz_sw *sw);
+
+	int (*chk)(struct ksz_sw *sw, u32 addr, SW_D bits);
+	void (*cfg)(struct ksz_sw *sw, u32 addr, SW_D bits, int set);
+
+	int (*port_get_link_speed)(struct ksz_port *port);
+	void (*port_set_link_speed)(struct ksz_port *port);
+	void (*port_force_link_speed)(struct ksz_port *port);
+
+	int (*port_r_cnt)(struct ksz_sw *sw, int port);
+	void (*get_mib_counters)(struct ksz_sw *sw, int first, int cnt,
+		u64 *counter);
+
+	ssize_t (*sysfs_read)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, ssize_t len, char *buf);
+	ssize_t (*sysfs_read_hw)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_write)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, int num, const char *buf);
+	ssize_t (*sysfs_port_read)(struct ksz_sw *sw, int proc_num, int port,
+		ssize_t len, char *buf);
+	ssize_t (*sysfs_port_read_hw)(struct ksz_sw *sw, int proc_num,
+		int port, ssize_t len, char *buf);
+	int (*sysfs_port_write)(struct ksz_sw *sw, int proc_num, int port,
+		int num, const char *buf);
+	ssize_t (*sysfs_mac_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mac_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_vlan_read)(struct ksz_sw *sw, int proc_num,
+		ssize_t len, char *buf);
+	int (*sysfs_vlan_write)(struct ksz_sw *sw, int proc_num, int num);
+	ssize_t (*sysfs_acl_read)(struct ksz_sw *sw, int proc_num, int port,
+		ssize_t len, char *buf);
+	int (*sysfs_acl_write)(struct ksz_sw *sw, int proc_num, int port,
+		int num, const char *buf);
+
+	void (*cfg_mac)(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+		int override, int use_fid, u16 fid);
+	void (*cfg_vlan)(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+		u32 ports);
+	u8 (*alloc_mac)(struct ksz_sw *sw);
+	void (*free_mac)(struct ksz_sw *sw, u8 index);
+	u8 (*alloc_vlan)(struct ksz_sw *sw);
+	void (*free_vlan)(struct ksz_sw *sw, u8 index);
+
+	int (*get_id)(struct ksz_sw *sw, u8 *id1, u8 *id2);
+	void (*cfg_tail_tag)(struct ksz_sw *sw, int enable);
+	void (*cfg_each_port)(struct ksz_sw *sw, int p, int cpu);
+	int (*port_to_phy_addr)(struct ksz_sw *sw, int p);
+	void (*set_port_addr)(struct ksz_sw *sw, int p, u8 *addr);
+
+	void (*cfg_src_filter)(struct ksz_sw *sw, int set);
+	void (*flush_table)(struct ksz_sw *sw, int port);
+
+};
+
+struct ksz_sw_tx_tag {
+	u32 timestamp;
+	u16 ports;
+};
+
+struct ksz_sw_cached_regs {
+	u32 ptp_unit_index;
+	u16 ptp_clk_ctrl;
+};
+
+/* Switch features and bug fixes. */
+#define STP_SUPPORT			(1 << 0)
+#define VLAN_PORT			(1 << 1)
+#define VLAN_PORT_REMOVE_TAG		(1 << 2)
+#define VLAN_PORT_TAGGING		(1 << 3)
+#define VLAN_PORT_START			200
+#define MRP_SUPPORT			(1 << 4)
+
+#define ACL_CORRUPT_BUG			(1 << 8)
+#define NO_GLOBAL_RESET			(1 << 9)
+#define SETUP_PHY			(1 << 16)
+
+#define IBA_SUPPORT			(1 << 20)
+#define NEW_CAP				(1 << 21)
+#define AVB_SUPPORT			(1 << 22)
+#define REDUNDANCY_SUPPORT		(1 << 23)
+#define DLR_HW				(1 << 24)
+#define HSR_HW				(1 << 25)
+#define DSA_SUPPORT			(1 << 28)
+#define DIFF_MAC_ADDR			(1 << 29)
+#define QW_HW				(1 << 30)
+#define PTP_HW				(1 << 31)
+
+/* Software overrides. */
+#define PAUSE_FLOW_CTRL			(1 << 0)
+#define FAST_AGING			(1 << 1)
+
+#define IBA_TEST			(1 << 16)
+#define ACL_INTR_MONITOR		(1 << 17)
+
+#define TAIL_PRP_0			(1 << 24)
+#define TAIL_PRP_1			(1 << 25)
+
+#define VLAN_SET			(1 << 28)
+#define PTP_TAG				(1 << 29)
+#define TAG_REMOVE			(1 << 30)
+#define TAIL_TAGGING			(1 << 31)
+
+/**
+ * struct ksz_sw - Virtual switch data structure
+ * @dev:		Pointer to hardware device.
+ * @phydev:		Pointer to PHY device interface.
+ * @interface:		The R/G/MII interface used.
+ * @msg_enable:		The message flags controlling driver output.
+ * @hwlock:		Pointer to hardware lock.
+ * @reglock:		Pointer to register lock.
+ * @acllock:		ACL table lock.
+ * @alulock:		ALU table lock.
+ * @vlanlock:		VLAN table lock.
+ * @lock		Software lock to switch structure.
+ * @locked:		locked status.
+ * @info:		Pointer to switch information structure.
+ * @port_info:		Port information.
+ * @netdev:		Pointer to OS dependent network devices.
+ * @phy:		Pointer to OS dependent PHY devices.
+ * @dev_offset:		Indication of a switch associated network device.
+ * @phy_offset:		Indication of a port associated PHY device.
+ * @port_state:		Port state information.
+ * @port_mib:		Port MIB information.
+ * @mib_cnt:		Number of MIB counters this switch has.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @phy_port_cnt:	Number of ports with actual PHY.
+ * @port_cnt:		Number of ports to support.
+ * @monitor_timer_info:	Timer information for monitoring ports.
+ * @counter:		Pointer to OS dependent MIB counter information.
+ * @link_read:		Workqueue for link monitoring.
+ * @stp_monitor:	Workqueue for STP monitoring.
+ * @ops:		Switch function access.
+ * @reg:		Switch register access.
+ * @net_ops:		Network related switch function access.
+ * @HOST_PORT:		A predefined value indicating the host port.
+ * @HOST_MASK:		A predefined value indicating the host port mask.
+ * @PORT_MASK:		A predefined value indicating the port mask.
+ * @TAIL_TAG_LOOKUP:	A predefined value indicating tx tail tag lookup.
+ * @TAIL_TAG_OVERRIDE:	A predefined value indicating tx tail tag override.
+ * @rx_ports:		Bitmap of ports with receive enabled.
+ * @tx_ports:		Bitmap of ports with transmit enabled.
+ * @dev_count:		Number of network devices this switch supports.
+ * @id:			Hardware ID.  Used for display only.
+ * @vlan_id		Used for the VLAN port forwarding feature.
+ * @vid:		Used for the VLAN port forwarding feature.
+ * @features:		Switch features to enable.
+ * @overrides:		Switch features to override.
+ * @need_link_up:	Indicate whether link should be always up.
+ * @link_change:	Used to indicate link is up after device open.
+ * @multi_dev:		Used to specify multiple devices mode.
+ * @stp:		Used to enable STP.
+ * @fast_aging:		Used to enable fast aging.
+ */
+struct ksz_sw {
+	void *dev;
+	void *phydev;
+	phy_interface_t interface;
+	u32 msg_enable;
+	wait_queue_head_t queue;
+	struct mutex *hwlock;
+	struct mutex *reglock;
+	struct mutex acllock;
+	struct mutex alulock;
+	struct mutex vlanlock;
+	struct mutex lock;
+	int intr_using;
+
+	struct ksz_sw_info *info;
+	struct ksz_port_info port_info[TOTAL_PORT_NUM];
+	struct net_device *main_dev;
+	struct net_device *netdev[TOTAL_PORT_NUM];
+	struct phy_device *phy[TOTAL_PORT_NUM + 1];
+	int dev_offset;
+	int phy_offset;
+	struct ksz_port_state port_state[TOTAL_PORT_NUM];
+	struct ksz_port_mib port_mib[TOTAL_PORT_NUM];
+	u8 mib_interval_start[4];
+	unsigned long next_jiffies;
+	int mib_cnt;
+	int mib_port_cnt;
+	int phy_port_cnt;
+	int port_cnt;
+	struct ksz_timer_info *monitor_timer_info;
+	struct ksz_counter_info *counter;
+	struct delayed_work *link_read;
+	struct delayed_work *stp_monitor;
+
+	const struct ksz_sw_ops *ops;
+	const struct ksz_sw_reg_ops *reg;
+	const struct ksz_sw_reg_ops *cur;
+	struct ksz_sw_net_ops *net_ops;
+	struct delayed_work set_ops;
+	struct work_struct set_addr;
+
+	int HOST_PORT;
+	u16 HOST_MASK;
+	u16 PORT_MASK;
+	u16 TAIL_TAG_LOOKUP;
+	u16 TAIL_TAG_OVERRIDE;
+	u32 intr_mask;
+	u32 port_intr_mask;
+	u32 phy_intr;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 tx_pad[60];
+	int tx_start;
+	struct ksz_sw_tx_tag tag;
+	struct ksz_sw_cached_regs cached;
+
+	int dev_count;
+	int id;
+	u32 vlan_id;
+	u16 vid;
+	u16 alu_index;
+	u8 alu_type;
+	u8 alu_dirty;
+	u16 vlan_index;
+	u8 vlan_dirty;
+
+	uint features;
+	uint overrides;
+
+	int need_link_up;
+	int link_change;
+
+	int multi_dev;
+	int stp;
+	int fast_aging;
+
+#ifdef KSZ_MRP
+	struct mrp_info mrp;
+#endif
+
+#ifdef CONFIG_1588_PTP
+	/* PTP structure size can be variable. */
+	struct ptp_info ptp_hw;
+#endif
+};
+
+struct ksz_sw_sysfs {
+	struct ksz_dev_attr *ksz_port_attrs[TOTAL_PORT_NUM];
+	struct attribute **port_attrs[TOTAL_PORT_NUM];
+};
+
+/**
+ * struct ksz_port - Virtual port data structure
+ * @first_port:		Index of first port this port supports.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports this port supports.
+ * @flow_ctrl:		Flow control setting.  PHY_NO_FLOW_CTRL for no flow
+ *			control, and PHY_FLOW_CTRL for flow control.
+ *			PHY_TX_ONLY and PHY_RX_ONLY are not supported for 100
+ *			Mbit PHY.
+ * @duplex:		Duplex mode setting.  1 for half duplex, 2 for full
+ *			duplex, and 0 for auto, which normally results in full
+ *			duplex.
+ * @speed:		Speed setting.  10 for 10 Mbit, 100 for 100 Mbit, and
+ *			0 for auto, which normally results in 100 Mbit.
+ * @force_link:		Force link setting.  0 for auto-negotiation, and 1 for
+ *			force.
+ * @linked:		Pointer to port information linked to this port.
+ * @sw:			Pointer to virtual switch structure.
+ */
+struct ksz_port {
+	int first_port;
+	int mib_port_cnt;
+	int port_cnt;
+
+	u8 flow_ctrl;
+	u8 duplex;
+	u16 speed;
+	u8 force_link;
+
+	struct ksz_port_info *linked;
+
+	struct ksz_sw *sw;
+	struct work_struct link_update;
+};
+
+struct lan_attributes {
+	int info;
+	int duplex;
+	int speed;
+	int force;
+	int flow_ctrl;
+	int features;
+	int overrides;
+	int mib;
+	int reg;
+	int vid;
+	int dynamic_table;
+	int static_table;
+	int vlan_table;
+	int aging;
+	int fast_aging;
+	int link_aging;
+	int bcast_per;
+	int mcast_storm;
+	int diffserv_map;
+	int p_802_1p_map;
+	int vlan;
+	int null_vid;
+	int drop_inv_vid;
+	int macaddr;
+	int mirror_mode;
+	int igmp_snoop;
+	int ipv6_mld_snoop;
+	int ipv6_mld_option;
+	int aggr_backoff;
+	int no_exc_drop;
+	int jumbo_packet;
+	int legal_packet;
+	int length_check;
+	int back_pressure;
+	int sw_flow_ctrl;
+	int sw_half_duplex;
+	int sw_10_mbit;
+	int fair_flow_ctrl;
+	int vlan_bound;
+	int double_tag;
+	int isp;
+	int hsr;
+	int mtu;
+	int unk_ucast_fwd;
+	int unk_ucast_ports;
+	int unk_mcast_fwd;
+	int unk_mcast_ports;
+	int unk_vid_fwd;
+	int unk_vid_ports;
+	int pass_pause;
+	int pme;
+	int pme_polarity;
+
+	int host_port;
+	int ports;
+	int dev_start;
+	int vlan_start;
+	int stp;
+
+	int alu_fid;
+	int alu_use_fid;
+	int alu_override;
+	int alu_valid;
+	int alu_mstp;
+	int alu_prio;
+	int alu_src;
+	int alu_dst;
+	int alu_ports;
+	int alu_addr;
+	int alu_type;
+	int alu_index;
+	int alu_info;
+
+	int vlan_valid;
+	int vlan_ports;
+	int vlan_untag;
+	int vlan_fid;
+	int vlan_mstp;
+	int vlan_prio;
+	int vlan_option;
+	int vlan_index;
+	int vlan_info;
+
+	int no_color;
+	int color_red;
+	int color_yellow;
+	int color_green;
+};
+
+struct sw_attributes {
+	int mib;
+	int vid;
+	int member;
+	int bcast_storm;
+	int diffserv;
+	int p_802_1p;
+	int prio_vlan;
+	int prio_mac;
+	int prio_acl;
+	int prio_highest;
+	int prio_or;
+	int port_prio;
+	int non_vid;
+	int ingress;
+	int drop_non_vlan;
+	int drop_tagged;
+	int replace_vid;
+	int replace_prio;
+	int src_addr_filter;
+	int vlan_lookup_0;
+	int mstp;
+	int rx;
+	int tx;
+	int learn;
+	int prio_queue;
+	int prio_rate;
+	int limit;
+	int limit_port_based;
+	int limit_packet_based;
+	int limit_flow_ctrl;
+	int limit_cnt_ifg;
+	int limit_cnt_pre;
+	int rx_p0_rate;
+	int rx_p1_rate;
+	int rx_p2_rate;
+	int rx_p3_rate;
+	int rx_p4_rate;
+	int rx_p5_rate;
+	int rx_p6_rate;
+	int rx_p7_rate;
+	int tx_q0_rate;
+	int tx_q1_rate;
+	int tx_q2_rate;
+	int tx_q3_rate;
+	int color_map;
+	int tc_map;
+	int mirror_port;
+	int mirror_rx;
+	int mirror_tx;
+	int back_pressure;
+	int force_flow_ctrl;
+	int pass_all;
+	int tail_tag;
+
+	int cust_vid;
+	int sr_1_vid;
+	int sr_2_vid;
+	int sr_1_type;
+	int sr_2_type;
+
+	int pme_ctrl;
+	int pme_status;
+
+	int authen_mode;
+	int acl;
+	int acl_first_rule;
+	int acl_ruleset;
+	int acl_mode;
+	int acl_enable;
+	int acl_src;
+	int acl_equal;
+	int acl_addr;
+	int acl_type;
+	int acl_cnt;
+	int acl_msec;
+	int acl_intr_mode;
+	int acl_ip_addr;
+	int acl_ip_mask;
+	int acl_protocol;
+	int acl_seqnum;
+	int acl_port_mode;
+	int acl_max_port;
+	int acl_min_port;
+	int acl_tcp_flag_enable;
+	int acl_tcp_flag;
+	int acl_tcp_flag_mask;
+	int acl_prio_mode;
+	int acl_prio;
+	int acl_vlan_prio_replace;
+	int acl_vlan_prio;
+	int acl_map_mode;
+	int acl_ports;
+	int acl_index;
+	int acl_act_index;
+	int acl_act;
+	int acl_info;
+	int acl_table;
+
+	int p_index;
+	int q_index;
+	int police_type;
+	int non_dscp_color;
+	int police_drop_all;
+	int police_port_based;
+	int color_mark;
+	int color_remap;
+	int drop_srp;
+	int color_aware;
+	int police;
+
+	int q_cir;
+	int q_pir;
+	int q_cbs;
+	int q_pbs;
+
+	int wred_max;
+	int wred_min;
+	int wred_multiplier;
+	int wred_avg_size;
+	int wred_q_max;
+	int wred_q_min;
+	int wred_q_multiplier;
+	int wred_q_avg_size;
+	int wred_random_drop;
+	int wred_drop_gyr;
+	int wred_drop_yr;
+	int wred_drop_r;
+	int wred_drop_all;
+	int wred_q_pmon;
+
+	int schedule;
+	int shaping;
+#ifdef MTI_PREEMPT_ENABLE
+	int preempt;
+#endif
+	int tx_ratio;
+	int credit_hi;
+	int credit_lo;
+	int credit_incr;
+	int srp;
+
+	int qm_drop;
+	int qm_burst;
+	int qm_resv_space;
+	int qm_hi;
+	int qm_lo;
+	int qm_tx_used;
+	int qm_tx_avail;
+	int qm_tx_calc;
+
+	int mmd_id;
+	int mmd_reg;
+	int mmd_val;
+
+	int rx_flow_ctrl;
+	int tx_flow_ctrl;
+
+	int duplex;
+	int speed;
+	int linkmd;
+};
+
+static int alloc_dev_attr(struct attribute **attrs, size_t attr_size, int item,
+	struct ksz_dev_attr **ksz_attrs, struct attribute ***item_attrs,
+	char *item_name, struct ksz_dev_attr **attrs_ptr)
+{
+	struct attribute **attr_ptr;
+	struct device_attribute *dev_attr;
+	struct ksz_dev_attr *new_attr;
+
+	*item_attrs = kmalloc(attr_size * sizeof(void *), GFP_KERNEL);
+	if (!*item_attrs)
+		return -ENOMEM;
+
+	attr_size--;
+	attr_size *= sizeof(struct ksz_dev_attr);
+	*ksz_attrs = *attrs_ptr;
+	*attrs_ptr += attr_size / sizeof(struct ksz_dev_attr);
+
+	new_attr = *ksz_attrs;
+	attr_ptr = *item_attrs;
+	while (*attrs != NULL) {
+		if (item_name && !strcmp((*attrs)->name, item_name))
+			break;
+		dev_attr = container_of(*attrs, struct device_attribute, attr);
+		memcpy(new_attr, dev_attr, sizeof(struct device_attribute));
+		strncpy(new_attr->dev_name, (*attrs)->name, DEV_NAME_SIZE);
+		if (10 <= item && item <= 15)
+			new_attr->dev_name[0] = item - 10 + 'a';
+		else
+			new_attr->dev_name[0] = item + '0';
+		new_attr->dev_attr.attr.name = new_attr->dev_name;
+		*attr_ptr = &new_attr->dev_attr.attr;
+		new_attr++;
+		attr_ptr++;
+		attrs++;
+	}
+	*attr_ptr = NULL;
+	return 0;
+}
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_sw_phy.h b/drivers/net/ethernet/micrel/ksz_sw_phy.h
new file mode 100644
index 0000000..64e2ede
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_sw_phy.h
@@ -0,0 +1,65 @@
+/**
+ * Micrel switch PHY common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_PHY_H
+#define KSZ_PHY_H
+
+#define ADDR_SHIFT			14
+#define ADDR_8				1
+#define ADDR_16				2
+#define ADDR_24				3
+#define ADDR_32				4
+
+#define BANK_SHIFT			12
+
+#define PHY_REG(addr, reg)		\
+	(((addr) << ADDR_SHIFT) | (reg))
+
+#define PHY_BANK_REG(addr, bank, reg)	\
+	(((addr) << ADDR_SHIFT) | ((bank) << BANK_SHIFT) | (reg))
+
+/* Use PHY access if no direct access. */
+#ifndef SW_R8
+#define SW_R8(s, r)	phy_read(s->phydev, PHY_REG(ADDR_8, r))
+#define SW_W8(s, r, v)	phy_write(s->phydev, PHY_REG(ADDR_8, r), v)
+#define SW_R16(s, r)	phy_read(s->phydev, PHY_REG(ADDR_16, r))
+#define SW_W16(s, r, v)	phy_write(s->phydev, PHY_REG(ADDR_16, r), v)
+#define SW_R32(s, r)	phy_read(s->phydev, PHY_REG(ADDR_32, r))
+#define SW_W32(s, r, v) \
+	do { \
+		phy_write(s->phydev, PHY_REG(ADDR_32, (r) + 2), (v) >> 16); \
+		phy_write(s->phydev, PHY_REG(ADDR_32, r), v); \
+	} while (0)
+#define SW_LOCK(s)				\
+	do {					\
+		mutex_lock(s->hwlock);		\
+	} while (0)
+#define SW_UNLOCK(s)				\
+	do {					\
+		mutex_unlock(s->hwlock);	\
+	} while (0)
+#endif
+
+struct phy_priv {
+	int ptp_irq;
+	int ptp_using;
+	struct ksz_port port;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c b/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c
new file mode 100644
index 0000000..4d9e917
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c
@@ -0,0 +1,816 @@
+/**
+ * Micrel gigabit switch common sysfs code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2011-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static char *sw_name[] = {
+	"sw0",
+	"sw1",
+	"sw2",
+	"sw3",
+	"sw4",
+	"sw5",
+	"sw6",
+	"sw7",
+};
+
+static ssize_t netlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_port *uninitialized_var(port);
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = sw->ops->sysfs_read(sw, proc_num, port, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_mac_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_vlan_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_read_hw(sw, proc_num, len, buf);
+	sw->ops->release(sw);
+
+netlan_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_port *uninitialized_var(port);
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_mac_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+
+	if (sw->ops->sysfs_vlan_write(sw, proc_num, num))
+		goto netlan_store_done;
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netlan_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netsw_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	int port;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+	len = sw->ops->sysfs_acl_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_port_read_hw(sw, num, port, len, buf);
+	sw->ops->release(sw);
+
+netsw_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netsw_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int port;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_acl_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_port_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netsw_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+#define LAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute lan_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETLAN_RD_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO, show_lan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETLAN_WR_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_lan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netlan_store(d, attr, buf, count,			\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO | S_IWUSR, show_lan_##name, store_lan_##name)
+
+#define SW_ATTR(_name, _mode, _show, _store) \
+struct device_attribute sw_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETSW_RD_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO, show_sw_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETSW_WR_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static ssize_t store_sw_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netsw_store(d, attr, buf, count,				\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO | S_IWUSR, show_sw_##name, store_sw_##name)
+
+NETLAN_WR_ENTRY(info);
+NETLAN_WR_ENTRY(duplex);
+NETLAN_WR_ENTRY(speed);
+NETLAN_WR_ENTRY(force);
+NETLAN_WR_ENTRY(flow_ctrl);
+NETLAN_WR_ENTRY(mib);
+NETLAN_WR_ENTRY(reg);
+NETLAN_WR_ENTRY(vid);
+NETLAN_WR_ENTRY(features);
+NETLAN_WR_ENTRY(overrides);
+
+NETLAN_WR_ENTRY(dynamic_table);
+NETLAN_WR_ENTRY(static_table);
+NETLAN_RD_ENTRY(vlan_table);
+NETLAN_WR_ENTRY(aging);
+NETLAN_WR_ENTRY(fast_aging);
+NETLAN_WR_ENTRY(link_aging);
+NETLAN_WR_ENTRY(bcast_per);
+NETLAN_WR_ENTRY(mcast_storm);
+NETLAN_WR_ENTRY(diffserv_map);
+NETLAN_WR_ENTRY(p_802_1p_map);
+NETLAN_WR_ENTRY(vlan);
+NETLAN_WR_ENTRY(null_vid);
+NETLAN_WR_ENTRY(drop_inv_vid);
+NETLAN_WR_ENTRY(macaddr);
+NETLAN_WR_ENTRY(mirror_mode);
+NETLAN_WR_ENTRY(igmp_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_option);
+NETLAN_WR_ENTRY(aggr_backoff);
+NETLAN_WR_ENTRY(no_exc_drop);
+NETLAN_WR_ENTRY(jumbo_packet);
+NETLAN_WR_ENTRY(legal_packet);
+NETLAN_WR_ENTRY(length_check);
+NETLAN_WR_ENTRY(back_pressure);
+NETLAN_WR_ENTRY(sw_flow_ctrl);
+NETLAN_WR_ENTRY(sw_half_duplex);
+#ifdef SWITCH_10_MBIT
+NETLAN_WR_ENTRY(sw_10_mbit);
+#endif
+NETLAN_WR_ENTRY(fair_flow_ctrl);
+NETLAN_WR_ENTRY(vlan_bound);
+NETLAN_WR_ENTRY(double_tag);
+NETLAN_WR_ENTRY(isp);
+NETLAN_WR_ENTRY(hsr);
+NETLAN_WR_ENTRY(mtu);
+NETLAN_WR_ENTRY(unk_ucast_fwd);
+NETLAN_WR_ENTRY(unk_ucast_ports);
+NETLAN_WR_ENTRY(unk_mcast_fwd);
+NETLAN_WR_ENTRY(unk_mcast_ports);
+NETLAN_WR_ENTRY(unk_vid_fwd);
+NETLAN_WR_ENTRY(unk_vid_ports);
+NETLAN_WR_ENTRY(pass_pause);
+NETLAN_WR_ENTRY(pme);
+NETLAN_WR_ENTRY(pme_polarity);
+
+NETLAN_RD_ENTRY(host_port);
+NETLAN_RD_ENTRY(ports);
+NETLAN_RD_ENTRY(dev_start);
+NETLAN_RD_ENTRY(vlan_start);
+NETLAN_RD_ENTRY(stp);
+
+NETLAN_WR_ENTRY(alu_fid);
+NETLAN_WR_ENTRY(alu_use_fid);
+NETLAN_WR_ENTRY(alu_override);
+NETLAN_WR_ENTRY(alu_valid);
+NETLAN_WR_ENTRY(alu_mstp);
+NETLAN_WR_ENTRY(alu_prio);
+NETLAN_WR_ENTRY(alu_src);
+NETLAN_WR_ENTRY(alu_dst);
+NETLAN_WR_ENTRY(alu_ports);
+NETLAN_WR_ENTRY(alu_addr);
+NETLAN_WR_ENTRY(alu_type);
+NETLAN_WR_ENTRY(alu_index);
+NETLAN_WR_ENTRY(alu_info);
+
+NETLAN_WR_ENTRY(vlan_valid);
+NETLAN_WR_ENTRY(vlan_ports);
+NETLAN_WR_ENTRY(vlan_untag);
+NETLAN_WR_ENTRY(vlan_fid);
+NETLAN_WR_ENTRY(vlan_mstp);
+NETLAN_WR_ENTRY(vlan_prio);
+NETLAN_WR_ENTRY(vlan_option);
+NETLAN_WR_ENTRY(vlan_index);
+NETLAN_WR_ENTRY(vlan_info);
+
+NETLAN_WR_ENTRY(no_color);
+NETLAN_WR_ENTRY(color_red);
+NETLAN_WR_ENTRY(color_yellow);
+NETLAN_WR_ENTRY(color_green);
+
+NETSW_WR_ENTRY(mib);
+NETSW_WR_ENTRY(vid);
+NETSW_WR_ENTRY(member);
+NETSW_WR_ENTRY(bcast_storm);
+NETSW_WR_ENTRY(mstp);
+NETSW_WR_ENTRY(rx);
+NETSW_WR_ENTRY(tx);
+NETSW_WR_ENTRY(learn);
+NETSW_WR_ENTRY(mirror_port);
+NETSW_WR_ENTRY(mirror_rx);
+NETSW_WR_ENTRY(mirror_tx);
+NETSW_WR_ENTRY(diffserv);
+NETSW_WR_ENTRY(p_802_1p);
+NETSW_WR_ENTRY(prio_vlan);
+NETSW_WR_ENTRY(prio_mac);
+NETSW_WR_ENTRY(prio_acl);
+NETSW_WR_ENTRY(prio_highest);
+NETSW_WR_ENTRY(prio_or);
+NETSW_WR_ENTRY(port_prio);
+NETSW_WR_ENTRY(non_vid);
+NETSW_WR_ENTRY(drop_non_vlan);
+NETSW_WR_ENTRY(drop_tagged);
+NETSW_WR_ENTRY(ingress);
+NETSW_WR_ENTRY(replace_vid);
+NETSW_WR_ENTRY(replace_prio);
+NETSW_WR_ENTRY(src_addr_filter);
+NETSW_WR_ENTRY(vlan_lookup_0);
+NETSW_WR_ENTRY(prio_queue);
+NETSW_WR_ENTRY(prio_rate);
+NETSW_WR_ENTRY(limit);
+NETSW_WR_ENTRY(limit_port_based);
+NETSW_WR_ENTRY(limit_packet_based);
+NETSW_WR_ENTRY(limit_flow_ctrl);
+NETSW_WR_ENTRY(limit_cnt_ifg);
+NETSW_WR_ENTRY(limit_cnt_pre);
+NETSW_WR_ENTRY(rx_p0_rate);
+NETSW_WR_ENTRY(rx_p1_rate);
+NETSW_WR_ENTRY(rx_p2_rate);
+NETSW_WR_ENTRY(rx_p3_rate);
+NETSW_WR_ENTRY(rx_p4_rate);
+NETSW_WR_ENTRY(rx_p5_rate);
+NETSW_WR_ENTRY(rx_p6_rate);
+NETSW_WR_ENTRY(rx_p7_rate);
+NETSW_WR_ENTRY(tx_q0_rate);
+NETSW_WR_ENTRY(tx_q1_rate);
+NETSW_WR_ENTRY(tx_q2_rate);
+NETSW_WR_ENTRY(tx_q3_rate);
+NETSW_WR_ENTRY(color_map);
+NETSW_WR_ENTRY(tc_map);
+NETSW_WR_ENTRY(back_pressure);
+NETSW_WR_ENTRY(force_flow_ctrl);
+NETSW_WR_ENTRY(pass_all);
+NETSW_WR_ENTRY(tail_tag);
+
+NETSW_WR_ENTRY(cust_vid);
+NETSW_WR_ENTRY(sr_1_vid);
+NETSW_WR_ENTRY(sr_2_vid);
+NETSW_WR_ENTRY(sr_1_type);
+NETSW_WR_ENTRY(sr_2_type);
+
+NETSW_WR_ENTRY(pme_ctrl);
+NETSW_WR_ENTRY(pme_status);
+
+NETSW_WR_ENTRY(authen_mode);
+NETSW_WR_ENTRY(acl);
+NETSW_WR_ENTRY(acl_first_rule);
+NETSW_WR_ENTRY(acl_ruleset);
+NETSW_WR_ENTRY(acl_mode);
+NETSW_WR_ENTRY(acl_enable);
+NETSW_WR_ENTRY(acl_src);
+NETSW_WR_ENTRY(acl_equal);
+NETSW_WR_ENTRY(acl_addr);
+NETSW_WR_ENTRY(acl_type);
+NETSW_WR_ENTRY(acl_cnt);
+NETSW_WR_ENTRY(acl_msec);
+NETSW_WR_ENTRY(acl_intr_mode);
+NETSW_WR_ENTRY(acl_ip_addr);
+NETSW_WR_ENTRY(acl_ip_mask);
+NETSW_WR_ENTRY(acl_protocol);
+NETSW_WR_ENTRY(acl_seqnum);
+NETSW_WR_ENTRY(acl_port_mode);
+NETSW_WR_ENTRY(acl_max_port);
+NETSW_WR_ENTRY(acl_min_port);
+NETSW_WR_ENTRY(acl_tcp_flag_enable);
+NETSW_WR_ENTRY(acl_tcp_flag);
+NETSW_WR_ENTRY(acl_tcp_flag_mask);
+NETSW_WR_ENTRY(acl_prio_mode);
+NETSW_WR_ENTRY(acl_prio);
+NETSW_WR_ENTRY(acl_vlan_prio_replace);
+NETSW_WR_ENTRY(acl_vlan_prio);
+NETSW_WR_ENTRY(acl_map_mode);
+NETSW_WR_ENTRY(acl_ports);
+NETSW_WR_ENTRY(acl_index);
+NETSW_WR_ENTRY(acl_act_index);
+NETSW_WR_ENTRY(acl_act);
+NETSW_WR_ENTRY(acl_info);
+NETSW_RD_ENTRY(acl_table);
+
+NETSW_WR_ENTRY(p_index);
+NETSW_WR_ENTRY(q_index);
+NETSW_WR_ENTRY(police_type);
+NETSW_WR_ENTRY(non_dscp_color);
+NETSW_WR_ENTRY(police_drop_all);
+NETSW_WR_ENTRY(police_port_based);
+NETSW_WR_ENTRY(color_mark);
+NETSW_WR_ENTRY(color_remap);
+NETSW_WR_ENTRY(drop_srp);
+NETSW_WR_ENTRY(color_aware);
+NETSW_WR_ENTRY(police);
+NETSW_WR_ENTRY(q_cir);
+NETSW_WR_ENTRY(q_pir);
+NETSW_WR_ENTRY(q_cbs);
+NETSW_WR_ENTRY(q_pbs);
+NETSW_WR_ENTRY(wred_max);
+NETSW_WR_ENTRY(wred_min);
+NETSW_WR_ENTRY(wred_multiplier);
+NETSW_RD_ENTRY(wred_avg_size);
+NETSW_WR_ENTRY(wred_q_max);
+NETSW_WR_ENTRY(wred_q_min);
+NETSW_WR_ENTRY(wred_q_multiplier);
+NETSW_RD_ENTRY(wred_q_avg_size);
+NETSW_WR_ENTRY(wred_random_drop);
+NETSW_WR_ENTRY(wred_drop_gyr);
+NETSW_WR_ENTRY(wred_drop_yr);
+NETSW_WR_ENTRY(wred_drop_r);
+NETSW_WR_ENTRY(wred_drop_all);
+NETSW_RD_ENTRY(wred_q_pmon);
+
+NETSW_WR_ENTRY(schedule);
+NETSW_WR_ENTRY(shaping);
+#ifdef MTI_PREEMPT_ENABLE
+NETSW_WR_ENTRY(preempt);
+#endif
+NETSW_WR_ENTRY(tx_ratio);
+NETSW_WR_ENTRY(credit_hi);
+NETSW_WR_ENTRY(credit_lo);
+NETSW_WR_ENTRY(credit_incr);
+NETSW_WR_ENTRY(srp);
+
+NETSW_WR_ENTRY(qm_drop);
+NETSW_WR_ENTRY(qm_burst);
+NETSW_WR_ENTRY(qm_resv_space);
+NETSW_WR_ENTRY(qm_hi);
+NETSW_WR_ENTRY(qm_lo);
+NETSW_RD_ENTRY(qm_tx_used);
+NETSW_RD_ENTRY(qm_tx_avail);
+NETSW_RD_ENTRY(qm_tx_calc);
+
+NETSW_WR_ENTRY(mmd_id);
+NETSW_WR_ENTRY(mmd_reg);
+NETSW_WR_ENTRY(mmd_val);
+
+NETSW_RD_ENTRY(rx_flow_ctrl);
+NETSW_RD_ENTRY(tx_flow_ctrl);
+
+NETSW_RD_ENTRY(duplex);
+NETSW_RD_ENTRY(speed);
+NETSW_WR_ENTRY(linkmd);
+
+static struct attribute *lan_attrs[] = {
+	&lan_attr_info.attr,
+#ifdef USE_SPEED_LINK
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+#endif
+#ifdef USE_MIB
+	&lan_attr_mib.attr,
+#endif
+	&lan_attr_reg.attr,
+	&lan_attr_vid.attr,
+	&lan_attr_features.attr,
+	&lan_attr_overrides.attr,
+
+	&lan_attr_dynamic_table.attr,
+	&lan_attr_static_table.attr,
+	&lan_attr_vlan_table.attr,
+	&lan_attr_aging.attr,
+	&lan_attr_fast_aging.attr,
+	&lan_attr_link_aging.attr,
+	&lan_attr_bcast_per.attr,
+	&lan_attr_mcast_storm.attr,
+	&lan_attr_diffserv_map.attr,
+	&lan_attr_p_802_1p_map.attr,
+	&lan_attr_vlan.attr,
+	&lan_attr_null_vid.attr,
+	&lan_attr_drop_inv_vid.attr,
+	&lan_attr_macaddr.attr,
+	&lan_attr_mirror_mode.attr,
+	&lan_attr_igmp_snoop.attr,
+	&lan_attr_ipv6_mld_snoop.attr,
+	&lan_attr_ipv6_mld_option.attr,
+	&lan_attr_aggr_backoff.attr,
+	&lan_attr_no_exc_drop.attr,
+	&lan_attr_jumbo_packet.attr,
+	&lan_attr_legal_packet.attr,
+	&lan_attr_length_check.attr,
+	&lan_attr_back_pressure.attr,
+	&lan_attr_sw_flow_ctrl.attr,
+	&lan_attr_sw_half_duplex.attr,
+#ifdef SWITCH_10_MBIT
+	&lan_attr_sw_10_mbit.attr,
+#endif
+	&lan_attr_fair_flow_ctrl.attr,
+	&lan_attr_vlan_bound.attr,
+	&lan_attr_double_tag.attr,
+	&lan_attr_isp.attr,
+	&lan_attr_hsr.attr,
+	&lan_attr_mtu.attr,
+	&lan_attr_unk_ucast_fwd.attr,
+	&lan_attr_unk_ucast_ports.attr,
+	&lan_attr_unk_mcast_fwd.attr,
+	&lan_attr_unk_mcast_ports.attr,
+	&lan_attr_unk_vid_fwd.attr,
+	&lan_attr_unk_vid_ports.attr,
+	&lan_attr_pass_pause.attr,
+	&lan_attr_pme.attr,
+	&lan_attr_pme_polarity.attr,
+
+	&lan_attr_host_port.attr,
+	&lan_attr_ports.attr,
+	&lan_attr_dev_start.attr,
+	&lan_attr_vlan_start.attr,
+	&lan_attr_stp.attr,
+
+	&lan_attr_alu_fid.attr,
+	&lan_attr_alu_use_fid.attr,
+	&lan_attr_alu_override.attr,
+	&lan_attr_alu_valid.attr,
+	&lan_attr_alu_mstp.attr,
+	&lan_attr_alu_prio.attr,
+	&lan_attr_alu_src.attr,
+	&lan_attr_alu_dst.attr,
+	&lan_attr_alu_ports.attr,
+	&lan_attr_alu_addr.attr,
+	&lan_attr_alu_type.attr,
+	&lan_attr_alu_index.attr,
+	&lan_attr_alu_info.attr,
+
+	&lan_attr_vlan_valid.attr,
+	&lan_attr_vlan_ports.attr,
+	&lan_attr_vlan_untag.attr,
+	&lan_attr_vlan_fid.attr,
+	&lan_attr_vlan_mstp.attr,
+	&lan_attr_vlan_prio.attr,
+	&lan_attr_vlan_option.attr,
+	&lan_attr_vlan_index.attr,
+	&lan_attr_vlan_info.attr,
+
+	&lan_attr_no_color.attr,
+	&lan_attr_color_red.attr,
+	&lan_attr_color_yellow.attr,
+	&lan_attr_color_green.attr,
+
+	NULL
+};
+
+static struct attribute *sw_attrs[] = {
+	&sw_attr_mib.attr,
+
+	&sw_attr_vid.attr,
+	&sw_attr_member.attr,
+	&sw_attr_bcast_storm.attr,
+	&sw_attr_mstp.attr,
+	&sw_attr_rx.attr,
+	&sw_attr_tx.attr,
+	&sw_attr_learn.attr,
+	&sw_attr_mirror_port.attr,
+	&sw_attr_mirror_rx.attr,
+	&sw_attr_mirror_tx.attr,
+	&sw_attr_diffserv.attr,
+	&sw_attr_p_802_1p.attr,
+	&sw_attr_prio_vlan.attr,
+	&sw_attr_prio_mac.attr,
+	&sw_attr_prio_acl.attr,
+	&sw_attr_prio_highest.attr,
+	&sw_attr_prio_or.attr,
+	&sw_attr_port_prio.attr,
+	&sw_attr_non_vid.attr,
+	&sw_attr_drop_non_vlan.attr,
+	&sw_attr_drop_tagged.attr,
+	&sw_attr_ingress.attr,
+	&sw_attr_replace_vid.attr,
+	&sw_attr_replace_prio.attr,
+	&sw_attr_src_addr_filter.attr,
+	&sw_attr_vlan_lookup_0.attr,
+	&sw_attr_prio_queue.attr,
+	&sw_attr_prio_rate.attr,
+	&sw_attr_limit.attr,
+	&sw_attr_limit_port_based.attr,
+	&sw_attr_limit_packet_based.attr,
+	&sw_attr_limit_flow_ctrl.attr,
+	&sw_attr_limit_cnt_ifg.attr,
+	&sw_attr_limit_cnt_pre.attr,
+	&sw_attr_rx_p0_rate.attr,
+	&sw_attr_rx_p1_rate.attr,
+	&sw_attr_rx_p2_rate.attr,
+	&sw_attr_rx_p3_rate.attr,
+	&sw_attr_rx_p4_rate.attr,
+	&sw_attr_rx_p5_rate.attr,
+	&sw_attr_rx_p6_rate.attr,
+	&sw_attr_rx_p7_rate.attr,
+	&sw_attr_tx_q0_rate.attr,
+	&sw_attr_tx_q1_rate.attr,
+	&sw_attr_tx_q2_rate.attr,
+	&sw_attr_tx_q3_rate.attr,
+	&sw_attr_color_map.attr,
+	&sw_attr_tc_map.attr,
+	&sw_attr_back_pressure.attr,
+	&sw_attr_force_flow_ctrl.attr,
+	&sw_attr_pass_all.attr,
+	&sw_attr_tail_tag.attr,
+
+	&sw_attr_cust_vid.attr,
+	&sw_attr_sr_1_vid.attr,
+	&sw_attr_sr_2_vid.attr,
+	&sw_attr_sr_1_type.attr,
+	&sw_attr_sr_2_type.attr,
+
+	&sw_attr_pme_ctrl.attr,
+	&sw_attr_pme_status.attr,
+
+	&sw_attr_authen_mode.attr,
+	&sw_attr_acl.attr,
+	&sw_attr_acl_first_rule.attr,
+	&sw_attr_acl_ruleset.attr,
+	&sw_attr_acl_mode.attr,
+	&sw_attr_acl_enable.attr,
+	&sw_attr_acl_src.attr,
+	&sw_attr_acl_equal.attr,
+	&sw_attr_acl_addr.attr,
+	&sw_attr_acl_type.attr,
+	&sw_attr_acl_cnt.attr,
+	&sw_attr_acl_msec.attr,
+	&sw_attr_acl_intr_mode.attr,
+	&sw_attr_acl_ip_addr.attr,
+	&sw_attr_acl_ip_mask.attr,
+	&sw_attr_acl_protocol.attr,
+	&sw_attr_acl_seqnum.attr,
+	&sw_attr_acl_port_mode.attr,
+	&sw_attr_acl_max_port.attr,
+	&sw_attr_acl_min_port.attr,
+	&sw_attr_acl_tcp_flag_enable.attr,
+	&sw_attr_acl_tcp_flag.attr,
+	&sw_attr_acl_tcp_flag_mask.attr,
+	&sw_attr_acl_prio_mode.attr,
+	&sw_attr_acl_prio.attr,
+	&sw_attr_acl_vlan_prio_replace.attr,
+	&sw_attr_acl_vlan_prio.attr,
+	&sw_attr_acl_map_mode.attr,
+	&sw_attr_acl_ports.attr,
+	&sw_attr_acl_index.attr,
+	&sw_attr_acl_act_index.attr,
+	&sw_attr_acl_act.attr,
+	&sw_attr_acl_info.attr,
+	&sw_attr_acl_table.attr,
+
+	&sw_attr_p_index.attr,
+	&sw_attr_q_index.attr,
+	&sw_attr_police_type.attr,
+	&sw_attr_non_dscp_color.attr,
+	&sw_attr_police_drop_all.attr,
+	&sw_attr_police_port_based.attr,
+	&sw_attr_color_mark.attr,
+	&sw_attr_color_remap.attr,
+	&sw_attr_drop_srp.attr,
+	&sw_attr_color_aware.attr,
+	&sw_attr_police.attr,
+
+	&sw_attr_q_cir.attr,
+	&sw_attr_q_pir.attr,
+	&sw_attr_q_cbs.attr,
+	&sw_attr_q_pbs.attr,
+
+	&sw_attr_wred_max.attr,
+	&sw_attr_wred_min.attr,
+	&sw_attr_wred_multiplier.attr,
+	&sw_attr_wred_avg_size.attr,
+	&sw_attr_wred_q_max.attr,
+	&sw_attr_wred_q_min.attr,
+	&sw_attr_wred_q_multiplier.attr,
+	&sw_attr_wred_q_avg_size.attr,
+	&sw_attr_wred_random_drop.attr,
+	&sw_attr_wred_drop_gyr.attr,
+	&sw_attr_wred_drop_yr.attr,
+	&sw_attr_wred_drop_r.attr,
+	&sw_attr_wred_drop_all.attr,
+	&sw_attr_wred_q_pmon.attr,
+
+	&sw_attr_schedule.attr,
+	&sw_attr_shaping.attr,
+#ifdef MTI_PREEMPT_ENABLE
+	&sw_attr_preempt.attr,
+#endif
+	&sw_attr_tx_ratio.attr,
+	&sw_attr_credit_hi.attr,
+	&sw_attr_credit_lo.attr,
+	&sw_attr_credit_incr.attr,
+	&sw_attr_srp.attr,
+
+	&sw_attr_qm_drop.attr,
+	&sw_attr_qm_burst.attr,
+	&sw_attr_qm_resv_space.attr,
+	&sw_attr_qm_hi.attr,
+	&sw_attr_qm_lo.attr,
+	&sw_attr_qm_tx_used.attr,
+	&sw_attr_qm_tx_avail.attr,
+	&sw_attr_qm_tx_calc.attr,
+
+	&sw_attr_mmd_id.attr,
+	&sw_attr_mmd_reg.attr,
+	&sw_attr_mmd_val.attr,
+
+	&sw_attr_rx_flow_ctrl.attr,
+	&sw_attr_tx_flow_ctrl.attr,
+
+	&sw_attr_duplex.attr,
+	&sw_attr_speed.attr,
+	&sw_attr_linkmd.attr,
+
+	NULL
+};
+
+static struct attribute_group lan_group = {
+	.name  = "sw",
+	.attrs  = lan_attrs,
+};
+
+static struct attribute_group sw_group = {
+	.name  = "sw0",
+	.attrs  = sw_attrs,
+};
+
+/* Kernel checking requires the attributes are in data segment. */
+#define SW_ATTRS_SIZE		(sizeof(sw_attrs) / sizeof(void *) - 1)
+
+#define MAX_SWITCHES		2
+
+static struct ksz_dev_attr ksz_sw_dev_attrs[(
+	SW_ATTRS_SIZE * TOTAL_PORT_NUM) * MAX_SWITCHES];
+static struct ksz_dev_attr *ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+static void exit_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int i;
+
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		sysfs_remove_group(&dev->kobj, &sw_group);
+		kfree(info->port_attrs[i]);
+		info->port_attrs[i] = NULL;
+		info->ksz_port_attrs[i] = NULL;
+	}
+
+	sysfs_remove_group(&dev->kobj, &lan_group);
+}
+
+static int init_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int err;
+	int i;
+
+	err = sysfs_create_group(&dev->kobj, &lan_group);
+	if (err)
+		return err;
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		if (i >= sw->phy_port_cnt)
+			err = alloc_dev_attr(sw_attrs,
+				sizeof(sw_attrs) / sizeof(void *), i,
+				&info->ksz_port_attrs[i], &info->port_attrs[i],
+				"0_duplex", &ksz_sw_dev_attrs_ptr);
+		else
+			err = alloc_dev_attr(sw_attrs,
+				sizeof(sw_attrs) / sizeof(void *), i,
+				&info->ksz_port_attrs[i], &info->port_attrs[i],
+				NULL, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &sw_group);
+		if (err)
+			return err;
+	}
+	return err;
+}
+
diff --git a/drivers/net/ethernet/micrel/micrel_ptp.c b/drivers/net/ethernet/micrel/micrel_ptp.c
new file mode 100644
index 0000000..654df7f
--- /dev/null
+++ b/drivers/net/ethernet/micrel/micrel_ptp.c
@@ -0,0 +1,264 @@
+/*
+ * PTP 1588 clock using the eTSEC
+ *
+ * Copyright (C) 2010 OMICRON electronics GmbH
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include <linux/device.h>
+#include <linux/hrtimer.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/timex.h>
+#include <linux/io.h>
+
+#include <linux/ptp_clock_kernel.h>
+
+#define N_ALARM		0
+#define N_EXT_TS	2
+
+struct micrel_ptp_info {
+	struct ptp_clock *clock;
+	struct ptp_clock_info caps;
+	struct ptp_info *ptp;
+
+	u32 clock_events;
+	u64 alarm_interval;
+	u64 alarm_value;
+};
+
+static void ptp_event_pps(struct micrel_ptp_info *info)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_PPS;
+	ptp_clock_event(info->clock, &event);
+}
+
+#if 0
+static void ptp_event_alarm(struct micrel_ptp_info *info)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_ALARM;
+	event.index = 0;
+	event.timestamp = info->alarm_value;
+	ptp_clock_event(info->clock, &event);
+}
+#endif
+
+static void ptp_event_trigger(struct micrel_ptp_info *info, int index,
+	u32 sec, u32 nsec)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_EXTTS;
+	event.index = index;
+	event.timestamp = sec;
+	event.timestamp *= NSEC_PER_SEC;
+	event.timestamp += nsec;
+	ptp_clock_event(info->clock, &event);
+}
+
+/*
+ * PTP clock operations
+ */
+
+static int ptp_adjfreq(struct ptp_clock_info *clock, s32 ppb)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	output = 1;
+	clk_opt.sec = clk_opt.nsec = 0;
+	clk_opt.drift = -ppb;
+	clk_opt.interval = NANOSEC_IN_SEC;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_adjtime(struct ptp_clock_info *clock, s64 delta)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int neg_adj = 0;
+	int err = 0;
+
+	if (delta < 0) {
+		neg_adj = 1;
+		delta = -delta;
+	}
+	clk_opt.sec = div_u64_rem(delta, NSEC_PER_SEC, &clk_opt.nsec);
+	if (!neg_adj)
+		output = 2;
+	else
+		output = 1;
+	clk_opt.interval = 0;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_gettime(struct ptp_clock_info *clock, struct timespec *ts)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	if (!ts)
+		return -info->ptp->drift;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_GET, DEV_PTP_CLK, 0,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	if (err)
+		return err;
+	ts->tv_sec = clk_opt.sec;
+	ts->tv_nsec = clk_opt.nsec;
+	return 0;
+}
+
+static int ptp_settime(struct ptp_clock_info *clock, const struct timespec *ts)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	output = 0;
+	clk_opt.sec = ts->tv_sec;
+	clk_opt.nsec = ts->tv_nsec;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_enable(struct ptp_clock_info *clock,
+	struct ptp_clock_request *rq, int on)
+{
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	struct ptp_info *ptp = info->ptp;
+	u32 bit;
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_EXTTS:
+		if (rq->extts.index >= 2)
+			return -EINVAL;
+		bit = 1 << rq->extts.index;
+		if (on)
+			ptp->clock_events |= bit;
+		else
+			ptp->clock_events &= ~bit;
+		return 0;
+
+	case PTP_CLK_REQ_PPS:
+		bit = 1 << 31;
+		if (on)
+			ptp->clock_events |= bit;
+		else
+			ptp->clock_events &= ~bit;
+		return 0;
+
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info ptp_caps = {
+	.owner		= THIS_MODULE,
+	/* Only 16 characters. */
+	.name		= "Micrel clock",
+	.max_adj	= MAX_DRIFT_CORR,
+	.n_alarm	= N_ALARM,
+	.n_ext_ts	= N_EXT_TS,
+	.n_per_out	= 0,
+	.pps		= 1,
+	.adjfreq	= ptp_adjfreq,
+	.adjtime	= ptp_adjtime,
+	.gettime	= ptp_gettime,
+	.settime	= ptp_settime,
+	.enable		= ptp_enable,
+};
+
+static int micrel_ptp_probe(struct ptp_info *ptp)
+{
+	struct micrel_ptp_info *info;
+#if 0
+	struct timespec now;
+#endif
+	int err = -ENOMEM;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info)
+		goto no_memory;
+
+	err = -ENODEV;
+
+	info->caps = ptp_caps;
+
+#if 0
+	getnstimeofday(&now);
+	ptp_settime(&info->caps, &now);
+#endif
+
+	info->clock = ptp_clock_register(&info->caps);
+	if (IS_ERR(info->clock)) {
+		err = PTR_ERR(info->clock);
+		goto no_clock;
+	}
+	info->ptp = ptp;
+	ptp->clock_info = info;
+
+	return 0;
+
+no_clock:
+	kfree(info);
+no_memory:
+	return err;
+}
+
+static int micrel_ptp_remove(struct ptp_info *ptp)
+{
+	struct micrel_ptp_info *info = ptp->clock_info;
+
+	ptp_clock_unregister(info->clock);
+	kfree(info);
+	ptp->clock_info = NULL;
+
+	return 0;
+}
+
diff --git a/drivers/net/ethernet/micrel/spi-ksz9897.c b/drivers/net/ethernet/micrel/spi-ksz9897.c
new file mode 100644
index 0000000..f1da0da
--- /dev/null
+++ b/drivers/net/ethernet/micrel/spi-ksz9897.c
@@ -0,0 +1,2244 @@
+/**
+ * Micrel KSZ9897 SPI driver
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#if 0
+#define DBG_SPI_ACCESS
+#endif
+#if 1
+#define CHK_SPI_ACCESS
+#endif
+#if 0
+#define NO_ATTACHED_DEV
+#endif
+
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include "ksz_cfg_9897.h"
+
+#if 0
+#define KSZ9897_FPGA
+#endif
+
+/* Used in FPGA. */
+#ifdef KSZ9897_FPGA
+#if 0
+#define NO_ACL
+#endif
+#if 0
+#define NO_SEC_TIMESTAMP
+#endif
+#if 0
+#define USE_OLD_PTP_UNIT_INDEX
+#endif
+#define NO_PHY_READ
+#endif
+
+#if 0
+#define NO_ACL
+#endif
+
+#if 1
+#define NO_EEE
+#endif
+
+#if 0
+#define TEST_IBA
+#endif
+#if 0
+#define NO_DIRECT_ACCESS
+#if 1
+#define SIMULATED_ID			0x00956700
+#endif
+#endif
+
+/* SPI access is unreliable. */
+#if 0
+#if defined(KSZ9897_FPGA) || !defined(KSZ_IBA) || defined(TEST_IBA)
+#define NO_PHY_READ
+#endif
+#endif
+
+#ifndef DEVINIT
+#if defined(CONFIG_SPI_FTDI) && defined(CONFIG_SPI_KSZ9897)
+#define DEVINIT
+#else
+#define DEVINIT __devinit
+#endif
+#endif
+
+#ifdef KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+
+#define KS9897MLI_DEV0			"ksz9897"
+#define KS9897MLI_DEV2			"ksz9897_2"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R24(ks, reg)		spi_rdreg24(ks, reg)
+#define HW_W24(ks, reg, val)	spi_wrreg24(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+
+#include "ksz_sw_phy.h"
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define contain_reg(addr, len, reg)	\
+	(addr <= (reg) && (reg) <= (addr + len - 1))
+
+static void spi_chk_regs(struct ksz_sw *sw, u32 addr, u8 *val, size_t txl)
+{
+	int i;
+	u32 port_reg;
+
+	port_reg = REG_PTP_MSG_CONF1 + 1;
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PTP_ENABLE)
+			sw->overrides |= PTP_TAG;
+		else
+			sw->overrides &= ~PTP_TAG;
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+		if (!(sw->features & NEW_CAP) &&
+		    sw->TAIL_TAG_LOOKUP >= 0x100)
+			sw->overrides |= PTP_TAG;
+#endif
+	}
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PORT_TAIL_TAG_ENABLE)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+	}
+}  /* spi_chk_regs */
+
+/* SPI frame opcodes */
+#define KS_SPIOP_RD			3
+#define KS_SPIOP_WR			2
+
+#define SPI_ADDR_SHIFT			24
+#define SPI_ADDR_MASK			((1 << SPI_ADDR_SHIFT) - 1)
+#define SPI_TURNAROUND_SHIFT		5
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @ks:		The switch device structure.
+ * @addr:	The register address.
+ * @val:	The value to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @addr.
+ */
+static void spi_wrreg(struct sw_priv *ks, u32 addr, void *txb, size_t txl)
+{
+	struct spi_hw_priv *hw_priv = ks->hw_dev;
+	struct spi_transfer *xfer = &hw_priv->spi_xfer1;
+	struct spi_message *msg = &hw_priv->spi_msg1;
+	struct spi_device *spi = hw_priv->spidev;
+	u32 *txc = (u32 *) hw_priv->txd;
+	int ret;
+	struct ksz_sw *sw = &ks->sw;
+
+	if (!(sw->features & NEW_CAP)) {
+		u32 len = (addr & 3) + txl;
+
+		if (len >= 4 && (len & 3))
+			pr_alert("W may not be correct: %x %x\n", addr, txl);
+	}
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("W not locked: %x\n", addr);
+	*txc = addr & SPI_ADDR_MASK;
+	*txc |= KS_SPIOP_WR << SPI_ADDR_SHIFT;
+	*txc <<= SPI_TURNAROUND_SHIFT;
+	*txc = cpu_to_be32(*txc);
+	++txc;
+	memcpy(txc, txb, txl);
+
+	xfer->tx_buf = hw_priv->txd;
+	xfer->rx_buf = NULL;
+	xfer->len = txl + 4;
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed: %x %u\n", addr, txl);
+	spi_chk_regs(sw, addr, txb, txl);
+#ifdef DBG_SPI_ACCESS_
+	do {
+		u8 *byte = hw_priv->txd;
+		for (ret = 0; ret < xfer->len; ret++)
+			printk("%02x ", byte[ret]);
+		printk("\n");
+	} while (0);
+#endif
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 4;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 4);
+}
+
+/**
+ * spi_wrreg24 - write 24bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg24(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 3;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 3);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 2;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 1;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @ks:		The device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *ks)
+{
+	return ks->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @ks:		The device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @op.
+ */
+static void spi_rdreg(struct sw_priv *ks, u32 addr, void *rxb, size_t rxl)
+{
+	struct spi_hw_priv *hw_priv = ks->hw_dev;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	struct spi_device *spi = hw_priv->spidev;
+	u32 *txc = (u32 *) hw_priv->txd;
+	u8 *trx = hw_priv->rxd;
+	int ret;
+
+if ((int) rxl < 0)
+printk("%s %x %x\n", __func__, addr, rxl);
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("R not locked: %x\n", addr);
+	*txc = addr & SPI_ADDR_MASK;
+	*txc |= KS_SPIOP_RD << SPI_ADDR_SHIFT;
+	*txc <<= SPI_TURNAROUND_SHIFT;
+	*txc = cpu_to_be32(*txc);
+
+	if (ksz_rx_1msg(hw_priv)) {
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl + 4;
+		++txc;
+		memset(txc, 0, rxl);
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		memcpy(hw_priv->rxd, hw_priv->txd, 4 + 1);
+		hw_priv->rxd[4] ^= 0xff;
+#endif
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = 4;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed: %x %u\n", addr, rxl);
+	else if (ksz_rx_1msg(hw_priv))
+		memcpy(rxb, trx + 4, rxl);
+	else
+		memcpy(rxb, trx, rxl);
+#ifdef DBG_SPI_ACCESS_
+	do {
+		u8 *byte = hw_priv->txd;
+		for (ret = 0; ret < 4; ret++)
+			printk("%02x ", byte[ret]);
+		printk("\n");
+	} while (0);
+#endif
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *ks, u32 reg)
+{
+	u8 rxb[1];
+
+	spi_rdreg(ks, reg, rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *ks, u32 reg)
+{
+	__le16 rx = 0;
+
+	spi_rdreg(ks, reg, &rx, 2);
+	return be16_to_cpu(rx);
+}
+
+/**
+ * spi_rdreg24 - read 24 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 24bit register from the chip.
+ */
+static u32 spi_rdreg24(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	spi_rdreg(ks, reg, &rx, 3);
+	return be32_to_cpu(rx);
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ */
+static u32 spi_rdreg32(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	spi_rdreg(ks, reg, &rx, 4);
+	return be32_to_cpu(rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * delay_micro - delay in microsecond
+ * @microsec:	Number of microseconds to delay.
+ *
+ * This routine delays in microseconds.
+ */
+static inline void delay_micro(uint microsec)
+{
+	uint millisec = microsec / 1000;
+
+	microsec %= 1000;
+	if (millisec)
+		mdelay(millisec);
+	if (microsec)
+		udelay(microsec);
+}
+
+/**
+ * delay_milli - delay in millisecond
+ * @millisec:	Number of milliseconds to delay.
+ *
+ * This routine delays in milliseconds.
+ */
+static void delay_milli(uint millisec)
+{
+	unsigned long ticks = millisec * HZ / 1000;
+
+	if (!ticks || in_interrupt())
+		mdelay(millisec);
+	else {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(ticks);
+	}
+}
+
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+static inline void copy_old_skb(struct sk_buff *old, struct sk_buff *skb)
+{
+	skb->dev = old->dev;
+	skb->protocol = old->protocol;
+	skb->ip_summed = old->ip_summed;
+	skb->csum = old->csum;
+	skb_set_network_header(skb, ETH_HLEN);
+
+	dev_kfree_skb(old);
+}  /* copy_old_skb */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define KSZSW_REGS_SIZE			0x8000
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x0000, 0x7FFF },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->reglock);
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed\n", __func__);
+		mutex_unlock(sw->reglock);
+		sw->reg->lock(sw);
+	}
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+#ifdef KSZ_IBA
+if (1 == sw->info->iba.use_iba)
+printk(KERN_ALERT "%s not using IBA\n", __func__);
+#ifdef TEST_IBA
+if (!sw->info->iba.use_iba)
+printk(KERN_ALERT "%s using SPI %x\n", __func__, reg);
+if (!sw->info->iba.use_iba)
+return 0;
+#endif
+#endif
+if (0x420 <= reg && reg <= 0x42C && !mutex_is_locked(&sw->alulock))
+printk(KERN_ALERT "%s uses %x\n", __func__, reg);
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+#ifdef KSZ_IBA
+if (1 == sw->info->iba.use_iba)
+printk(KERN_ALERT "%s not using IBA %x\n", __func__, reg);
+#ifdef TEST_IBA
+if (!sw->info->iba.use_iba)
+printk(KERN_ALERT "%s using SPI %x\n", __func__, reg);
+if (!sw->info->iba.use_iba)
+return 0;
+#endif
+#endif
+if (0x420 <= reg && reg <= 0x42C && !mutex_is_locked(&sw->alulock))
+printk(KERN_ALERT "%s uses %x\n", __func__, reg);
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r24(struct ksz_sw *sw, unsigned reg)
+{
+#ifdef KSZ_IBA
+if (1 == sw->info->iba.use_iba)
+printk(KERN_ALERT "%s not using IBA %x\n", __func__, reg);
+#ifdef TEST_IBA
+if (!sw->info->iba.use_iba)
+printk(KERN_ALERT "%s using SPI %x\n", __func__, reg);
+if (!sw->info->iba.use_iba)
+return 0;
+#endif
+#endif
+if (0x420 <= reg && reg <= 0x42C && !mutex_is_locked(&sw->alulock))
+printk(KERN_ALERT "%s uses %x\n", __func__, reg);
+	return HW_R24(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+#ifdef KSZ_IBA
+if (1 == sw->info->iba.use_iba)
+printk(KERN_ALERT "%s not using IBA %x\n", __func__, reg);
+#ifdef TEST_IBA
+if (!sw->info->iba.use_iba)
+printk(KERN_ALERT "%s using SPI %x\n", __func__, reg);
+if (!sw->info->iba.use_iba)
+return 0;
+#endif
+#endif
+if (0x420 <= reg && reg <= 0x42C && !mutex_is_locked(&sw->alulock))
+printk(KERN_ALERT "%s uses %x\n", __func__, reg);
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	spi_rdreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	spi_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba)
+return;
+#endif
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba)
+return;
+#endif
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba)
+return;
+#endif
+	HW_W24(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+#if defined(NO_DIRECT_ACCESS)
+if (!sw->info->iba.use_iba)
+return;
+#endif
+	HW_W32(sw->dev, reg, val);
+}
+
+static void sw_port_r16(struct ksz_sw *sw, int port, int offset, u16 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw_r16(sw, addr);
+}  /* sw_port_r16 */
+
+static void sw_port_w16(struct ksz_sw *sw, int port, int offset, u16 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw_w16(sw, addr, data);
+}  /* sw_port_w16 */
+
+#ifdef CONFIG_KSZ_STP
+static u8 get_port_state(struct net_device *dev, struct net_device **br_dev)
+{
+	struct net_bridge_port *p;
+	u8 state;
+
+	/* This state is not defined in kernel. */
+	state = STP_STATE_SIMPLE;
+	if (br_port_exists(dev)) {
+		p = br_port_get_rcu(dev);
+		state = p->state;
+
+		/* Port is under bridge. */
+		*br_dev = p->br->dev;
+	}
+	return state;
+}  /* get_port_state */
+#endif
+
+static void link_update_work(struct work_struct *work)
+{
+	struct ksz_port *port =
+		container_of(work, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct phy_device *phydev;
+	int i;
+	int link;
+
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ops->link_change(dlr,
+			sw->port_info[dlr->ports[0]].state == media_connected,
+			sw->port_info[dlr->ports[1]].state == media_connected);
+	}
+#endif
+
+	/* This only matters when one phy device is used for the switch. */
+	if (1 == sw->dev_count) {
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		for (i = 0; i < sw->mib_port_cnt; i++) {
+			if (i == sw->HOST_PORT)
+				continue;
+			if (port->linked == &sw->port_info[i]) {
+				hw_priv->phy_id = i + 1;
+				break;
+			}
+		}
+	}
+	for (i = 0; i < sw->phy_port_cnt; i++) {
+		struct ksz_port_info *info = &sw->port_info[i];
+
+		if (!info->report)
+			continue;
+		info->report = false;
+		phydev = sw->phy[i + 1];
+		if (i == sw->HOST_PORT)
+			phydev = sw->phydev;
+		phydev->link = (info->state == media_connected);
+		phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		phydev->duplex = (info->duplex == 2);
+		if (phydev->attached_dev) {
+			link = netif_carrier_ok(phydev->attached_dev);
+			if (link != phydev->link) {
+				if (phydev->link)
+					netif_carrier_on(phydev->attached_dev);
+				else
+					netif_carrier_off(phydev->attached_dev);
+				if (netif_msg_link(sw))
+					pr_info("%s link %s\n",
+						phydev->attached_dev->name,
+						phydev->link ? "on" : "off");
+			}
+			if (phydev->adjust_link)
+				phydev->adjust_link(phydev->attached_dev);
+		}
+	}
+
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->HOST_PORT >= sw->phy_port_cnt) {
+		struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+		int phy_link;
+
+		phydev = sw->phydev;
+		phydev->link = (info->state == media_connected);
+		phy_link = phydev->link;
+		if (!sw->need_link_up &&
+		    (1 == sw->dev_count || 1 == sw->dev_offset))
+			phy_link = (port->linked->state == media_connected);
+		phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		phydev->duplex = (info->duplex == 2);
+		if (phydev->attached_dev) {
+			link = netif_carrier_ok(phydev->attached_dev);
+			if (link != phy_link) {
+				if (phy_link)
+					netif_carrier_on(phydev->attached_dev);
+				else
+					netif_carrier_off(phydev->attached_dev);
+				if (netif_msg_link(sw))
+					pr_info("%s link %s\n",
+						phydev->attached_dev->name,
+						phy_link ? "on" : "off");
+			}
+			if (phydev->adjust_link)
+				phydev->adjust_link(phydev->attached_dev);
+		}
+	}
+}  /* link_update_work */
+
+static void sw_dis_intr(struct ksz_sw *sw);
+static void sw_ena_intr(struct ksz_sw *sw);
+
+#define USE_DIFF_PORT_PRIORITY
+#include "ksz_sw_9897.c"
+
+static void sw_dis_intr(struct ksz_sw *sw)
+{
+	HW_W32(sw->dev, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	HW_W32(sw->dev, REG_SW_PORT_INT_MASK__4, sw->PORT_MASK);
+}  /* sw_dis_intr */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	int i;
+
+	HW_W32(sw->dev, REG_SW_INT_MASK__4,
+		~sw->intr_mask & SWITCH_INT_MASK);
+	HW_W32(sw->dev, REG_SW_PORT_INT_MASK__4,
+		~sw->port_intr_mask & sw->PORT_MASK);
+	for (i = 0; i < sw->mib_port_cnt; i++)
+		port_w(sw, i, REG_PORT_INT_MASK,
+			~sw->info->port_cfg[i].intr_mask & PORT_INT_MASK);
+}  /* sw_ena_intr */
+
+/* -------------------------------------------------------------------------- */
+
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *ks = seq->private;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&ks->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = HW_R(ks, i + j * SW_SIZE);
+		mutex_unlock(&ks->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, "\n");
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @ks:		The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void DEVINIT create_debugfs(struct sw_priv *ks)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "spi_%s",
+		 dev_name(ks->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root\n");
+		return;
+	}
+
+	ks->debug_root = root;
+	ks->debug_file = debugfs_create_file("state", 0444, root,
+		ks, &state_fops);
+	if (IS_ERR(ks->debug_file))
+		pr_err("cannot create debugfs state file\n");
+}
+
+static void __devexit delete_debugfs(struct sw_priv *ks)
+{
+	debugfs_remove(ks->debug_file);
+	debugfs_remove(ks->debug_root);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs_9897.c"
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_sysfs.c"
+#endif
+#ifdef KSZ_DLR
+#include "ksz_dlr_sysfs.c"
+#endif
+
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct phy_device *phydev = phy_dat;
+	struct sw_priv *ks = phydev->bus->priv;
+
+	if (IRQF_TRIGGER_LOW == ks->intr_mode)
+		disable_irq_nosync(irq);
+	atomic_inc(&phydev->irq_disable);
+	ks->sw.intr_using = 1;
+	schedule_work(&phydev->phy_queue);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct phy_device *phydev =
+		container_of(work, struct phy_device, phy_queue);
+	struct sw_priv *ks = phydev->bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+
+	ks->intr_working |= 1;
+	if (ks->sw.info->port_cfg[0].intr_mask & PORT_PHY_INT)
+		ks->intr_working |= 2;
+	mutex_lock(&sw->lock);
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	sw_proc_intr(&ks->sw);
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	mutex_unlock(&ks->hwlock);
+	mutex_unlock(&sw->lock);
+	sw->intr_using = 0;
+
+	atomic_dec(&phydev->irq_disable);
+	if (IRQF_TRIGGER_LOW == ks->intr_mode)
+		enable_irq(ks->irq);
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	struct phy_device *phydev = ks->phydev;
+	int err = 0;
+
+	INIT_WORK(&phydev->phy_queue, sw_change);
+
+	atomic_set(&phydev->irq_disable, 0);
+	if (request_irq(ks->irq, sw_interrupt, ks->intr_mode, name,
+			phydev) < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)\n",
+			phydev->bus->name,
+			ks->irq);
+		phydev->irq = PHY_POLL;
+		return 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	struct phy_device *phydev = ks->phydev;
+
+	free_irq(ks->irq, phydev);
+	cancel_work_sync(&phydev->phy_queue);
+	while (atomic_dec_return(&phydev->irq_disable) >= 0)
+		enable_irq(ks->irq);
+}  /* sw_stop_interrupt */
+
+#define KSZ9897_SW_ID		0x9897
+#define PHY_ID_KSZ_SW		((KSZ9897_ID_HI << 16) | KSZ9897_SW_ID)
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver = {
+	.phy_id		= PHY_ID_KSZ_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Micrel KSZ9897 Switch",
+	.features	= (PHY_GBIT_FEATURES |
+				SUPPORTED_Pause | SUPPORTED_Asym_Pause),
+	.flags		= PHY_HAS_MAGICANEG | PHY_HAS_INTERRUPT,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.driver		= { .owner = THIS_MODULE, },
+};
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy_id, u16 phy, u16 reg, u16 *val)
+{
+	u16 data;
+	u16 ret;
+	int id = phy;
+
+	if (0 == phy || phy > sw->phy_port_cnt)
+		phy = phy_id;
+	sw_port_r16(sw, phy - 1, P_PHY_CTRL + reg * 2, &data);
+	ret = data;
+
+	/* Use unique switch id to differentiate from regular PHY. */
+	if (3 == reg)
+		ret = KSZ9897_SW_ID;
+	if (0 == id || id > sw->phy_port_cnt) {
+		switch (reg) {
+		case 0:
+			ret = 0x1140;
+			break;
+		case 1:
+			ret = 0x796d;
+			break;
+		case 4:
+			ret = 0x05e1;
+			break;
+		case 5:
+			ret = 0xc5e1;
+			break;
+		case 9:
+			ret = 0x0700;
+			break;
+		case 10:
+			if (0 == id)
+				id = sw->HOST_PORT;
+			if (sw->port_info[id].tx_rate >= 1000 * TX_RATE_UNIT)
+				ret = 0x7800;
+			else
+				ret = 0;
+			break;
+		}
+	}
+	if (1 == reg && !(ret & PORT_LINK_STATUS)) {
+		sw_port_r16(sw, phy - 1, P_SPEED_STATUS, &data);
+		if ((ret & PORT_AUTO_NEG_ACKNOWLEDGE) &&
+		    (data & (PORT_STAT_SPEED_1000MBIT |
+		    PORT_STAT_SPEED_100MBIT |
+		    PORT_STAT_SPEED_10MBIT)))
+			ret |= PORT_LINK_STATUS;
+	}
+	*val = ret;
+}  /* sw_r_phy */
+
+static int ksz_mii_addr(int *reg, int *bank)
+{
+	int ret;
+
+	ret = (*reg & 0xC000) >> ADDR_SHIFT;
+	*bank = (*reg & 0x3000) >> BANK_SHIFT;
+	*reg &= 0x0FFF;
+	return ret;
+}
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int addr;
+	int bank;
+	int ret = 0xffff;
+
+	if (phy_id > sw->mib_port_cnt)
+		return 0xffff;
+
+	addr = ksz_mii_addr(&regnum, &bank);
+
+	mutex_lock(&ks->lock);
+
+#ifdef KSZ_IBA
+	/* Indicate okay to use SPI when IBA is enabled. */
+	sw->info->iba.use_iba |= 0x40;
+#endif
+	switch (addr) {
+	case ADDR_8:
+		ret = HW_R8(ks, regnum);
+		break;
+	case ADDR_16:
+		ret = HW_R16(ks, regnum);
+		break;
+	case ADDR_32:
+		ret = HW_R32(ks, regnum);
+		break;
+	default:
+		if (regnum < 11) {
+			u16 data;
+
+			sw_r_phy(sw, ks->phy_id, phy_id, regnum, &data);
+			ret = data;
+		} else
+			ret = 0;
+	}
+#ifdef KSZ_IBA
+	sw->info->iba.use_iba &= ~0x40;
+#endif
+	mutex_unlock(&ks->lock);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	static int last_reg;
+	static int last_val;
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int addr;
+	int bank;
+	int reg;
+
+	if (phy_id > sw->mib_port_cnt)
+		return -EINVAL;
+
+	reg = regnum;
+	addr = ksz_mii_addr(&regnum, &bank);
+
+	mutex_lock(&ks->lock);
+
+#ifdef KSZ_IBA
+	/* Indicate okay to use SPI when IBA is enabled. */
+	sw->info->iba.use_iba |= 0x40;
+#endif
+	switch (addr) {
+	case ADDR_8:
+		HW_W8(ks, regnum, val);
+		break;
+	case ADDR_16:
+		HW_W16(ks, regnum, val);
+		break;
+	case ADDR_32:
+		/*
+		 * The phy_write interface allows only 16-bit value.  Break
+		 * the 32-bit write into two calls for SPI efficiency.
+		 */
+
+		/* Previous write to high word. */
+		if (last_reg == reg + 2) {
+			last_val <<= 16;
+			last_val |= val;
+			HW_W32(ks, regnum, last_val);
+			last_reg = 0;
+		} else {
+			/* Somebody has written to different address! */
+			if (last_reg) {
+				int last_bank;
+
+				addr = ksz_mii_addr(&last_reg, &last_bank);
+				HW_W16(ks, last_reg, last_val);
+				last_reg = 0;
+			}
+
+			/* Cache the 16-bit write to high word. */
+			if (reg & 3) {
+				last_reg = reg;
+				last_val = val;
+
+			/* Did not find the previous write to high word.*/
+			} else
+				HW_W16(ks, regnum, val);
+		}
+		break;
+	default:
+		if (regnum < 11) {
+			int i;
+
+			/* PHY device driver resets or powers down the PHY. */
+			if (0 == regnum &&
+			    (val & (PORT_PHY_RESET | PORT_POWER_DOWN)))
+{
+dbg_msg("%s %x %x %x\n", __func__, phy_id, regnum, val);
+				break;
+}
+			for (i = 0; i < sw->phy_port_cnt; i++) {
+				if (i + 1 == phy_id || 0 == phy_id)
+					sw_port_w16(sw, i,
+						P_PHY_CTRL + regnum * 2, val);
+			}
+		}
+		break;
+	}
+#ifdef KSZ_IBA
+	sw->info->iba.use_iba &= ~0x40;
+#endif
+	mutex_unlock(&ks->lock);
+	return 0;
+}  /* ksz_mii_write */
+
+static int DEVINIT ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	int err;
+	int i;
+	int driver_installed = false;
+
+	pdev = platform_device_register_simple("Switch MII bus", ks->sw.id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	err = phy_driver_register(&kszsw_phy_driver);
+	if (err)
+		goto mii_init_free_mii_bus;
+	driver_installed = true;
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.mib_port_cnt + 1)) - 1);
+	bus->priv = ks;
+	bus->irq = ks->bus_irqs;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = ks->irq;
+
+	ks->phy_id = 1;
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	if (!bus->phy_map[0]) {
+		printk(KERN_WARNING "No PHY detected\n");
+		mdiobus_unregister(bus);
+		err = -ENODEV;
+		goto mii_init_free_mii_bus;
+	}
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i]) {
+			struct phy_priv *phydata;
+
+			phydata = kzalloc(sizeof(struct phy_priv), GFP_KERNEL);
+			if (!phydata) {
+				err = -ENOMEM;
+				goto mii_init_free_mii_bus;
+			}
+			phydata->port.sw = &ks->sw;
+			INIT_WORK(&phydata->port.link_update, link_update_work);
+			bus->phy_map[i]->priv = phydata;
+		}
+
+	ks->bus = bus;
+	ks->pdev = pdev;
+	ks->phydev = bus->phy_map[0];
+	ks->phydev->interface = ks->sw.interface;
+
+	/* The switch is always linked; speed and duplex are also fixed. */
+	do {
+		struct ksz_port_info *info =
+			&ks->sw.port_info[ks->sw.HOST_PORT];
+
+		ks->phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		ks->phydev->duplex = (info->duplex == 2);
+		ks->phydev->pause = 1;
+	} while (0);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i])
+			kfree(bus->phy_map[i]->priv);
+	if (driver_installed)
+		phy_driver_unregister(&kszsw_phy_driver);
+	mdiobus_free(bus);
+
+mii_init_reg:
+	platform_device_unregister(pdev);
+
+	return err;
+}  /* ksz_mii_init */
+
+static void __devexit ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct platform_device *pdev = ks->pdev;
+	struct mii_bus *bus = ks->bus;
+
+	if (ks->irq > 0) {
+		mutex_lock(&ks->lock);
+#if 1
+		HW_W32(ks, REG_SW_INT_MASK__4, 0xffffffff);
+		HW_W32(ks, REG_SW_PORT_INT_MASK__4, 0xffffffff);
+#endif
+		mutex_unlock(&ks->lock);
+		sw_stop_interrupt(ks);
+	}
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i])
+			kfree(bus->phy_map[i]->priv);
+	mdiobus_unregister(bus);
+	phy_driver_unregister(&kszsw_phy_driver);
+	mdiobus_free(bus);
+	platform_device_unregister(pdev);
+}  /* ksz_mii_exit */
+
+/* driver bus management functions */
+
+static void ksz9897_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	int i;
+	int cnt = 0;
+
+	/* Find out how many ports are connected. */
+	for (i = 0; i < sw->mib_port_cnt; i++)
+		if (media_connected == sw->port_state[i].state)
+			++cnt;
+	if (!cnt)
+		cnt++;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += MIB_READ_INTERVAL * cnt;
+	}
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		mib = &sw->port_mib[i];
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[i].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, i))
+				return;
+			hw_priv->counter[i].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				hw_priv->counter[i].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[i].counter);
+			}
+		} else if (time_after_eq(jiffies, hw_priv->counter[i].time)) {
+			hw_priv->counter[i].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[i].state) {
+				hw_priv->counter[i].read = 1;
+				sw->next_jiffies += MIB_READ_INTERVAL;
+			}
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[i].link_down) {
+			sw->port_state[i].link_down = 0;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[i].read = 1;
+		}
+	}
+}  /* ksz9897_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct phy_device *phydev;
+	struct ksz_port *port = NULL;
+	int i;
+	int changes = 0;
+	int s = 1;
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+	sw->ops->acquire(sw);
+	for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+		struct phy_priv *phydata;
+		struct net_device *dev = sw->netdev[i];
+
+		phydev = sw->phy[i + s];
+		phydata = phydev->priv;
+		if (dev && sw->net_ops->get_priv_port)
+			port = sw->net_ops->get_priv_port(dev);
+		else
+			port = &phydata->port;
+		changes |= port_get_link_speed(port);
+
+		/* Copy all port information for user access. */
+		if (port != &phydata->port) {
+			copy_port_status(port, &phydata->port);
+			if (phydata != hw_priv->phydev->priv) {
+				phydata = hw_priv->phydev->priv;
+				copy_port_status(port, &phydata->port);
+			}
+		}
+	}
+	sw->ops->release(sw);
+
+	/* Not to read PHY registers unnecessarily if no link change. */
+	if (!changes)
+		return;
+}  /* link_read_work */
+
+static void stp_work(struct work_struct *work)
+{
+#ifdef CONFIG_KSZ_STP
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, stp_monitor);
+	struct ksz_sw *sw = &hw_priv->sw;
+
+	sw->net_ops->monitor_ports(sw);
+#endif
+}  /* stp_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz9897_mib_monitor(unsigned long ptr)
+{
+	struct sw_priv *hw_priv = (struct sw_priv *) ptr;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz9897_mib_monitor */
+
+static void ksz9897_dev_monitor(unsigned long ptr)
+{
+	struct sw_priv *hw_priv = (struct sw_priv *) ptr;
+
+#if 1
+	if ((hw_priv->intr_working & 2) &&
+	    !(hw_priv->sw.features & STP_SUPPORT))
+		return;
+	if (!(hw_priv->intr_working & 2))
+#endif
+		schedule_delayed_work(&hw_priv->link_read, 0);
+	if (hw_priv->sw.features & STP_SUPPORT)
+		schedule_delayed_work(&hw_priv->stp_monitor, 0);
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz9897_dev_monitor */
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+#include "ksz_dsa.c"
+#endif
+
+static int intr_mode;
+static int need_link_up;
+static int rx_1msg = 1;
+static int spi_bus;
+static int sw_host_port;
+
+#define MAX_SPI_DEVICES		2
+
+static int sw_device_present;
+
+static int DEVINIT ksz9897_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *ks;
+	struct ksz_sw *sw;
+	struct ksz_port *port;
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	u32 id;
+	u32 id1;
+	u32 id2;
+	int cnt;
+	int i;
+	int mib_port_count;
+	int phy_port_count;
+	int pi;
+	int port_count;
+	int ret;
+
+	spi->bits_per_word = 8;
+
+	ks = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!ks)
+		return -ENOMEM;
+
+	ks->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!ks->hw_dev) {
+		kfree(ks);
+		return -ENOMEM;
+	}
+	hw_priv = ks->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	ks->intr_mode = intr_mode ? IRQF_TRIGGER_FALLING :
+		IRQF_TRIGGER_LOW;
+	ks->irq = spi->irq;
+	ks->dev = &spi->dev;
+
+	dev_set_drvdata(ks->dev, ks);
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	/* simple check for a valid chip being connected to the bus */
+	mutex_lock(&ks->lock);
+#if !defined(NO_DIRECT_ACCESS)
+	id = HW_R32(ks, REG_CHIP_ID0__1);
+#else
+	id = SIMULATED_ID;
+#endif
+	mutex_unlock(&ks->lock);
+	id1 = id;
+	id1 >>= 8;
+	id1 &= 0xffff;
+	id2 = id1 & 0xff;
+	id1 >>= 8;
+dbg_msg("%02x %02x\n", id1, id2);
+#ifndef DBG_SPI_ACCESS
+	if (id1 != FAMILY_ID_95 && id1 != FAMILY_ID_98 &&
+	    id1 != FAMILY_ID_94 && id1 != FAMILY_ID_85 && id1 != 0x64) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)\n", id);
+		ret = -ENODEV;
+		goto err_sw;
+	}
+	dev_info(ks->dev, "chip id 0x%08x\n", id);
+#endif
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	mutex_init(&sw->acllock);
+	mutex_init(&sw->alulock);
+	mutex_init(&sw->vlanlock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+
+	port_count = 1;
+	mib_port_count = 1;
+	phy_port_count = 1;
+#ifdef CONFIG_1588_PTP
+	if ((FAMILY_ID_95 & 0x0f) == (id1 & 0x0f)) {
+		sw->features |= PTP_HW;
+		sw->features |= ACL_CORRUPT_BUG;
+	}
+	if (0x64 == id1) {
+		sw->features |= PTP_HW;
+#if 1
+		sw->features |= SETUP_PHY;
+#endif
+	}
+#endif
+	if ((FAMILY_ID_85 & 0xf0) == (id1 & 0xf0))
+		sw->features |= QW_HW;
+	sw->features |= NO_GLOBAL_RESET;
+
+	/* Check for S2 revision. */
+	mutex_lock(&ks->lock);
+	id = HW_R(ks, REG_GLOBAL_OPTIONS);
+	mutex_unlock(&ks->lock);
+	if (id) {
+		sw->features &= ~ACL_CORRUPT_BUG;
+		sw->features &= ~SETUP_PHY;
+		sw->features &= ~NO_GLOBAL_RESET;
+		sw->features |= NEW_CAP;
+		if (id & SW_AVB_ABLE) {
+			sw->features |= AVB_SUPPORT;
+			sw->features |= PTP_HW;
+		}
+		if (id & SW_REDUNDANCY_ABLE) {
+			sw->features |= REDUNDANCY_SUPPORT;
+#ifdef KSZ_DLR
+			sw->features |= DLR_HW;
+#endif
+			sw->features |= HSR_HW;
+		}
+dbg_msg("avb=%d  rr=%d  giga=%d\n",
+!!(id & SW_AVB_ABLE), !!(id & SW_REDUNDANCY_ABLE), !!(id & SW_GIGABIT_ABLE));
+	}
+	if ((CHIP_ID_67 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 7;
+		mib_port_count = 7;
+		phy_port_count = 5;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+	} else if ((CHIP_ID_66 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 6;
+		mib_port_count = 6;
+		phy_port_count = 5;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+	} else if ((CHIP_ID_63 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 3;
+		mib_port_count = 3;
+		phy_port_count = 2;
+		sw->TAIL_TAG_LOOKUP = (1 << (3 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (3 + 2));
+	}
+dbg_msg("port: %x %x %x\n", port_count, mib_port_count, phy_port_count);
+
+	sw->dev_count = 1;
+
+	sw->PORT_MASK = (1 << (mib_port_count + 0)) - 1;
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = mib_port_count;
+	sw->phy_port_cnt = phy_port_count;
+	sw->port_cnt = mib_port_count;
+	if (sw_host_port && sw_host_port <= port_count)
+		sw->HOST_PORT = sw_host_port - 1;
+	else
+		sw->HOST_PORT = port_count - 1;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+dbg_msg("mask: %x %x; %x %x\n", sw->HOST_MASK, sw->PORT_MASK,
+sw->TAIL_TAG_LOOKUP, sw->TAIL_TAG_OVERRIDE);
+
+	/* Last port is the host port. */
+	if (sw->HOST_PORT == port_count - 1 || !sw->HOST_PORT) {
+		port_count = port_count - 1;
+		mib_port_count = mib_port_count - 1;
+	}
+
+	sw->dev = ks;
+	sw->id = sw_device_present;
+
+	sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+	if (!sw->info) {
+		ret = -ENOMEM;
+		goto err_sw;
+	}
+
+	sw->reg = &sw_reg_ops;
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+
+	init_waitqueue_head(&sw->queue);
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+	INIT_DELAYED_WORK(&ks->stp_monitor, stp_work);
+
+	for (cnt = 0, pi = 0; cnt < phy_port_count; cnt++, pi++) {
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		sw->port_info[pi].link = 0xFF;
+		sw->port_info[pi].state = media_disconnected;
+		sw->port_info[pi].report = true;
+	}
+	sw->interface = PHY_INTERFACE_MODE_MII;
+#ifndef KSZ9897_FPGA
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+	mutex_lock(&ks->lock);
+	for (; cnt < sw->mib_port_cnt; cnt++, pi++) {
+		u16 data;
+		u16 orig;
+		u8 *data_lo;
+		u8 *data_hi;
+		int speed;
+		phy_interface_t phy;
+		struct ksz_port_info *info = &sw->port_info[pi];
+		int gbit;
+		int mode;
+
+		port_r16(sw, pi, REG_PORT_XMII_CTRL_0, &data);
+		orig = data;
+		data_hi = (u8 *) &data;
+		data_lo = data_hi + 1;
+
+/**
+ * THa  2015/08/27
+ * Port 6 or 7 may never start transmiting and cause flow control problem in
+ * the receive port.
+ * Not guaranteed to work all the time.
+ */
+		if (sw->features & NEW_CAP) {
+			*data_hi ^= (PORT_MII_NOT_1GBIT | PORT_MII_MAC_MODE |
+				PORT_MII_SEL_M);
+			port_w8(sw, pi, REG_PORT_XMII_CTRL_1, *data_hi);
+			data = orig;
+			port_w8(sw, pi, REG_PORT_XMII_CTRL_1, *data_hi);
+		}
+#ifdef USE_10_MBIT_MODE
+		*data_lo &= ~PORT_MII_100MBIT;
+#endif
+#ifdef USE_HALF_DUPLEX
+		*data_lo &= ~PORT_MII_FULL_DUPLEX;
+#endif
+#ifdef USE_RGMII_MODE
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+#endif
+#ifdef USE_GMII_100_MODE
+		sw_set_gbit(sw, false, data_hi);
+#endif
+#ifdef USE_MII_MODE
+		sw_set_xmii(sw, 0, data_hi);
+#endif
+#ifdef USE_GMII_MODE
+		sw_set_xmii(sw, 2, data_hi);
+#endif
+#ifdef USE_RMII_MODE
+		sw_set_xmii(sw, 1, data_hi);
+#endif
+/* Strap options may not valid after reset. */
+#if 1
+if (PORT_RMII_SEL == (*data_hi & PORT_MII_SEL_M)) {
+dbg_msg("?%02x\n", *data_hi);
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+}
+#endif
+		gbit = sw_get_gbit(sw, *data_hi);
+		mode = sw_get_xmii(sw, *data_hi);
+		switch (mode) {
+		case 2:
+			phy = PHY_INTERFACE_MODE_GMII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 1000;
+			if (gbit)
+				break;
+		case 0:
+			phy = PHY_INTERFACE_MODE_MII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			break;
+		case 1:
+			phy = PHY_INTERFACE_MODE_RMII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			break;
+		default:
+			phy = PHY_INTERFACE_MODE_RGMII;
+			if (*data_hi & PORT_RGMII_ID_IG_ENABLE)
+				phy = PHY_INTERFACE_MODE_RGMII_RXID;
+			if (*data_hi & PORT_RGMII_ID_EG_ENABLE) {
+				if (PHY_INTERFACE_MODE_RGMII_RXID == phy)
+					phy = PHY_INTERFACE_MODE_RGMII_ID;
+				else
+					phy = PHY_INTERFACE_MODE_RGMII_TXID;
+			}
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			if (gbit)
+				speed = 1000;
+			break;
+		}
+		if (sw->HOST_PORT == pi)
+dbg_msg("host: %d %d\n", sw->HOST_PORT, sw->interface);
+		info->state = media_connected;
+		if (!(*data_lo & PORT_MII_100MBIT))
+			info->tx_rate = 10 * TX_RATE_UNIT;
+		else
+			info->tx_rate = speed * TX_RATE_UNIT;
+		if (*data_lo & PORT_MII_FULL_DUPLEX)
+			info->duplex = 2;
+		else
+			info->duplex = 1;
+		info->flow_ctrl = 0x33;
+dbg_msg("xmii: %04x %02x %02x; %u %u\n", orig, *data_lo, *data_hi,
+info->tx_rate / TX_RATE_UNIT, info->duplex);
+	}
+	mutex_unlock(&ks->lock);
+#endif
+#endif
+
+#ifndef DBG_SPI_ACCESS
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+#endif
+
+/* Link auto aging does not work on FPGA board. */
+#ifdef KSZ9897_FPGA
+fast_aging = 1;
+#endif
+#ifdef KSZ_IBA
+	sw->need_link_up = need_link_up;
+#endif
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+
+	sw->phydev = ks->phydev;
+	sw->counter = ks->counter;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+	sw->stp_monitor = &ks->stp_monitor;
+
+	sw_setup_mib(sw);
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+	sw_init_mib(sw);
+#endif
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+	create_debugfs(ks);
+
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_init(&sw->info->dlr, sw);
+#endif
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+	sw->ops->acquire(sw);
+
+	/* Turn off PTP in case the feature is not enabled. */
+	sw_w16(sw, REG_PTP_MSG_CONF1, 0);
+
+#if 0
+	/* Turn off IBA first for KSZ9893. */
+	if (sw->features & SETUP_PHY)
+		sw_w8(sw, REG_SW_IBA__4, 0);
+#endif
+	sw_reset(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw->ops->release(sw);
+#endif
+
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		init_dlr_sysfs(ks->dev);
+#endif
+#endif
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+	sema_init(&ks->proc_sem, 1);
+
+#ifndef DBG_SPI_ACCESS
+	for (i = 0; i <= sw->mib_port_cnt; i++) {
+		sw->phy[i] = ks->bus->phy_map[i];
+		phydev = sw->phy[i];
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		port = &priv->port;
+		port->port_cnt = port_count;
+		port->mib_port_cnt = mib_port_count;
+		port->first_port = 0;
+		port->flow_ctrl = PHY_TX_ONLY;
+
+		port->linked = &sw->port_info[port->first_port];
+	}
+#ifdef NO_ATTACHED_DEV
+	sw->ops->acquire(sw);
+	phydev = sw->phydev;
+	priv = phydev->priv;
+	port = &priv->port;
+	port_set_link_speed(port);
+	sw->ops->release(sw);
+#endif
+#endif
+
+	INIT_WORK(&sw->set_addr, sw_delayed_set_addr);
+
+#ifdef KSZ_IBA
+#ifndef DBG_SPI_ACCESS
+	ksz_iba_init(&sw->info->iba, sw);
+#endif
+	INIT_DELAYED_WORK(&sw->set_ops, sw_set_ops);
+#else
+#ifdef TEST_IBA
+do {
+	u32 data;
+
+	sw->ops->acquire(sw);
+	data = sw_r32(sw, REG_SW_IBA__4);
+	data &= ~(SW_IBA_PORT_MASK << SW_IBA_PORT_SHIFT);
+	data |= 2 << SW_IBA_PORT_SHIFT;
+	data |= SW_IBA_ENABLE;
+	data |= SW_IBA_INIT;
+#if 0
+	data |= SW_IBA_DA_MATCH;
+#endif
+	sw_w32(sw, REG_SW_IBA__4, data);
+	sw->ops->release(sw);
+} while (0);
+#endif
+#endif
+
+	INIT_WORK(&ks->mib_read, ksz9897_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz9897_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz9897_dev_monitor, ks);
+
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!sw->multi_dev && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period);
+#endif
+
+	sw_device_present++;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ports = sw->mib_port_cnt - 0;
+		ptp->reg = &ptp_reg_ops;
+		ptp->ops = &ptp_ops;
+		ptp->parent = ks->dev;
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+#ifdef NO_ATTACHED_DEV
+		ptp->ops->init(ptp, sw->info->mac_addr);
+		ptp->reg->start(ptp, true);
+#endif
+#endif
+		init_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+	}
+#endif
+#ifdef KSZ_MRP
+	sw->features |= MRP_SUPPORT;
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops = &mrp_ops;
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+		mrp->ops->init(mrp);
+#endif
+	}
+#endif
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+	ksz_dsa_init();
+#endif
+
+#ifdef DBG_SPI_ACCESS
+	mutex_lock(&ks->lock);
+	for (i = 0; i < sw->phy_port_cnt; i++) {
+		u16 val = 0;
+
+		/* Disable EEE for now. */
+		port_mmd_write(sw, i, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+	}
+	mutex_unlock(&ks->lock);
+#endif
+
+#if !defined(DBG_SPI_ACCESS) && !defined(NO_DIRECT_ACCESS)
+	if (ks->irq <= 0)
+		return 0;
+	mutex_lock(&ks->lock);
+	HW_W32(ks, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	HW_W32(ks, REG_SW_PORT_INT_MASK__4, sw->PORT_MASK);
+	sw_w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	for (i = 0; i < sw->phy_port_cnt; i++)
+		port_w8(sw, i, REG_PORT_PHY_INT_ENABLE, 0);
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		port_w(sw, i, REG_PORT_INT_MASK, 0xff);
+		port_w16(sw, i, REG_PTP_PORT_TX_INT_STATUS__2, 0xffff);
+	}
+	mutex_unlock(&ks->lock);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0)
+		printk(KERN_WARNING "No switch interrupt\n");
+	else {
+		mutex_lock(&ks->lock);
+		sw_ena_intr(sw);
+		mutex_unlock(&ks->lock);
+	}
+#endif
+
+	return 0;
+
+#ifndef DBG_SPI_ACCESS
+err_mii:
+	kfree(sw->info);
+#endif
+
+err_sw:
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return ret;
+}
+
+static int __devexit ksz9897_remove(struct spi_device *spi)
+{
+	struct sw_priv *ks = dev_get_drvdata(&spi->dev);
+	struct ksz_sw *sw = &ks->sw;
+
+#ifdef CONFIG_NET_DSA_TAG_TAIL
+	ksz_dsa_cleanup();
+#endif
+#ifdef NO_ATTACHED_DEV
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->stop(ptp);
+		ptp->ops->exit(ptp);
+	}
+#endif
+#endif
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+#ifndef CONFIG_MICREL_SWITCH_EMBEDDED
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		exit_dlr_sysfs(ks->dev);
+#endif
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		exit_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+#endif
+	exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+	cancel_delayed_work_sync(&ks->link_read);
+	cancel_delayed_work_sync(&ks->stp_monitor);
+	delete_debugfs(ks);
+#ifdef KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_exit(&sw->info->dlr);
+#endif
+#ifdef KSZ_IBA
+	ksz_iba_exit(&sw->info->iba);
+#endif
+	kfree(sw->info);
+#ifndef DBG_SPI_ACCESS
+	ksz_mii_exit(ks);
+#endif
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return 0;
+}
+
+static const struct of_device_id ksz9897_dt_ids[] = {
+	{ .compatible = "micrel,ksz9567" },
+	{ .compatible = "micrel,ksz9467" },
+	{ .compatible = "micrel,ksz9893" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz9897_dt_ids);
+
+static struct spi_driver ksz9897_driver = {
+	.driver = {
+		.name = KS9897MLI_DEV0,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz9897_dt_ids),
+	},
+	.probe = ksz9897_probe,
+	.remove = __devexit_p(ksz9897_remove),
+};
+
+#if defined(CONFIG_SPI_FTDI) && defined(CONFIG_ARCH_PEGASUS) && !defined(CONFIG_SPI_KSZ9897_MODULE)
+static void ksz9897_late_init(void)
+{
+	spi_register_driver(&ksz9897_driver);
+}
+#endif
+
+static int __init ksz9897_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+#if defined(CONFIG_SPI_FTDI) && defined(CONFIG_ARCH_PEGASUS)
+	if (spi_bus == 2)
+		sprintf((char *) ksz9897_driver.driver.name, KS9897MLI_DEV2);
+#endif
+
+#if defined(CONFIG_SPI_FTDI) && defined(CONFIG_ARCH_PEGASUS) && !defined(CONFIG_SPI_KSZ9897_MODULE)
+	pegasus_register_late_call(ksz9897_late_init);
+	return 0;
+#else
+	return spi_register_driver(&ksz9897_driver);
+#endif
+}
+
+static void __exit ksz9897_exit(void)
+{
+	spi_unregister_driver(&ksz9897_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_MICREL_KSZ9897_EMBEDDED
+module_init(ksz9897_init);
+module_exit(ksz9897_exit);
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+
+#ifdef KSZ_IBA
+module_param(iba, int, 0);
+MODULE_PARM_DESC(iba, "IBA support");
+#endif
+#endif
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(0=level low, 1=falling)");
+
+module_param(need_link_up, int, 0);
+MODULE_PARM_DESC(need_link_up,
+	"Configure whether to always indicate link is up");
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+module_param(sw_host_port, int, 0);
+MODULE_PARM_DESC(sw_host_port,
+	"Configure switch host port");
+
+#ifndef CONFIG_MICREL_KSZ9897_EMBEDDED
+MODULE_DESCRIPTION("Micrel KSZ9897 SPI Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz9897");
+#endif
diff --git a/drivers/ptp/Kconfig b/drivers/ptp/Kconfig
index cd9bc3b..9be1572 100644
--- a/drivers/ptp/Kconfig
+++ b/drivers/ptp/Kconfig
@@ -53,6 +53,16 @@ config PTP_1588_CLOCK_IXP46X
 	  To compile this driver as a module, choose M here: the module
 	  will be called ptp_ixp46x.
 
+config PTP_1588_CLOCK_MICREL
+	bool "Micrel PTP clock"
+	depends on PTP_1588_CLOCK
+	depends on MICREL_KSZ9897_PTP
+	help
+	  This adds support for using Micrel PTP clock device as a PTP
+	  clock. This clock is only useful if your PTP programs are
+	  getting hardware time stamps on the PTP Ethernet packets
+	  using the SO_TIMESTAMPING API.
+
 comment "Enable PHYLIB and NETWORK_PHY_TIMESTAMPING to see the additional clocks."
 	depends on PTP_1588_CLOCK && (PHYLIB=n || NETWORK_PHY_TIMESTAMPING=n)
 
diff --git a/drivers/ptp/ptp_clock.c b/drivers/ptp/ptp_clock.c
index f519a13..b8e0ea3 100644
--- a/drivers/ptp/ptp_clock.c
+++ b/drivers/ptp/ptp_clock.c
@@ -148,6 +148,18 @@ static int ptp_clock_adjtime(struct posix_clock *pc, struct timex *tx)
 
 		err = ops->adjfreq(ops, scaled_ppm_to_ppb(tx->freq));
 	}
+#ifdef CONFIG_PTP_1588_CLOCK_MICREL
+	else if (!tx->modes) {
+		long freq;
+
+		err = ops->gettime(ops, NULL);
+		freq = err;
+		freq <<= 13;
+		freq /= 125;
+		tx->freq = freq;
+		err = 0;
+	}
+#endif
 
 	return err;
 }
